#!/usr/bin/env python3
"""
Document Processor - æ–‡æª”è™•ç†æ ¸å¿ƒæ¨¡çµ„
æ•´åˆ Docling, LangChain ç­‰å·¥å…·ï¼Œæä¾›çµ±ä¸€çš„æ–‡æª”è™•ç†ä»‹é¢

Author: Data Governance Team
Version: 1.0
Date: 2025-01-17
"""

from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import re
import time

# æ–‡æª”è™•ç†
from docling.document_converter import DocumentConverter
from langchain_text_splitters import RecursiveCharacterTextSplitter

# æ•¸æ“šè™•ç†
import pandas as pd
import numpy as np


@dataclass
class DocumentMetadata:
    """
    æ¨™æº–åŒ–æ–‡æª”å…ƒè³‡æ–™çµæ§‹

    åŸºæ–¼é‹ç®—æ€ç¶­çš„æŠ½è±¡åŒ–è¨­è¨ˆ
    """

    # æ ¸å¿ƒè­˜åˆ¥
    document_id: str
    filename: str
    title: str
    content_hash: str

    # åˆ†é¡è³‡è¨Š
    category: str
    document_type: str
    year: Optional[int]

    # å…§å®¹ç‰¹å¾µ
    word_count: int
    char_count: int
    page_count: Optional[int]

    # çµæ§‹ç‰¹å¾µ
    has_abstract: bool
    has_references: bool
    has_tables: bool
    has_figures: bool

    # å“è³ªæŒ‡æ¨™
    quality_score: float
    extraction_confidence: float

    # æ™‚é–“æˆ³è¨˜
    created_at: str
    processed_at: str

    # å¯é¸æ¬„ä½
    keywords: Optional[List[str]] = None
    language: str = "en"
    authors: Optional[List[str]] = None

    def to_dict(self) -> Dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return asdict(self)


@dataclass
class ProcessingResult:
    """æ–‡æª”è™•ç†çµæœ"""

    success: bool
    content: Optional[str]
    metadata: Optional[DocumentMetadata]
    error: Optional[str] = None
    processing_time: float = 0.0


class DocumentProcessor:
    """
    æ–‡æª”è™•ç†å™¨ä¸»é¡

    æ•´åˆå¤šç¨®æ–‡æª”è™•ç†å·¥å…·ï¼Œæä¾›çµ±ä¸€ä»‹é¢
    åŸºæ–¼é‹ç®—æ€ç¶­çš„åˆ†è§£èˆ‡æ¨¡å¼è­˜åˆ¥è¨­è¨ˆ
    """

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å«:
                - quality_threshold: å“è³ªé–¾å€¼
                - chunk_size: åˆ†å¡Šå¤§å°
                - chunk_overlap: åˆ†å¡Šé‡ç–Š
        """
        self.config = config or {}

        # Docling è½‰æ›å™¨
        self.converter = DocumentConverter()

        # LangChain åˆ†å¡Šå™¨
        self.chunker = RecursiveCharacterTextSplitter(
            chunk_size=self.config.get('chunk_size', 1000),
            chunk_overlap=self.config.get('chunk_overlap', 200),
            separators=["\n\n", "\n", "ã€‚", ".", " "]
        )

        # å“è³ªé–¾å€¼
        self.quality_threshold = self.config.get('quality_threshold', 0.7)

        # è™•ç†çµ±è¨ˆ
        self.stats = {
            'total_processed': 0,
            'success_count': 0,
            'failed_count': 0,
            'total_time': 0.0
        }

    def process_document(self, file_path: str) -> ProcessingResult:
        """
        è™•ç†å–®å€‹æ–‡æª”

        ä¸»æµç¨‹ï¼š
        1. æå–å…§å®¹ï¼ˆDoclingï¼‰
        2. åˆ†æçµæ§‹
        3. æå–å…ƒè³‡æ–™
        4. è©•ä¼°å“è³ª
        5. è¿”å›çµæœ

        Args:
            file_path: æ–‡æª”è·¯å¾‘

        Returns:
            ProcessingResult ç‰©ä»¶
        """
        start_time = time.time()
        self.stats['total_processed'] += 1

        try:
            # Step 1: æå–å…§å®¹
            content = self._extract_content(file_path)

            # Step 2: åˆ†æçµæ§‹
            structure_features = self._analyze_structure(content)

            # Step 3: æå–å…ƒè³‡æ–™
            metadata = self._extract_metadata(
                file_path,
                content,
                structure_features
            )

            # Step 4: è©•ä¼°å“è³ª
            quality_score = self._assess_quality(metadata, content)
            metadata.quality_score = quality_score

            processing_time = time.time() - start_time
            self.stats['success_count'] += 1
            self.stats['total_time'] += processing_time

            return ProcessingResult(
                success=True,
                content=content,
                metadata=metadata,
                processing_time=processing_time
            )

        except Exception as e:
            processing_time = time.time() - start_time
            self.stats['failed_count'] += 1
            self.stats['total_time'] += processing_time

            return ProcessingResult(
                success=False,
                content=None,
                metadata=None,
                error=str(e),
                processing_time=processing_time
            )

    def _extract_content(self, file_path: str) -> str:
        """
        ä½¿ç”¨ Docling æå–æ–‡æª”å…§å®¹

        Args:
            file_path: æ–‡æª”è·¯å¾‘

        Returns:
            æå–çš„æ–‡æœ¬å…§å®¹
        """
        result = self.converter.convert(file_path)
        content = result.document.export_to_markdown()
        return content

    def _analyze_structure(self, content: str) -> Dict:
        """
        åˆ†ææ–‡æª”çµæ§‹ç‰¹å¾µ

        åŸºæ–¼æ¨¡å¼è­˜åˆ¥ï¼šè«–æ–‡é€šå¸¸æœ‰å›ºå®šçµæ§‹

        Args:
            content: æ–‡æª”å…§å®¹

        Returns:
            çµæ§‹ç‰¹å¾µå­—å…¸
        """
        features = {}

        # æª¢æ¸¬ Abstract
        features['has_abstract'] = bool(
            re.search(r'(?i)\babstract\b', content[:2000])
        )

        # æª¢æ¸¬ References
        features['has_references'] = bool(
            re.search(r'(?i)\breferences?\b', content)
        )

        # æª¢æ¸¬è¡¨æ ¼ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        features['has_tables'] = content.count('|') > 10

        # æª¢æ¸¬åœ–ç‰‡å¼•ç”¨
        features['has_figures'] = bool(
            re.search(r'(?i)\b(figure|fig\.|åœ–)\s*\d+', content)
        )

        # çµ±è¨ˆ
        features['word_count'] = len(content.split())
        features['char_count'] = len(content)

        # æ¨™é¡Œæ•¸é‡
        features['header_count'] = content.count('#')

        return features

    def _extract_metadata(self,
                         file_path: str,
                         content: str,
                         structure_features: Dict) -> DocumentMetadata:
        """
        æå–æ–‡æª”å…ƒè³‡æ–™

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            content: æ–‡æª”å…§å®¹
            structure_features: çµæ§‹ç‰¹å¾µ

        Returns:
            DocumentMetadata ç‰©ä»¶
        """
        path = Path(file_path)

        # ç”Ÿæˆæ–‡æª” ID
        doc_id = hashlib.md5(path.name.encode()).hexdigest()[:16]

        # è¨ˆç®—å…§å®¹ hash
        content_hash = hashlib.sha256(
            content.encode() if content else b""
        ).hexdigest()[:16]

        # å¾æª”åæå–å¹´ä»½
        year_match = re.match(r"(\d{4})", path.stem)
        year = int(year_match.group(1)) if year_match else None

        # å¾è·¯å¾‘æå–åˆ†é¡
        category = path.parent.name

        # æå–æ¨™é¡Œï¼ˆå¾æª”åï¼‰
        title = path.stem.replace('_', ' ')

        # æª¢æ¸¬æ–‡æª”é¡å‹
        document_type = self._detect_document_type(content, structure_features)

        return DocumentMetadata(
            document_id=doc_id,
            filename=path.name,
            title=title,
            content_hash=content_hash,
            category=category,
            document_type=document_type,
            year=year,
            word_count=structure_features['word_count'],
            char_count=structure_features['char_count'],
            page_count=None,  # å¯å¾ Docling çµæœæå–
            has_abstract=structure_features['has_abstract'],
            has_references=structure_features['has_references'],
            has_tables=structure_features['has_tables'],
            has_figures=structure_features['has_figures'],
            quality_score=0.0,  # ç¨å¾Œè¨ˆç®—
            extraction_confidence=0.85,
            created_at=datetime.fromtimestamp(path.stat().st_ctime).isoformat(),
            processed_at=datetime.now().isoformat(),
            language="en"
        )

    def _detect_document_type(self, content: str, features: Dict) -> str:
        """
        æª¢æ¸¬æ–‡æª”é¡å‹

        åŸºæ–¼æ¨¡å¼è­˜åˆ¥

        Args:
            content: æ–‡æª”å…§å®¹
            features: çµæ§‹ç‰¹å¾µ

        Returns:
            æ–‡æª”é¡å‹å­—ç¬¦ä¸²
        """
        content_lower = content.lower()[:5000]  # åªçœ‹å‰ 5000 å­—ç¬¦

        # è«–æ–‡ç‰¹å¾µ
        if features['has_abstract'] and features['has_references']:
            return "research_paper"

        # æŠ€è¡“æ–‡æª”
        if any(keyword in content_lower for keyword in ['api', 'specification', 'implementation']):
            return "technical_doc"

        # å ±å‘Š
        if any(keyword in content_lower for keyword in ['report', 'analysis', 'findings']):
            return "report"

        # æ‰‹å†Š
        if any(keyword in content_lower for keyword in ['manual', 'guide', 'tutorial']):
            return "manual"

        # æ”¿ç­–
        if any(keyword in content_lower for keyword in ['policy', 'compliance', 'regulation']):
            return "policy"

        return "general"

    def _assess_quality(self, metadata: DocumentMetadata, content: str) -> float:
        """
        è©•ä¼°æ–‡æª”å“è³ª

        ç°¡åŒ–ç‰ˆ - åŸºæ–¼çµæ§‹å®Œæ•´æ€§å’Œå…§å®¹é•·åº¦

        Args:
            metadata: å…ƒè³‡æ–™
            content: æ–‡æª”å…§å®¹

        Returns:
            å“è³ªåˆ†æ•¸ (0-1)
        """
        score = 0.0

        # åŸºç¤åˆ† (40%)
        score += 0.4

        # çµæ§‹å®Œæ•´æ€§ (30%)
        if metadata.has_abstract:
            score += 0.15
        if metadata.has_references:
            score += 0.15

        # å…§å®¹é•·åº¦ (20%)
        if metadata.word_count >= 5000:
            score += 0.2
        elif metadata.word_count >= 2000:
            score += 0.15
        elif metadata.word_count >= 1000:
            score += 0.1

        # çµæ§‹è±å¯Œåº¦ (10%)
        if metadata.has_tables or metadata.has_figures:
            score += 0.1

        return round(min(1.0, score), 2)

    def chunk_content(self, content: str, metadata: Optional[DocumentMetadata] = None) -> List[Dict]:
        """
        å°‡å…§å®¹åˆ†å¡Š

        ä½¿ç”¨ LangChain RecursiveCharacterTextSplitter

        Args:
            content: æ–‡æª”å…§å®¹
            metadata: å…ƒè³‡æ–™ï¼ˆå¯é¸ï¼‰

        Returns:
            åˆ†å¡Šåˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« text å’Œ metadata
        """
        chunks = self.chunker.split_text(content)

        chunk_list = []
        for i, chunk in enumerate(chunks):
            chunk_data = {
                "text": chunk,
                "chunk_id": i,
                "char_count": len(chunk),
                "word_count": len(chunk.split())
            }

            # é™„åŠ æ–‡æª”å…ƒè³‡æ–™
            if metadata:
                chunk_data["document_id"] = metadata.document_id
                chunk_data["source_file"] = metadata.filename
                chunk_data["category"] = metadata.category

            chunk_list.append(chunk_data)

        return chunk_list

    def batch_process(self,
                     file_paths: List[str],
                     show_progress: bool = True) -> List[ProcessingResult]:
        """
        æ‰¹é‡è™•ç†æ–‡æª”

        Args:
            file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
            show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦

        Returns:
            è™•ç†çµæœåˆ—è¡¨
        """
        results = []

        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(file_paths, desc="Processing documents")
            except ImportError:
                iterator = file_paths
                print(f"Processing {len(file_paths)} documents...")
        else:
            iterator = file_paths

        for file_path in iterator:
            result = self.process_document(file_path)
            results.append(result)

            # é¿å…éåº¦è² è¼‰
            time.sleep(0.1)

        return results

    def get_stats(self) -> Dict:
        """
        ç²å–è™•ç†çµ±è¨ˆ

        Returns:
            çµ±è¨ˆå­—å…¸
        """
        stats = self.stats.copy()
        if stats['total_processed'] > 0:
            stats['success_rate'] = round(
                stats['success_count'] / stats['total_processed'] * 100,
                1
            )
            stats['avg_time'] = round(
                stats['total_time'] / stats['total_processed'],
                2
            )
        return stats

    def reset_stats(self):
        """é‡ç½®çµ±è¨ˆ"""
        self.stats = {
            'total_processed': 0,
            'success_count': 0,
            'failed_count': 0,
            'total_time': 0.0
        }


# ===== è¼”åŠ©å‡½æ•¸ =====

def scan_directory(directory: Path,
                   extensions: Optional[List[str]] = None) -> List[Path]:
    """
    æƒæç›®éŒ„ï¼Œæ‰¾å‡ºæ‰€æœ‰ç¬¦åˆçš„æ–‡æª”

    Args:
        directory: ç›®éŒ„è·¯å¾‘
        extensions: æª”æ¡ˆå‰¯æª”ååˆ—è¡¨ï¼ˆé è¨­ ['.pdf', '.docx', '.txt']ï¼‰

    Returns:
        æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    if extensions is None:
        extensions = ['.pdf', '.docx', '.txt', '.md']

    files = []
    for ext in extensions:
        files.extend(directory.rglob(f"*{ext}"))

    return sorted(files)


def results_to_dataframe(results: List[ProcessingResult]) -> pd.DataFrame:
    """
    å°‡è™•ç†çµæœè½‰æ›ç‚º DataFrame

    Args:
        results: è™•ç†çµæœåˆ—è¡¨

    Returns:
        pandas DataFrame
    """
    data = []

    for result in results:
        if result.success and result.metadata:
            row = result.metadata.to_dict()
            row['processing_time'] = result.processing_time
            row['processing_success'] = True
        else:
            row = {
                'processing_success': False,
                'error': result.error,
                'processing_time': result.processing_time
            }

        data.append(row)

    return pd.DataFrame(data)


# ===== ç¤ºä¾‹ä½¿ç”¨ =====

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šè™•ç†å–®å€‹æ–‡æª”
    processor = DocumentProcessor(config={
        'quality_threshold': 0.75,
        'chunk_size': 1000
    })

    # å‡è¨­æœ‰ä¸€å€‹æ¸¬è©¦æª”æ¡ˆ
    test_file = "../../papers/01_model_paradigm/2017_Transformer.pdf"

    if Path(test_file).exists():
        result = processor.process_document(test_file)

        if result.success:
            print(f"âœ… è™•ç†æˆåŠŸ: {result.metadata.title}")
            print(f"   å“è³ªåˆ†æ•¸: {result.metadata.quality_score}")
            print(f"   å­—æ•¸: {result.metadata.word_count:,}")

            # åˆ†å¡Š
            chunks = processor.chunk_content(result.content, result.metadata)
            print(f"   åˆ†å¡Šæ•¸: {len(chunks)}")
        else:
            print(f"âŒ è™•ç†å¤±æ•—: {result.error}")

        # é¡¯ç¤ºçµ±è¨ˆ
        stats = processor.get_stats()
        print(f"\nğŸ“Š è™•ç†çµ±è¨ˆ:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
    else:
        print(f"æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨: {test_file}")
