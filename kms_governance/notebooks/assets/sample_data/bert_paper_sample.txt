
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Abstract
We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.

1. Introduction
Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence classification tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically.
