{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÁµÑ 2: ‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñôÁÆ°ÁêÜËàáÁ¥¢ÂºïÁ≥ªÁµ±\n",
    "## Module 2: Enterprise Metadata Management & Indexing Systems\n",
    "\n",
    "> **ÊïôÂ≠∏ÁõÆÊ®ô**: Ë®≠Ë®à‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñô schema„ÄÅÂª∫Á´ãÂ§öÂ±§Á¥¢ÂºïÁ≥ªÁµ±„ÄÅÂØ¶‰ΩúÊ∑∑ÂêàÊ™¢Á¥¢Á≠ñÁï•\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Â≠∏ÁøíÊàêÊûú (Learning Outcomes)\n",
    "\n",
    "ÂÆåÊàêÊú¨Ê®°ÁµÑÂæåÔºåÊÇ®Â∞áËÉΩÂ§†Ôºö\n",
    "1. **Ë®≠Ë®àÂèØÊì¥Â±ïÁöÑÂÖÉË≥áÊñô schema** - ÊîØÊè¥Â§öÊñáÊ™îÈ°ûÂûãÁöÑÁµ±‰∏ÄÊ®°Âûã\n",
    "2. **Âª∫Á´ã‰ºÅÊ•≠Á¥öÁ¥¢ÂºïÁ≥ªÁµ±** - ÂêëÈáèÁ¥¢Âºï„ÄÅÈóúÈçµÂ≠óÁ¥¢Âºï„ÄÅÊ∑∑ÂêàÁ¥¢Âºï\n",
    "3. **ÂØ¶‰ΩúÈ´òÊïàÊ™¢Á¥¢Á≠ñÁï•** - Ë™ûÁæ©Ê™¢Á¥¢ + ÈóúÈçµÂ≠óÊ™¢Á¥¢ÁöÑËûçÂêà\n",
    "4. **Âª∫Á´ãÂÖÉË≥áÊñôÊ≤ªÁêÜÊ©üÂà∂** - ÁâàÊú¨ÊéßÂà∂„ÄÅË°ÄÁ∑£ËøΩËπ§„ÄÅÂìÅË≥™Áõ£Êéß\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ê†∏ÂøÉÊ¶ÇÂøµ (Core Concepts)\n",
    "\n",
    "### 1. ‰ºÅÊ•≠ÂÖÉË≥áÊñôÊû∂Êßã\n",
    "- **Áµ±‰∏ÄË≥áÊñôÊ®°Âûã**: ÊîØÊè¥Ë´ñÊñá„ÄÅÂ†±Âëä„ÄÅÊîøÁ≠ñÁ≠âÂ§öÈ°ûÂûãÊñáÊ™î\n",
    "- **ÂÖÉË≥áÊñôË°ÄÁ∑£**: ËøΩËπ§Ë≥áÊñô‰æÜÊ∫êËàáËôïÁêÜÊ≠∑Á®ã\n",
    "- **Ë™ûÁæ©Ê®ôË®ª**: Ëá™ÂãïÊèêÂèñÂØ¶È´î„ÄÅÈóú‰øÇ„ÄÅÊ¶ÇÂøµ\n",
    "- **ÂìÅË≥™ÊåáÊ®ô**: ÂÆåÊï¥ÊÄß„ÄÅÊ∫ñÁ¢∫ÊÄß„ÄÅ‰∏ÄËá¥ÊÄßË©ï‰º∞\n",
    "\n",
    "### 2. Â§öÂ±§Á¥¢ÂºïÁ≠ñÁï•\n",
    "- **ÂêëÈáèÁ¥¢Âºï**: È´òÁ∂≠Ë™ûÁæ©ÂµåÂÖ•Ê™¢Á¥¢\n",
    "- **ÂÄíÊéíÁ¥¢Âºï**: ÂÇ≥Áµ±ÈóúÈçµÂ≠óÊ™¢Á¥¢\n",
    "- **ÂúñÁ¥¢Âºï**: ÂØ¶È´îÈóú‰øÇÁ∂≤Áµ°Ê™¢Á¥¢\n",
    "- **Ê∑∑ÂêàÁ¥¢Âºï**: ËûçÂêàÂ§öÁ®ÆÊ™¢Á¥¢ÊñπÂºè\n",
    "\n",
    "### 3. Ê™¢Á¥¢Â¢ûÂº∑ÊäÄË°ì\n",
    "- **Êü•Ë©¢ÈáçÂØ´**: Êì¥Â±ï‰ΩøÁî®ËÄÖÊü•Ë©¢ÊÑèÂúñ\n",
    "- **ÁµêÊûúÈáçÊéí**: Âü∫ÊñºÁõ∏ÈóúÊÄßÂíåÂ§öÊ®£ÊÄß\n",
    "- **ÂÄã‰∫∫Âåñ**: Ê†πÊìö‰ΩøÁî®ËÄÖÊ≠∑Âè≤Ë™øÊï¥ÁµêÊûú\n",
    "- **Ëß£ÈáãÊÄß**: Êèê‰æõÊ™¢Á¥¢ÁêÜÁî±Ë™™Êòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Áí∞Â¢ÉË®≠ÂÆöËàá‰æùË≥¥ÂÆâË£ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chromadb in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: sentence-transformers in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (5.1.2)\n",
      "Requirement already satisfied: sqlalchemy in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (2.0.43)\n",
      "Requirement already satisfied: alembic in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (1.16.5)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: importlib-resources in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (2.12.4)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (3.11.4)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (6.0.3)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (0.30.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: Pillow in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: scikit-learn in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sentence-transformers) (2.4.0+cu121)\n",
      "Requirement already satisfied: greenlet>=1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: tomli in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from alembic) (2.2.1)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic) (1.1.3)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: certifi in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx>=0.27.0->chromadb) (3.3)\n",
      "Requirement already satisfied: anyio in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: h11>=0.16 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: requests in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: requests-oauthlib in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: coloredlogs in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.41.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: whoosh in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (2.7.4)\n",
      "Requirement already satisfied: elasticsearch-dsl in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (8.18.0)\n",
      "Requirement already satisfied: rank-bm25 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: python-dateutil in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from elasticsearch-dsl) (2.9.0.post0)\n",
      "Requirement already satisfied: elastic-transport<9.0.0,>=8.0.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from elasticsearch-dsl) (8.17.1)\n",
      "Requirement already satisfied: typing-extensions in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from elasticsearch-dsl) (4.15.0)\n",
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.0.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from elasticsearch-dsl) (8.19.2)\n",
      "Requirement already satisfied: numpy in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from rank-bm25) (1.26.4)\n",
      "Requirement already satisfied: certifi in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from elastic-transport<9.0.0,>=8.0.0->elasticsearch-dsl) (2025.10.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from elastic-transport<9.0.0,>=8.0.0->elasticsearch-dsl) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from python-dateutil->elasticsearch-dsl) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (3.1)\n",
      "Requirement already satisfied: plotly in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (6.4.0)\n",
      "Requirement already satisfied: pandas in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from plotly) (2.11.0)\n",
      "Requirement already satisfied: packaging in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# ÂÆâË£ùÊ†∏ÂøÉ‰æùË≥¥Â•ó‰ª∂\n",
    "!pip install chromadb sentence-transformers sqlalchemy alembic\n",
    "!pip install whoosh elasticsearch-dsl rank-bm25\n",
    "!pip install networkx plotly pandas numpy scikit-learn\n",
    "!pip install pydantic fastapi uvicorn\n",
    "\n",
    "# ÂÆâË£ù NLP Áõ∏ÈóúÂ•ó‰ª∂\n",
    "!pip install spacy transformers torch\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Áí∞Â¢ÉË®≠ÂÆöÂÆåÊàê\n",
      "üìÇ Â∞àÊ°àÊ†πÁõÆÈåÑ: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance\n",
      "ü§ñ CUDA ÂèØÁî®: True\n"
     ]
    }
   ],
   "source": [
    "# Â∞éÂÖ•ÂøÖË¶ÅÁöÑÂáΩÂºèÂ∫´\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Optional, Tuple, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ë≥áÊñôËôïÁêÜËàáÂàÜÊûê\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ÂµåÂÖ•ËàáÂêëÈáèËôïÁêÜ\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Ë≥áÊñôÂ∫´Ëàá ORM\n",
    "from sqlalchemy import create_engine, Column, String, Integer, Float, DateTime, Text, JSON, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.dialects.sqlite import insert\n",
    "\n",
    "# NLP ËôïÁêÜ\n",
    "import spacy\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Ë≥áÊñôÈ©óË≠â\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# ÂèØË¶ñÂåñ\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "\n",
    "# Ë®≠ÂÆöÂ∞àÊ°àË∑ØÂæë\n",
    "PROJECT_ROOT = Path('/home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance')\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"‚úÖ Áí∞Â¢ÉË®≠ÂÆöÂÆåÊàê\")\n",
    "print(f\"üìÇ Â∞àÊ°àÊ†πÁõÆÈåÑ: {PROJECT_ROOT}\")\n",
    "print(f\"ü§ñ CUDA ÂèØÁî®: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã ‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñôÊ®°ÂûãË®≠Ë®à\n",
    "\n",
    "### Áµ±‰∏ÄÂÖÉË≥áÊñô Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñôÊ®°ÂûãÂÆöÁæ©ÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "# SQLAlchemy Âü∫Á§éË®≠ÂÆö\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class DocumentType(Enum):\n",
    "    \"\"\"ÊñáÊ™îÈ°ûÂûãÊûöËàâ\"\"\"\n",
    "    ACADEMIC_PAPER = \"academic_paper\"\n",
    "    BUSINESS_REPORT = \"business_report\"\n",
    "    TECHNICAL_DOC = \"technical_doc\"\n",
    "    POLICY = \"policy\"\n",
    "    PROCEDURE = \"procedure\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class ProcessingStatus(Enum):\n",
    "    \"\"\"ËôïÁêÜÁãÄÊÖãÊûöËàâ\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    PROCESSING = \"processing\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    ARCHIVED = \"archived\"\n",
    "\n",
    "class SecurityLevel(Enum):\n",
    "    \"\"\"ÂÆâÂÖ®Á≠âÁ¥öÊûöËàâ\"\"\"\n",
    "    PUBLIC = \"public\"\n",
    "    INTERNAL = \"internal\"\n",
    "    CONFIDENTIAL = \"confidential\"\n",
    "    RESTRICTED = \"restricted\"\n",
    "\n",
    "# ‰ºÅÊ•≠Á¥öÊñáÊ™îÂÖÉË≥áÊñôË°®\n",
    "class EnterpriseDocument(Base):\n",
    "    \"\"\"‰ºÅÊ•≠Á¥öÊñáÊ™îÂÖÉË≥áÊñôË°®\"\"\"\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    # Âü∫Á§éË≠òÂà•Ë≥áË®ä\n",
    "    document_id = Column(String(50), primary_key=True)\n",
    "    title = Column(String(500), nullable=False, index=True)\n",
    "    subtitle = Column(String(500))\n",
    "    \n",
    "    # ÂÖßÂÆπÂàÜÈ°û\n",
    "    document_type = Column(String(50), nullable=False, index=True)\n",
    "    category = Column(String(100), index=True)\n",
    "    subcategory = Column(String(100))\n",
    "    \n",
    "    # ‰ΩúËÄÖËàá‰æÜÊ∫ê\n",
    "    authors = Column(JSON)  # List of author names\n",
    "    organization = Column(String(200))\n",
    "    department = Column(String(100))\n",
    "    source_url = Column(String(1000))\n",
    "    \n",
    "    # Ê™îÊ°àË≥áË®ä\n",
    "    file_path = Column(String(1000), nullable=False)\n",
    "    file_name = Column(String(255), nullable=False)\n",
    "    file_size = Column(Integer)\n",
    "    file_format = Column(String(10))\n",
    "    \n",
    "    # ÂÖßÂÆπÁµ±Ë®à\n",
    "    page_count = Column(Integer, default=0)\n",
    "    word_count = Column(Integer, default=0)\n",
    "    paragraph_count = Column(Integer, default=0)\n",
    "    \n",
    "    # Ë™ûË®ÄËàáÂú∞ÂçÄ\n",
    "    language = Column(String(10), default='en')\n",
    "    region = Column(String(50))\n",
    "    \n",
    "    # ÊôÇÈñìÊà≥Ë®ò\n",
    "    created_date = Column(DateTime, nullable=False)\n",
    "    modified_date = Column(DateTime)\n",
    "    processed_date = Column(DateTime)\n",
    "    ingested_date = Column(DateTime, default=datetime.now)\n",
    "    \n",
    "    # ËôïÁêÜÁãÄÊÖã\n",
    "    processing_status = Column(String(20), default='pending', index=True)\n",
    "    \n",
    "    # ÂìÅË≥™ËàáÂÆâÂÖ®\n",
    "    quality_score = Column(Float, default=0.0, index=True)\n",
    "    security_level = Column(String(20), default='internal')\n",
    "    \n",
    "    # Ë™ûÁæ©Ê®ôÁ±§\n",
    "    keywords = Column(JSON)  # List of extracted keywords\n",
    "    entities = Column(JSON)  # Named entities\n",
    "    topics = Column[Any](JSON)    # Topic modeling results\n",
    "    \n",
    "    # ÁâàÊú¨ÊéßÂà∂\n",
    "    version = Column(String(20), default='1.0')\n",
    "    parent_id = Column(String(50), ForeignKey('documents.document_id'))\n",
    "    \n",
    "    # ‰ΩøÁî®Áµ±Ë®à\n",
    "    access_count = Column(Integer, default=0)\n",
    "    last_accessed = Column(DateTime)\n",
    "    \n",
    "    # Ëá™Ë®ÇÊ¨Ñ‰Ωç\n",
    "    custom_metadata = Column(JSON)  # Flexible additional metadata\n",
    "    \n",
    "    # ÈóúËÅØ\n",
    "    chunks = relationship(\"DocumentChunk\", back_populates=\"document\", cascade=\"all, delete-orphan\")\n",
    "    lineage_records = relationship(\"DataLineage\", back_populates=\"document\")\n",
    "\n",
    "# ÊñáÊ™îÂàÜÂ°äË°®\n",
    "class DocumentChunk(Base):\n",
    "    \"\"\"ÊñáÊ™îÂàÜÂ°äË°®\"\"\"\n",
    "    __tablename__ = 'document_chunks'\n",
    "    \n",
    "    chunk_id = Column(String(50), primary_key=True)\n",
    "    document_id = Column(String(50), ForeignKey('documents.document_id'), nullable=False, index=True)\n",
    "    \n",
    "    # ÂÖßÂÆπË≥áË®ä\n",
    "    content = Column(Text, nullable=False)\n",
    "    content_hash = Column(String(64), index=True)  # SHA-256 of content\n",
    "    \n",
    "    # ‰ΩçÁΩÆË≥áË®ä\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    start_page = Column(Integer)\n",
    "    end_page = Column(Integer)\n",
    "    start_offset = Column(Integer)\n",
    "    end_offset = Column(Integer)\n",
    "    \n",
    "    # Áµ±Ë®àË≥áË®ä\n",
    "    word_count = Column(Integer, nullable=False)\n",
    "    sentence_count = Column(Integer)\n",
    "    \n",
    "    # Ë™ûÁæ©È°ûÂûã\n",
    "    semantic_type = Column(String(50), default='text')  # text, table, figure, header, etc.\n",
    "    \n",
    "    # ÂêëÈáèÂµåÂÖ•\n",
    "    embedding_model = Column(String(100))\n",
    "    embedding_dim = Column(Integer)\n",
    "    \n",
    "    # ÂìÅË≥™ÊåáÊ®ô\n",
    "    chunk_quality = Column(Float, default=0.0)\n",
    "    \n",
    "    # ÊôÇÈñìÊà≥Ë®ò\n",
    "    created_date = Column(DateTime, default=datetime.now)\n",
    "    \n",
    "    # ÈóúËÅØ\n",
    "    document = relationship(\"EnterpriseDocument\", back_populates=\"chunks\")\n",
    "\n",
    "# Ë≥áÊñôË°ÄÁ∑£ËøΩËπ§Ë°®\n",
    "class DataLineage(Base):\n",
    "    \"\"\"Ë≥áÊñôË°ÄÁ∑£ËøΩËπ§Ë°®\"\"\"\n",
    "    __tablename__ = 'data_lineage'\n",
    "    \n",
    "    lineage_id = Column[str](String(50), primary_key=True)\n",
    "    document_id = Column[str](String(50), ForeignKey('documents.document_id'), nullable=False)\n",
    "    \n",
    "    # ËôïÁêÜÈöéÊÆµ\n",
    "    stage = Column[str](String(50), nullable=False)  # ingestion, processing, indexing, etc.\n",
    "    operation = Column[str](String(100), nullable=False)  # extract, chunk, embed, etc.\n",
    "    \n",
    "    # Ëº∏ÂÖ•Ëº∏Âá∫\n",
    "    input_data = Column[Any](JSON)   # Input parameters/data\n",
    "    output_data = Column[Any](JSON)  # Output results\n",
    "    \n",
    "    # ËôïÁêÜË≥áË®ä\n",
    "    processor = Column[str](String(100))  # Which component processed\n",
    "    processor_version = Column[str](String(20))\n",
    "    processing_time_ms = Column[int](Integer)\n",
    "    \n",
    "    # ÂìÅË≥™ÊåáÊ®ô\n",
    "    success = Column[int](Integer, default=1)  # 1=success, 0=failure\n",
    "    error_message = Column[str](Text)\n",
    "    \n",
    "    # ÊôÇÈñìÊà≥Ë®ò\n",
    "    timestamp = Column[datetime](DateTime, default=datetime.now, index=True)\n",
    "    \n",
    "    # ÈóúËÅØ\n",
    "    document = relationship(\"EnterpriseDocument\", back_populates=\"lineage_records\")\n",
    "\n",
    "print(\"‚úÖ ‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñôÊ®°ÂûãÂÆöÁæ©ÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic Ë≥áÊñôÈ©óË≠âÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pydantic È©óË≠âÊ®°ÂûãÂÆöÁæ©ÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "# Pydantic Ê®°ÂûãÁî®ÊñºË≥áÊñôÈ©óË≠âÂíå API ‰ªãÈù¢\n",
    "\n",
    "class DocumentMetadataRequest(BaseModel):\n",
    "    \"\"\"ÊñáÊ™îÂÖÉË≥áÊñôË´ãÊ±ÇÊ®°Âûã\"\"\"\n",
    "    title: str = Field(..., min_length=1, max_length=500)\n",
    "    subtitle: Optional[str] = Field(None, max_length=500)\n",
    "    document_type: str = Field(..., pattern=\"^(academic_paper|business_report|technical_doc|policy|procedure|other)$\")\n",
    "    category: Optional[str] = Field(None, max_length=100)\n",
    "    authors: List[str] = Field(default_factory=list)\n",
    "    organization: Optional[str] = Field(None, max_length=200)\n",
    "    language: str = Field(default=\"en\", pattern=\"^[a-z]{2}$\")\n",
    "    security_level: str = Field(default=\"internal\", pattern=\"^(public|internal|confidential|restricted)$\")\n",
    "    keywords: List[str] = Field(default_factory=list)\n",
    "    custom_metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \n",
    "    @validator('keywords')\n",
    "    def validate_keywords(cls, v):\n",
    "        if len(v) > 50:  # ÈôêÂà∂ÈóúÈçµÂ≠óÊï∏Èáè\n",
    "            raise ValueError('Too many keywords (max 50)')\n",
    "        return v\n",
    "\n",
    "class DocumentChunkRequest(BaseModel):\n",
    "    \"\"\"ÊñáÊ™îÂàÜÂ°äË´ãÊ±ÇÊ®°Âûã\"\"\"\n",
    "    content: str = Field(..., min_length=1)\n",
    "    chunk_index: int = Field(..., ge=0)\n",
    "    semantic_type: str = Field(default=\"text\")\n",
    "    start_page: Optional[int] = Field(None, ge=1)\n",
    "    end_page: Optional[int] = Field(None, ge=1)\n",
    "    \n",
    "    @validator('content')\n",
    "    def validate_content_length(cls, v):\n",
    "        if len(v.split()) < 5:  # Ëá≥Â∞ë5ÂÄãË©û\n",
    "            raise ValueError('Content too short (min 5 words)')\n",
    "        return v\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"ÊêúÂ∞ãÊü•Ë©¢Ê®°Âûã\"\"\"\n",
    "    query: str = Field(..., min_length=1, max_length=1000)\n",
    "    search_type: str = Field(default=\"hybrid\", pattern=\"^(semantic|keyword|hybrid)$\")\n",
    "    document_types: List[str] = Field(default_factory=list)\n",
    "    categories: List[str] = Field(default_factory=list)\n",
    "    date_from: Optional[datetime] = None\n",
    "    date_to: Optional[datetime] = None\n",
    "    min_quality_score: float = Field(default=0.0, ge=0.0, le=1.0)\n",
    "    limit: int = Field(default=10, ge=1, le=100)\n",
    "    include_chunks: bool = Field(default=False)\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    \"\"\"ÊêúÂ∞ãÁµêÊûúÊ®°Âûã\"\"\"\n",
    "    document_id: str\n",
    "    title: str\n",
    "    document_type: str\n",
    "    relevance_score: float\n",
    "    snippet: str\n",
    "    matched_chunks: List[str] = Field(default_factory=list)\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "print(\"‚úÖ Pydantic È©óË≠âÊ®°ÂûãÂÆöÁæ©ÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç ‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñôÁÆ°ÁêÜÂô®\n",
    "\n",
    "### Ê†∏ÂøÉÁÆ°ÁêÜÂô®ÂØ¶‰Ωú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EnterpriseMetadataManager ÈÉ®ÂàÜÊñπÊ≥ïÂÆöÁæ©ÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.orm.session import Session\n",
    "\n",
    "\n",
    "class EnterpriseMetadataManager:\n",
    "    \"\"\"\n",
    "    ‰ºÅÊ•≠Á¥öÂÖÉË≥áÊñôÁÆ°ÁêÜÂô®\n",
    "    Áµ±‰∏ÄÁÆ°ÁêÜÊñáÊ™îÂÖÉË≥áÊñôÁöÑ CRUD Êìç‰Ωú\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str, create_tables: bool = True):\n",
    "        \"\"\"ÂàùÂßãÂåñÂÖÉË≥áÊñôÁÆ°ÁêÜÂô®\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.engine = create_engine(f\"sqlite:///{db_path}\", echo=False)\n",
    "        self.SessionLocal = sessionmaker[Session](autocommit=False, autoflush=False, bind=self.engine)\n",
    "        \n",
    "        if create_tables:\n",
    "            Base.metadata.create_all(bind=self.engine)\n",
    "            print(f\"‚úÖ Ë≥áÊñôÂ∫´Ë°®Â∑≤Âª∫Á´ã: {db_path}\")\n",
    "        \n",
    "        # ÂàùÂßãÂåñ NLP Ê®°Âûã\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        print(\"‚úÖ NLP Ê®°ÂûãËºâÂÖ•ÂÆåÊàê\")\n",
    "    \n",
    "    def get_session(self):\n",
    "        \"\"\"Áç≤ÂèñË≥áÊñôÂ∫´ÊúÉË©±\"\"\"\n",
    "        return self.SessionLocal()\n",
    "    \n",
    "    def create_document(self, file_path: str, content: str, metadata_request: DocumentMetadataRequest) -> str:\n",
    "        \"\"\"\n",
    "        Âª∫Á´ãÊñ∞ÊñáÊ™îË®òÈåÑ\n",
    "        \n",
    "        Args:\n",
    "            file_path: Ê™îÊ°àË∑ØÂæë\n",
    "            content: Ê™îÊ°àÂÖßÂÆπ\n",
    "            metadata_request: ÂÖÉË≥áÊñôË´ãÊ±Ç\n",
    "            \n",
    "        Returns:\n",
    "            str: ÊñáÊ™î ID\n",
    "        \"\"\"\n",
    "        session = self.get_session()\n",
    "        \n",
    "        try:\n",
    "            # ÁîüÊàêÊñáÊ™î ID\n",
    "            document_id = self._generate_document_id(file_path, metadata_request.title)\n",
    "            \n",
    "            # ÊèêÂèñÊ™îÊ°àË≥áË®ä\n",
    "            file_info = self._extract_file_info(file_path)\n",
    "            \n",
    "            # ÂàÜÊûêÂÖßÂÆπÁµ±Ë®à\n",
    "            content_stats = self._analyze_content(content)\n",
    "            \n",
    "            # ÊèêÂèñË™ûÁæ©Ë≥áË®ä\n",
    "            semantic_info = self._extract_semantic_info(content)\n",
    "            \n",
    "            # Âª∫Á´ãÊñáÊ™îË®òÈåÑ\n",
    "            document = EnterpriseDocument(\n",
    "                document_id=document_id,\n",
    "                title=metadata_request.title,\n",
    "                subtitle=metadata_request.subtitle,\n",
    "                document_type=metadata_request.document_type,\n",
    "                category=metadata_request.category,\n",
    "                authors=metadata_request.authors,\n",
    "                organization=metadata_request.organization,\n",
    "                file_path=str(file_path),\n",
    "                file_name=file_info['name'],\n",
    "                file_size=file_info['size'],\n",
    "                file_format=file_info['format'],\n",
    "                word_count=content_stats['word_count'],\n",
    "                paragraph_count=content_stats['paragraph_count'],\n",
    "                language=metadata_request.language,\n",
    "                created_date=file_info['created_date'],\n",
    "                modified_date=file_info['modified_date'],\n",
    "                processed_date=datetime.now(),\n",
    "                processing_status='completed',\n",
    "                security_level=metadata_request.security_level,\n",
    "                keywords=metadata_request.keywords + semantic_info['keywords'],\n",
    "                entities=semantic_info['entities'],\n",
    "                topics=semantic_info['topics'],\n",
    "                custom_metadata=metadata_request.custom_metadata\n",
    "            )\n",
    "            \n",
    "            session.add(document)\n",
    "            session.commit()\n",
    "            \n",
    "            # Ë®òÈåÑË°ÄÁ∑£ - ‰øÆÊ≠£PydanticÊ®°ÂûãËΩâÊèõ\n",
    "            self._record_lineage(session, document_id, 'metadata_creation', 'create_document', \n",
    "                               {'input': metadata_request.dict()}, {'document_id': document_id})\n",
    "            \n",
    "            print(f\"‚úÖ ÊñáÊ™îÂ∑≤Âª∫Á´ã: {document_id}\")\n",
    "            return document_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"‚ùå Âª∫Á´ãÊñáÊ™îÂ§±Êïó: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "    \n",
    "    def add_document_chunks(self, document_id: str, chunks_data: List[DocumentChunkRequest]) -> List[str]:\n",
    "        \"\"\"\n",
    "        ÁÇ∫ÊñáÊ™îÊ∑ªÂä†ÂàÜÂ°ä\n",
    "        \n",
    "        Args:\n",
    "            document_id: ÊñáÊ™î ID\n",
    "            chunks_data: ÂàÜÂ°äË≥áÊñôÂàóË°®\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: ÂàÜÂ°ä ID ÂàóË°®\n",
    "        \"\"\"\n",
    "        session = self.get_session()\n",
    "        \n",
    "        try:\n",
    "            chunk_ids = []\n",
    "            \n",
    "            for chunk_data in chunks_data:\n",
    "                chunk_id = f\"{document_id}_chunk_{chunk_data.chunk_index}\"\n",
    "                content_hash = hashlib.sha256(chunk_data.content.encode()).hexdigest()\n",
    "                \n",
    "                chunk = DocumentChunk(\n",
    "                    chunk_id=chunk_id,\n",
    "                    document_id=document_id,\n",
    "                    content=chunk_data.content,\n",
    "                    content_hash=content_hash,\n",
    "                    chunk_index=chunk_data.chunk_index,\n",
    "                    start_page=chunk_data.start_page,\n",
    "                    end_page=chunk_data.end_page,\n",
    "                    word_count=len(chunk_data.content.split()),\n",
    "                    sentence_count=len(list(self.nlp(chunk_data.content).sents)),\n",
    "                    semantic_type=chunk_data.semantic_type,\n",
    "                )\n",
    "                \n",
    "                session.add(chunk)\n",
    "                chunk_ids.append(chunk_id)\n",
    "            \n",
    "            session.commit()\n",
    "            \n",
    "            # Ë®òÈåÑË°ÄÁ∑£\n",
    "            self._record_lineage(session, document_id, 'chunking', 'add_chunks',\n",
    "                               {'chunk_count': len(chunks_data)}, {'chunk_ids': chunk_ids})\n",
    "            \n",
    "            print(f\"‚úÖ Â∑≤Ê∑ªÂä† {len(chunk_ids)} ÂÄãÂàÜÂ°ä\")\n",
    "            return chunk_ids\n",
    "            \n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"‚ùå Ê∑ªÂä†ÂàÜÂ°äÂ§±Êïó: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "    \n",
    "    def get_document(self, document_id: str, include_chunks: bool = False) -> Optional[Dict]:\n",
    "        \"\"\"Áç≤ÂèñÊñáÊ™îË≥áË®ä\"\"\"\n",
    "        session = self.get_session()\n",
    "        \n",
    "        try:\n",
    "            # Êü•Ë©¢ÊñáÊ™î\n",
    "            document = session.query(EnterpriseDocument).filter(\n",
    "                EnterpriseDocument.document_id == document_id\n",
    "            ).first()\n",
    "            \n",
    "            if not document:\n",
    "                return None\n",
    "            \n",
    "            # ËΩâÊèõÁÇ∫Â≠óÂÖ∏\n",
    "            doc_dict = {\n",
    "                'document_id': document.document_id,\n",
    "                'title': document.title,\n",
    "                'subtitle': document.subtitle,\n",
    "                'document_type': document.document_type,\n",
    "                'category': document.category,\n",
    "                'authors': document.authors or [],\n",
    "                'organization': document.organization,\n",
    "                'file_path': document.file_path,\n",
    "                'file_name': document.file_name,\n",
    "                'file_size': document.file_size,\n",
    "                'word_count': document.word_count,\n",
    "                'language': document.language,\n",
    "                'created_date': document.created_date,\n",
    "                'processed_date': document.processed_date,\n",
    "                'quality_score': document.quality_score,\n",
    "                'security_level': document.security_level,\n",
    "                'keywords': document.keywords or [],\n",
    "                'entities': document.entities or {},\n",
    "                'topics': document.topics or [],\n",
    "                'custom_metadata': document.custom_metadata or {}\n",
    "            }\n",
    "            \n",
    "            # ÂåÖÂê´ÂàÜÂ°äË≥áË®ä\n",
    "            if include_chunks:\n",
    "                chunks = session.query(DocumentChunk).filter(\n",
    "                    DocumentChunk.document_id == document_id\n",
    "                ).order_by(DocumentChunk.chunk_index).all()\n",
    "                \n",
    "                doc_dict['chunks'] = [{\n",
    "                    'chunk_id': chunk.chunk_id,\n",
    "                    'content': chunk.content,\n",
    "                    'chunk_index': chunk.chunk_index,\n",
    "                    'word_count': chunk.word_count,\n",
    "                    'semantic_type': chunk.semantic_type\n",
    "                } for chunk in chunks]\n",
    "            \n",
    "            return doc_dict\n",
    "            \n",
    "        finally:\n",
    "            session.close()\n",
    "    \n",
    "    def list_documents(self, document_type: Optional[str] = None, \n",
    "                      category: Optional[str] = None,\n",
    "                      limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"ÂàóÂá∫ÊñáÊ™î\"\"\"\n",
    "        session = self.get_session()\n",
    "        \n",
    "        try:\n",
    "            query = session.query(EnterpriseDocument)\n",
    "            \n",
    "            if document_type:\n",
    "                query = query.filter(EnterpriseDocument.document_type == document_type)\n",
    "            \n",
    "            if category:\n",
    "                query = query.filter(EnterpriseDocument.category == category)\n",
    "            \n",
    "            documents = query.order_by(EnterpriseDocument.processed_date.desc()).limit(limit).all()\n",
    "            \n",
    "            return [{\n",
    "                'document_id': doc.document_id,\n",
    "                'title': doc.title,\n",
    "                'document_type': doc.document_type,\n",
    "                'category': doc.category,\n",
    "                'authors': doc.authors or [],\n",
    "                'word_count': doc.word_count,\n",
    "                'quality_score': doc.quality_score,\n",
    "                'processed_date': doc.processed_date\n",
    "            } for doc in documents]\n",
    "            \n",
    "        finally:\n",
    "            session.close()\n",
    "    \n",
    "    def update_quality_score(self, document_id: str, quality_score: float) -> bool:\n",
    "        \"\"\"Êõ¥Êñ∞ÊñáÊ™îÂìÅË≥™ÂàÜÊï∏\"\"\"\n",
    "        session = self.get_session()\n",
    "        \n",
    "        try:\n",
    "            document = session.query(EnterpriseDocument).filter(\n",
    "                EnterpriseDocument.document_id == document_id\n",
    "            ).first()\n",
    "            \n",
    "            if document:\n",
    "                old_score = document.quality_score\n",
    "                document.quality_score = quality_score\n",
    "                session.commit()\n",
    "                \n",
    "                # Ë®òÈåÑË°ÄÁ∑£\n",
    "                self._record_lineage(session, document_id, 'quality_update', 'update_score',\n",
    "                                   {'old_score': old_score}, {'new_score': quality_score})\n",
    "                \n",
    "                print(f\"‚úÖ ÂìÅË≥™ÂàÜÊï∏Â∑≤Êõ¥Êñ∞: {old_score} ‚Üí {quality_score}\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"‚ùå Êõ¥Êñ∞ÂìÅË≥™ÂàÜÊï∏Â§±Êïó: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "print(\"‚úÖ EnterpriseMetadataManager ÈÉ®ÂàÜÊñπÊ≥ïÂÆöÁæ©ÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ËºîÂä©ÊñπÊ≥ïÂØ¶‰Ωú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EnterpriseMetadataManager ËºîÂä©ÊñπÊ≥ïÂÆöÁæ©ÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "# ÁπºÁ∫å EnterpriseMetadataManager ÁöÑËºîÂä©ÊñπÊ≥ï\n",
    "\n",
    "def _generate_document_id(self, file_path: str, title: str) -> str:\n",
    "    \"\"\"ÁîüÊàêÂîØ‰∏ÄÊñáÊ™î ID\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    content = f\"{file_path}_{title}_{timestamp}\"\n",
    "    return hashlib.md5(content.encode()).hexdigest()[:16]\n",
    "\n",
    "def _extract_file_info(self, file_path: str) -> Dict:\n",
    "    \"\"\"ÊèêÂèñÊ™îÊ°àÂü∫Êú¨Ë≥áË®ä\"\"\"\n",
    "    path = Path(file_path)\n",
    "    \n",
    "    if path.exists():\n",
    "        stats = path.stat()\n",
    "        return {\n",
    "            'name': path.name,\n",
    "            'size': stats.st_size,\n",
    "            'format': path.suffix.lower().lstrip('.'),\n",
    "            'created_date': datetime.fromtimestamp(stats.st_ctime),\n",
    "            'modified_date': datetime.fromtimestamp(stats.st_mtime)\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'name': path.name,\n",
    "            'size': 0,\n",
    "            'format': path.suffix.lower().lstrip('.'),\n",
    "            'created_date': datetime.now(),\n",
    "            'modified_date': datetime.now()\n",
    "        }\n",
    "\n",
    "def _analyze_content(self, content: str) -> Dict:\n",
    "    \"\"\"ÂàÜÊûêÂÖßÂÆπÁµ±Ë®àË≥áË®ä\"\"\"\n",
    "    if not content.strip():\n",
    "        return {'word_count': 0, 'paragraph_count': 0}\n",
    "    \n",
    "    words = content.split()\n",
    "    paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    return {\n",
    "        'word_count': len(words),\n",
    "        'paragraph_count': len(paragraphs)\n",
    "    }\n",
    "\n",
    "def _extract_semantic_info(self, content: str) -> Dict:\n",
    "    \"\"\"\n",
    "    ÊèêÂèñË™ûÁæ©Ë≥áË®ä\n",
    "    ÂåÖÂê´ÂØ¶È´î„ÄÅÈóúÈçµÂ≠ó„ÄÅ‰∏ªÈ°å\n",
    "    \"\"\"\n",
    "    if not content.strip():\n",
    "        return {'keywords': [], 'entities': {}, 'topics': []}\n",
    "    \n",
    "    # ‰ΩøÁî® spaCy ËôïÁêÜ\n",
    "    doc = self.nlp(content[:10000])  # ÈôêÂà∂Èï∑Â∫¶ÈÅøÂÖçË®òÊÜ∂È´îÂïèÈ°å\n",
    "    \n",
    "    # ÊèêÂèñÂëΩÂêçÂØ¶È´î\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        ent_type = ent.label_\n",
    "        if ent_type not in entities:\n",
    "            entities[ent_type] = []\n",
    "        if ent.text not in entities[ent_type]:\n",
    "            entities[ent_type].append(ent.text)\n",
    "    \n",
    "    # ÊèêÂèñÈóúÈçµË©ûÔºàÂêçË©ûÁü≠Ë™ûÔºâ\n",
    "    keywords = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if 2 <= len(chunk.text.split()) <= 4:  # 2-4Ë©ûÁöÑÂêçË©ûÁü≠Ë™û\n",
    "            keywords.append(chunk.text.lower())\n",
    "    \n",
    "    # ÂéªÈáç‰∏¶ÈôêÂà∂Êï∏Èáè\n",
    "    keywords = list(set(keywords))[:20]\n",
    "    \n",
    "    # Á∞°ÂåñÁöÑ‰∏ªÈ°åÊèêÂèñÔºàÂü∫ÊñºÈ†ªÁπÅË©ûÔºâ\n",
    "    important_words = [token.lemma_.lower() for token in doc \n",
    "                      if token.is_alpha and not token.is_stop and len(token.text) > 3]\n",
    "    \n",
    "    from collections import Counter\n",
    "    word_freq = Counter(important_words)\n",
    "    topics = [word for word, count in word_freq.most_common(10) if count >= 2]\n",
    "    \n",
    "    return {\n",
    "        'keywords': keywords,\n",
    "        'entities': entities,\n",
    "        'topics': topics\n",
    "    }\n",
    "\n",
    "def _record_lineage(self, session, document_id: str, stage: str, operation: str, \n",
    "                   input_data: Dict, output_data: Dict) -> None:\n",
    "    \"\"\"Ë®òÈåÑË≥áÊñôË°ÄÁ∑£\"\"\"\n",
    "    lineage_id = f\"{document_id}_{stage}_{operation}_{int(datetime.now().timestamp())}\"\n",
    "    \n",
    "    lineage = DataLineage(\n",
    "        lineage_id=lineage_id,\n",
    "        document_id=document_id,\n",
    "        stage=stage,\n",
    "        operation=operation,\n",
    "        input_data=input_data,\n",
    "        output_data=output_data,\n",
    "        processor='EnterpriseMetadataManager',\n",
    "        processor_version='1.0',\n",
    "        success=1\n",
    "    )\n",
    "    \n",
    "    session.add(lineage)\n",
    "    session.commit()\n",
    "\n",
    "def get_lineage_history(self, document_id: str) -> List[Dict]:\n",
    "    \"\"\"Áç≤ÂèñÊñáÊ™îÁöÑË°ÄÁ∑£Ê≠∑Âè≤\"\"\"\n",
    "    session = self.get_session()\n",
    "    \n",
    "    try:\n",
    "        lineages = session.query(DataLineage).filter(\n",
    "            DataLineage.document_id == document_id\n",
    "        ).order_by(DataLineage.timestamp.asc()).all()\n",
    "        \n",
    "        return [{\n",
    "            'lineage_id': lineage.lineage_id,\n",
    "            'stage': lineage.stage,\n",
    "            'operation': lineage.operation,\n",
    "            'processor': lineage.processor,\n",
    "            'timestamp': lineage.timestamp,\n",
    "            'success': lineage.success,\n",
    "            'input_data': lineage.input_data,\n",
    "            'output_data': lineage.output_data\n",
    "        } for lineage in lineages]\n",
    "        \n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "def generate_statistics(self) -> Dict:\n",
    "    \"\"\"ÁîüÊàêÂÖÉË≥áÊñôÁµ±Ë®àÂ†±Âëä\"\"\"\n",
    "    session = self.get_session()\n",
    "    \n",
    "    try:\n",
    "        # Âü∫Êú¨Áµ±Ë®à\n",
    "        total_docs = session.query(EnterpriseDocument).count()\n",
    "        total_chunks = session.query(DocumentChunk).count()\n",
    "        \n",
    "        # ÊåâÈ°ûÂûãÁµ±Ë®à\n",
    "        type_stats = session.query(\n",
    "            EnterpriseDocument.document_type,\n",
    "            func.count(EnterpriseDocument.document_id).label('count')\n",
    "        ).group_by(EnterpriseDocument.document_type).all()\n",
    "        \n",
    "        # ÊåâÁãÄÊÖãÁµ±Ë®à\n",
    "        status_stats = session.query(\n",
    "            EnterpriseDocument.processing_status,\n",
    "            func.count(EnterpriseDocument.document_id).label('count')\n",
    "        ).group_by(EnterpriseDocument.processing_status).all()\n",
    "        \n",
    "        # ÂìÅË≥™ÂàÜÂ∏É\n",
    "        quality_avg = session.query(\n",
    "            func.avg(EnterpriseDocument.quality_score)\n",
    "        ).scalar() or 0.0\n",
    "        \n",
    "        return {\n",
    "            'total_documents': total_docs,\n",
    "            'total_chunks': total_chunks,\n",
    "            'avg_chunks_per_doc': total_chunks / total_docs if total_docs > 0 else 0,\n",
    "            'document_types': {row.document_type: row.count for row in type_stats},\n",
    "            'processing_status': {row.processing_status: row.count for row in status_stats},\n",
    "            'average_quality_score': round(quality_avg, 3)\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Â∞áÊñπÊ≥ïÊ∑ªÂä†Âà∞ EnterpriseMetadataManager È°ûÂà•\n",
    "from sqlalchemy import func\n",
    "\n",
    "EnterpriseMetadataManager._generate_document_id = _generate_document_id\n",
    "EnterpriseMetadataManager._extract_file_info = _extract_file_info\n",
    "EnterpriseMetadataManager._analyze_content = _analyze_content\n",
    "EnterpriseMetadataManager._extract_semantic_info = _extract_semantic_info\n",
    "EnterpriseMetadataManager._record_lineage = _record_lineage\n",
    "EnterpriseMetadataManager.get_lineage_history = get_lineage_history\n",
    "EnterpriseMetadataManager.generate_statistics = generate_statistics\n",
    "\n",
    "print(\"‚úÖ EnterpriseMetadataManager ËºîÂä©ÊñπÊ≥ïÂÆöÁæ©ÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Â§öÂ±§Á¥¢ÂºïÁ≥ªÁµ±ÂØ¶‰Ωú\n",
    "\n",
    "### ÂêëÈáèÁ¥¢ÂºïÁÆ°ÁêÜÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VectorIndexManager ÂÆöÁæ©ÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "class VectorIndexManager:\n",
    "    \"\"\"\n",
    "    ÂêëÈáèÁ¥¢ÂºïÁÆ°ÁêÜÂô®\n",
    "    ‰ΩøÁî® ChromaDB ÁÆ°ÁêÜÂêëÈáèÂµåÂÖ•ÂíåË™ûÁæ©ÊêúÁ¥¢\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chroma_path: str,\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 collection_name: str = \"document_chunks\"):\n",
    "        \"\"\"\n",
    "        ÂàùÂßãÂåñÂêëÈáèÁ¥¢ÂºïÁÆ°ÁêÜÂô®\n",
    "        \n",
    "        Args:\n",
    "            chroma_path: ChromaDB Â≠òÂÑ≤Ë∑ØÂæë\n",
    "            embedding_model: ÂµåÂÖ•Ê®°ÂûãÂêçÁ®±\n",
    "            collection_name: ÈõÜÂêàÂêçÁ®±\n",
    "        \"\"\"\n",
    "        self.chroma_path = chroma_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # ÂàùÂßãÂåñÂµåÂÖ•Ê®°Âûã\n",
    "        print(f\"üîÑ ËºâÂÖ•ÂµåÂÖ•Ê®°Âûã: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        print(f\"‚úÖ ÂµåÂÖ•Ê®°ÂûãËºâÂÖ•ÂÆåÊàê\")\n",
    "        \n",
    "        # ÂàùÂßãÂåñ ChromaDB\n",
    "        self.client = chromadb.PersistentClient(path=chroma_path)\n",
    "        \n",
    "        # Âª∫Á´ãÊàñÁç≤ÂèñÈõÜÂêà\n",
    "        try:\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"hnsw:space\": \"cosine\"},  # ‰ΩøÁî®È§òÂº¶Áõ∏‰ººÂ∫¶\n",
    "            )\n",
    "            print(f\"‚úÖ Âª∫Á´ãÊñ∞ÈõÜÂêà: {collection_name}\")\n",
    "        except Exception:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f\"‚úÖ ‰ΩøÁî®ÁèæÊúâÈõÜÂêà: {collection_name}\")\n",
    "        \n",
    "        print(f\"üìä ÈõÜÂêàÁµ±Ë®à: {self.collection.count()} ÂÄãÂêëÈáè\")\n",
    "    \n",
    "    def add_document_embeddings(self, document_id: str, chunks: List[Dict]) -> List[str]:\n",
    "        \"\"\"\n",
    "        ÁÇ∫ÊñáÊ™îÂàÜÂ°äÁîüÊàê‰∏¶Â≠òÂÑ≤ÂêëÈáèÂµåÂÖ•\n",
    "        \n",
    "        Args:\n",
    "            document_id: ÊñáÊ™î ID\n",
    "            chunks: ÂàÜÂ°äË≥áÊñôÂàóË°®\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Â∑≤ËôïÁêÜÁöÑÂàÜÂ°ä ID ÂàóË°®\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        print(f\"üîÑ ËôïÁêÜ {len(chunks)} ÂÄãÂàÜÂ°äÁöÑÂêëÈáèÂµåÂÖ•...\")\n",
    "        \n",
    "        # Ê∫ñÂÇôË≥áÊñô\n",
    "        chunk_ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_id = chunk.get('chunk_id') or f\"{document_id}_chunk_{chunk['chunk_index']}\"\n",
    "            content = chunk['content']\n",
    "            \n",
    "            # ÈÅéÊøæÂ§™Áü≠ÁöÑÂÖßÂÆπ\n",
    "            if len(content.split()) < 5:\n",
    "                continue\n",
    "            \n",
    "            chunk_ids.append(chunk_id)\n",
    "            documents.append(content)\n",
    "            metadatas.append({\n",
    "                'document_id': document_id,\n",
    "                'chunk_index': chunk['chunk_index'],\n",
    "                'word_count': chunk.get('word_count', len(content.split())),\n",
    "                'semantic_type': chunk.get('semantic_type', 'text'),\n",
    "                'embedding_model': self.embedding_model_name,\n",
    "                'created_date': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        if not chunk_ids:\n",
    "            print(\"‚ö†Ô∏è  Ê≤íÊúâÊúâÊïàÁöÑÂàÜÂ°äÂèØËôïÁêÜ\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # ÊâπÊ¨°ÁîüÊàêÂµåÂÖ•ÂêëÈáè\n",
    "            embeddings = self.embedding_model.encode(\n",
    "                documents, \n",
    "                batch_size=32,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            # Ê∑ªÂä†Âà∞ ChromaDB\n",
    "            self.collection.add(\n",
    "                ids=chunk_ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings.tolist(),\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ ÊàêÂäüÊ∑ªÂä† {len(chunk_ids)} ÂÄãÂêëÈáèÂµåÂÖ•\")\n",
    "            return chunk_ids\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ê∑ªÂä†ÂêëÈáèÂµåÂÖ•Â§±Êïó: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def semantic_search(self, \n",
    "                       query: str, \n",
    "                       top_k: int = 10,\n",
    "                       document_types: Optional[List[str]] = None,\n",
    "                       min_similarity: float = 0.0) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Âü∑Ë°åË™ûÁæ©ÊêúÁ¥¢\n",
    "        \n",
    "        Args:\n",
    "            query: ÊêúÁ¥¢Êü•Ë©¢\n",
    "            top_k: ËøîÂõûÁµêÊûúÊï∏Èáè\n",
    "            document_types: ÊñáÊ™îÈ°ûÂûãÈÅéÊøæ\n",
    "            min_similarity: ÊúÄÂ∞èÁõ∏‰ººÂ∫¶ÈñæÂÄº\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: ÊêúÁ¥¢ÁµêÊûú\n",
    "        \"\"\"\n",
    "        if not query.strip():\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # ÁîüÊàêÊü•Ë©¢ÂêëÈáè\n",
    "            query_embedding = self.embedding_model.encode([query])\n",
    "            \n",
    "            # ÊßãÂª∫ÈÅéÊøæÊ¢ù‰ª∂\n",
    "            where_filter = {}\n",
    "            if document_types:\n",
    "                # Ê≥®ÊÑèÔºöÈúÄË¶ÅÂæûÊñáÊ™îÈ°ûÂûãÊò†Â∞ÑÂà∞ÂØ¶ÈöõÁöÑ document_id ÈÅéÊøæ\n",
    "                # ÈÄôË£°Á∞°ÂåñËôïÁêÜÔºåÂØ¶ÈöõÊáâË©≤Êü•Ë©¢ÂÖÉË≥áÊñôÁÆ°ÁêÜÂô®Áç≤ÂèñÂ∞çÊáâÁöÑ document_id ÂàóË°®\n",
    "                pass\n",
    "            \n",
    "            # Âü∑Ë°åÊêúÁ¥¢\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_embedding.tolist(),\n",
    "                n_results=top_k,\n",
    "                where=where_filter if where_filter else None,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "            )\n",
    "            \n",
    "            # ËôïÁêÜÁµêÊûú\n",
    "            search_results = []\n",
    "            \n",
    "            for i in range(len(results['ids'][0])):\n",
    "                chunk_id = results['ids'][0][i]\n",
    "                document = results['documents'][0][i]\n",
    "                metadata = results['metadatas'][0][i]\n",
    "                distance = results['distances'][0][i]\n",
    "                \n",
    "                # ËΩâÊèõË∑ùÈõ¢ÁÇ∫Áõ∏‰ººÂ∫¶ÂàÜÊï∏ (cosine distance -> similarity)\n",
    "                similarity = 1.0 - distance\n",
    "                \n",
    "                # ÊáâÁî®Áõ∏‰ººÂ∫¶ÈñæÂÄºÈÅéÊøæ\n",
    "                if similarity >= min_similarity:\n",
    "                    search_results.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'document_id': metadata['document_id'],\n",
    "                        'content': document,\n",
    "                        'similarity_score': round(similarity, 4),\n",
    "                        'chunk_index': metadata.get('chunk_index', 0),\n",
    "                        'word_count': metadata.get('word_count', 0),\n",
    "                        'semantic_type': metadata.get('semantic_type', 'text')\n",
    "                    })\n",
    "            \n",
    "            print(f\"üîç Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ {len(search_results)} ÂÄãÁµêÊûú\")\n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ë™ûÁæ©ÊêúÁ¥¢Â§±Êïó: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_similar_chunks(self, chunk_id: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ÊâæÂà∞ËàáÊåáÂÆöÂàÜÂ°äÁõ∏‰ººÁöÑÂÖ∂‰ªñÂàÜÂ°ä\n",
    "        \n",
    "        Args:\n",
    "            chunk_id: ÂàÜÂ°ä ID\n",
    "            top_k: ËøîÂõûÁµêÊûúÊï∏Èáè\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Áõ∏‰ººÂàÜÂ°äÂàóË°®\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Áç≤ÂèñÁõÆÊ®ôÂàÜÂ°äÁöÑÂêëÈáè\n",
    "            target_result = self.collection.get(\n",
    "                ids=[chunk_id],\n",
    "                include=[\"embeddings\", \"documents\", \"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            if not target_result['ids']:\n",
    "                return []\n",
    "            \n",
    "            target_embedding = target_result['embeddings'][0]\n",
    "            \n",
    "            # ÊêúÁ¥¢Áõ∏‰ººÂêëÈáè\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[target_embedding],\n",
    "                n_results=top_k + 1,  # +1 Âõ†ÁÇ∫ÊúÉÂåÖÂê´Ëá™Â∑±\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "            )\n",
    "            \n",
    "            # ÈÅéÊøæÊéâËá™Â∑±ÔºåËøîÂõûÂÖ∂‰ªñÁõ∏‰ººÂàÜÂ°ä\n",
    "            similar_chunks = []\n",
    "            for i, returned_id in enumerate(results['ids'][0]):\n",
    "                if returned_id != chunk_id:\n",
    "                    similarity = 1.0 - results['distances'][0][i]\n",
    "                    similar_chunks.append({\n",
    "                        'chunk_id': returned_id,\n",
    "                        'document_id': results['metadatas'][0][i]['document_id'],\n",
    "                        'content': results['documents'][0][i],\n",
    "                        'similarity_score': round(similarity, 4)\n",
    "                    })\n",
    "            \n",
    "            return similar_chunks[:top_k]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Áç≤ÂèñÁõ∏‰ººÂàÜÂ°äÂ§±Êïó: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def delete_document_embeddings(self, document_id: str) -> bool:\n",
    "        \"\"\"Âà™Èô§ÊñáÊ™îÁöÑÊâÄÊúâÂêëÈáèÂµåÂÖ•\"\"\"\n",
    "        try:\n",
    "            # Êü•ÊâæÊñáÊ™îÁöÑÊâÄÊúâÂàÜÂ°ä\n",
    "            results = self.collection.get(\n",
    "                where={\"document_id\": document_id},\n",
    "                include=[\"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            if results['ids']:\n",
    "                self.collection.delete(ids=results['ids'])\n",
    "                print(f\"‚úÖ Â∑≤Âà™Èô§ÊñáÊ™î {document_id} ÁöÑ {len(results['ids'])} ÂÄãÂêëÈáè\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  ÊñáÊ™î {document_id} Ê≤íÊúâÂêëÈáèÂµåÂÖ•\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Âà™Èô§ÂêëÈáèÂµåÂÖ•Â§±Êïó: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_collection_stats(self) -> Dict:\n",
    "        \"\"\"Áç≤ÂèñÈõÜÂêàÁµ±Ë®àË≥áË®ä\"\"\"\n",
    "        try:\n",
    "            total_count = self.collection.count()\n",
    "            \n",
    "            # Áç≤Âèñ‰∏Ä‰∫õÊ®£Êú¨‰æÜÂàÜÊûê\n",
    "            sample_size = min(100, total_count)\n",
    "            if sample_size > 0:\n",
    "                sample = self.collection.get(\n",
    "                    limit=sample_size,\n",
    "                    include=[\"metadatas\"]\n",
    "                )\n",
    "                \n",
    "                # Áµ±Ë®àÊñáÊ™îÂàÜÂ∏É\n",
    "                doc_counts = {}\n",
    "                semantic_types = {}\n",
    "                \n",
    "                for metadata in sample['metadatas']:\n",
    "                    doc_id = metadata['document_id']\n",
    "                    doc_counts[doc_id] = doc_counts.get(doc_id, 0) + 1\n",
    "                    \n",
    "                    sem_type = metadata.get('semantic_type', 'text')\n",
    "                    semantic_types[sem_type] = semantic_types.get(sem_type, 0) + 1\n",
    "                \n",
    "                return {\n",
    "                    'total_vectors': total_count,\n",
    "                    'embedding_model': self.embedding_model_name,\n",
    "                    'collection_name': self.collection_name,\n",
    "                    'unique_documents': len(doc_counts),\n",
    "                    'avg_chunks_per_doc': np.mean(list(doc_counts.values())),\n",
    "                    'semantic_type_distribution': semantic_types\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'total_vectors': 0,\n",
    "                    'embedding_model': self.embedding_model_name,\n",
    "                    'collection_name': self.collection_name\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Áç≤ÂèñÁµ±Ë®àË≥áË®äÂ§±Êïó: {e}\")\n",
    "            return {}\n",
    "\n",
    "print(\"‚úÖ VectorIndexManager ÂÆöÁæ©ÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ ÂØ¶‰ΩúÁ∑¥Áøí: ‰ºÅÊ•≠ÂÖÉË≥áÊñôÁ≥ªÁµ±Ê∏¨Ë©¶\n",
    "\n",
    "### Âª∫Á´ãÊ∏¨Ë©¶Áí∞Â¢É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Ê∏¨Ë©¶ÁõÆÈåÑ: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance/notebooks/02_metadata_management/test_data\n",
      "üóÑÔ∏è  Ë≥áÊñôÂ∫´Ë∑ØÂæë: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance/notebooks/02_metadata_management/test_data/enterprise_metadata.db\n",
      "üîç ÂêëÈáèÁ¥¢ÂºïË∑ØÂæë: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance/notebooks/02_metadata_management/test_data/vector_index\n",
      "‚úÖ Âª∫Á´ã‰∫Ü 3 ÂÄãÊ∏¨Ë©¶ÊñáÊ™î\n"
     ]
    }
   ],
   "source": [
    "# Ë®≠ÂÆöÊ∏¨Ë©¶Áí∞Â¢ÉË∑ØÂæë\n",
    "test_dir = PROJECT_ROOT / 'notebooks' / '02_metadata_management' / 'test_data'\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "db_path = test_dir / 'enterprise_metadata.db'\n",
    "chroma_path = test_dir / 'vector_index'\n",
    "\n",
    "print(f\"üìÇ Ê∏¨Ë©¶ÁõÆÈåÑ: {test_dir}\")\n",
    "print(f\"üóÑÔ∏è  Ë≥áÊñôÂ∫´Ë∑ØÂæë: {db_path}\")\n",
    "print(f\"üîç ÂêëÈáèÁ¥¢ÂºïË∑ØÂæë: {chroma_path}\")\n",
    "\n",
    "# Âª∫Á´ãÊ∏¨Ë©¶ÊñáÊ™î\n",
    "test_documents = {\n",
    "    'transformer_paper.txt': \"\"\"\n",
    "Attention Is All You Need\n",
    "\n",
    "Abstract\n",
    "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\n",
    "1. Introduction\n",
    "Recurrent neural networks, long short-term memory and gated recurrent units in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
    "\n",
    "The goal of reducing sequential computation also leads to the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.\n",
    "\n",
    "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
    "\n",
    "2. Background\n",
    "The goal of reducing sequential computation also motivated the creation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building blocks.\n",
    "\n",
    "3. Model Architecture\n",
    "Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence to a sequence of continuous representations, which is then mapped to an output sequence by the decoder.\n",
    "\n",
    "4. Why Self-Attention\n",
    "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence.\n",
    "\"\"\",\n",
    "    \n",
    "    'bert_paper.txt': \"\"\"\n",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "Abstract\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
    "\n",
    "1. Introduction\n",
    "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically.\n",
    "\n",
    "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach uses task-specific architectures that include the pre-trained representations as additional features.\n",
    "\n",
    "2. Related Work\n",
    "There has been a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.\n",
    "\n",
    "2.1 Unsupervised Feature-based Approaches\n",
    "Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods.\n",
    "\n",
    "2.2 Unsupervised Fine-tuning Approaches\n",
    "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text.\n",
    "\n",
    "3. BERT\n",
    "We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning.\n",
    "\"\"\",\n",
    "    \n",
    "    'ai_report.txt': \"\"\"\n",
    "Artificial Intelligence Market Analysis Report 2024\n",
    "\n",
    "Executive Summary\n",
    "The artificial intelligence market has experienced unprecedented growth in 2024, with global revenue reaching $180 billion, representing a 32% increase from the previous year.\n",
    "\n",
    "Market Overview\n",
    "The AI market is characterized by rapid technological advancement and increasing enterprise adoption across various industries. Machine learning and deep learning technologies continue to drive innovation.\n",
    "\n",
    "Key Trends\n",
    "1. Large Language Models (LLMs) have dominated the market with breakthrough applications in natural language processing.\n",
    "2. Computer vision technologies have seen significant improvements in accuracy and efficiency.\n",
    "3. Edge AI deployment has increased by 45% as organizations seek to reduce latency and improve privacy.\n",
    "\n",
    "Industry Applications\n",
    "Healthcare: AI-powered diagnostic tools have shown 95% accuracy in medical image analysis.\n",
    "Finance: Algorithmic trading and fraud detection systems have reduced financial losses by 28%.\n",
    "Automotive: Autonomous vehicle technology has advanced with Level 3 automation becoming commercially viable.\n",
    "\n",
    "Market Projections\n",
    "We project the AI market to reach $300 billion by 2026, driven by enterprise AI adoption and continued research breakthroughs.\n",
    "\n",
    "Recommendations\n",
    "1. Invest in AI talent development and training programs.\n",
    "2. Develop comprehensive AI governance frameworks.\n",
    "3. Focus on explainable AI to build trust and compliance.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Âª∫Á´ãÊ∏¨Ë©¶Ê™îÊ°à\n",
    "for filename, content in test_documents.items():\n",
    "    file_path = test_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"‚úÖ Âª∫Á´ã‰∫Ü {len(test_documents)} ÂÄãÊ∏¨Ë©¶ÊñáÊ™î\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂàùÂßãÂåñÁÆ°ÁêÜÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ë≥áÊñôÂ∫´Ë°®Â∑≤Âª∫Á´ã: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance/notebooks/02_metadata_management/test_data/enterprise_metadata.db\n",
      "‚úÖ NLP Ê®°ÂûãËºâÂÖ•ÂÆåÊàê\n",
      "üîÑ ËºâÂÖ•ÂµåÂÖ•Ê®°Âûã: all-MiniLM-L6-v2\n",
      "‚úÖ ÂµåÂÖ•Ê®°ÂûãËºâÂÖ•ÂÆåÊàê\n",
      "‚úÖ ‰ΩøÁî®ÁèæÊúâÈõÜÂêà: document_chunks\n",
      "üìä ÈõÜÂêàÁµ±Ë®à: 0 ÂÄãÂêëÈáè\n",
      "‚úÖ ÁÆ°ÁêÜÂô®ÂàùÂßãÂåñÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "# ÂàùÂßãÂåñÂÖÉË≥áÊñôÁÆ°ÁêÜÂô®\n",
    "metadata_manager = EnterpriseMetadataManager(str(db_path), create_tables=True)\n",
    "\n",
    "# ÂàùÂßãÂåñÂêëÈáèÁ¥¢ÂºïÁÆ°ÁêÜÂô®\n",
    "vector_manager = VectorIndexManager(str(chroma_path))\n",
    "\n",
    "print(\"‚úÖ ÁÆ°ÁêÜÂô®ÂàùÂßãÂåñÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ê∏¨Ë©¶ÊñáÊ™îËôïÁêÜÊµÅÁ®ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ÈñãÂßãËôïÁêÜÊ∏¨Ë©¶ÊñáÊ™î\n",
      "\n",
      "‚úÖ ÊñáÊ™îÂ∑≤Âª∫Á´ã: 5e2046078b2618a9\n",
      "‚úÖ Â∑≤Ê∑ªÂä† 7 ÂÄãÂàÜÂ°ä\n",
      "üîÑ ËôïÁêÜ 7 ÂÄãÂàÜÂ°äÁöÑÂêëÈáèÂµåÂÖ•...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d71729fddb84d93a76046eb52ff898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÊàêÂäüÊ∑ªÂä† 7 ÂÄãÂêëÈáèÂµåÂÖ•\n",
      "‚úÖ Â∑≤ËôïÁêÜ: transformer_paper.txt (ÊñáÊ™îID: 5e2046078b2618a9, ÂàÜÂ°ä: 7)\n",
      "\n",
      "‚úÖ ÊñáÊ™îÂ∑≤Âª∫Á´ã: f088c9e9011861a7\n",
      "‚úÖ Â∑≤Ê∑ªÂä† 7 ÂÄãÂàÜÂ°ä\n",
      "üîÑ ËôïÁêÜ 7 ÂÄãÂàÜÂ°äÁöÑÂêëÈáèÂµåÂÖ•...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bddd47ed02d4eb6826eb98c6b12b6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÊàêÂäüÊ∑ªÂä† 7 ÂÄãÂêëÈáèÂµåÂÖ•\n",
      "‚úÖ Â∑≤ËôïÁêÜ: bert_paper.txt (ÊñáÊ™îID: f088c9e9011861a7, ÂàÜÂ°ä: 7)\n",
      "\n",
      "‚úÖ ÊñáÊ™îÂ∑≤Âª∫Á´ã: 19b7b90a4bb6d87c\n",
      "‚úÖ Â∑≤Ê∑ªÂä† 6 ÂÄãÂàÜÂ°ä\n",
      "üîÑ ËôïÁêÜ 6 ÂÄãÂàÜÂ°äÁöÑÂêëÈáèÂµåÂÖ•...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8a7d6a5cfc4b4fa6f2c5f67be60322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÊàêÂäüÊ∑ªÂä† 6 ÂÄãÂêëÈáèÂµåÂÖ•\n",
      "‚úÖ Â∑≤ËôïÁêÜ: ai_report.txt (ÊñáÊ™îID: 19b7b90a4bb6d87c, ÂàÜÂ°ä: 6)\n",
      "\n",
      "üéâ ÊâÄÊúâÊñáÊ™îËôïÁêÜÂÆåÊàêÔºÅ\n",
      "üìÑ transformer_paper.txt: 7 ÂÄãÂàÜÂ°ä, 7 ÂÄãÂêëÈáè\n",
      "üìÑ bert_paper.txt: 7 ÂÄãÂàÜÂ°ä, 7 ÂÄãÂêëÈáè\n",
      "üìÑ ai_report.txt: 6 ÂÄãÂàÜÂ°ä, 6 ÂÄãÂêëÈáè\n"
     ]
    }
   ],
   "source": [
    "# ËôïÁêÜÊ∏¨Ë©¶ÊñáÊ™î\n",
    "print(\"üîÑ ÈñãÂßãËôïÁêÜÊ∏¨Ë©¶ÊñáÊ™î\\n\")\n",
    "\n",
    "# ÂÆöÁæ©ÊñáÊ™îÂÖÉË≥áÊñô\n",
    "documents_metadata = [\n",
    "    {\n",
    "        'file': 'transformer_paper.txt',\n",
    "        'metadata': DocumentMetadataRequest(\n",
    "            title=\"Attention Is All You Need\",\n",
    "            document_type=\"academic_paper\",\n",
    "            category=\"deep_learning\",\n",
    "            authors=[\"Ashish Vaswani\", \"Noam Shazeer\", \"Niki Parmar\"],\n",
    "            organization=\"Google Research\",\n",
    "            keywords=[\"transformer\", \"attention mechanism\", \"neural networks\"],\n",
    "            custom_metadata={\n",
    "                \"venue\": \"NIPS 2017\",\n",
    "                \"citation_count\": 50000,\n",
    "                \"field\": \"Natural Language Processing\"\n",
    "            }\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        'file': 'bert_paper.txt',\n",
    "        'metadata': DocumentMetadataRequest(\n",
    "            title=\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
    "            document_type=\"academic_paper\",\n",
    "            category=\"deep_learning\",\n",
    "            authors=[\"Jacob Devlin\", \"Ming-Wei Chang\", \"Kenton Lee\"],\n",
    "            organization=\"Google AI Language\",\n",
    "            keywords=[\"BERT\", \"bidirectional\", \"transformers\", \"pre-training\"],\n",
    "            custom_metadata={\n",
    "                \"venue\": \"NAACL 2019\",\n",
    "                \"citation_count\": 40000,\n",
    "                \"field\": \"Natural Language Processing\"\n",
    "            }\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        'file': 'ai_report.txt',\n",
    "        'metadata': DocumentMetadataRequest(\n",
    "            title=\"Artificial Intelligence Market Analysis Report 2024\",\n",
    "            document_type=\"business_report\",\n",
    "            category=\"market_analysis\",\n",
    "            authors=[\"AI Research Team\"],\n",
    "            organization=\"Tech Analytics Corp\",\n",
    "            keywords=[\"AI market\", \"machine learning\", \"industry trends\"],\n",
    "            custom_metadata={\n",
    "                \"report_type\": \"market_analysis\",\n",
    "                \"year\": 2024,\n",
    "                \"industry\": \"Technology\"\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "processed_documents = []\n",
    "\n",
    "for doc_info in documents_metadata:\n",
    "    file_path = test_dir / doc_info['file']\n",
    "    \n",
    "    # ËÆÄÂèñÊñáÊ™îÂÖßÂÆπ\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Âª∫Á´ãÊñáÊ™îË®òÈåÑ\n",
    "    document_id = metadata_manager.create_document(\n",
    "        str(file_path), content, doc_info['metadata']\n",
    "    )\n",
    "    \n",
    "    # Á∞°ÂñÆÂàÜÂ°äËôïÁêÜÔºàÊåâÊÆµËêΩÔºâ\n",
    "    paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks_data = []\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        if len(paragraph.split()) >= 10:  # ÈÅéÊøæÂ§™Áü≠ÁöÑÊÆµËêΩ\n",
    "            chunks_data.append(DocumentChunkRequest(\n",
    "                content=paragraph,\n",
    "                chunk_index=i,\n",
    "                semantic_type=\"text\"\n",
    "            ))\n",
    "    \n",
    "    # Ê∑ªÂä†ÂàÜÂ°ä\n",
    "    chunk_ids = metadata_manager.add_document_chunks(document_id, chunks_data)\n",
    "    \n",
    "    # ÁÇ∫ÂêëÈáèÁ¥¢ÂºïÊ∫ñÂÇôÂàÜÂ°äË≥áÊñô\n",
    "    chunk_dicts = [{\n",
    "        'chunk_id': chunk_ids[i],\n",
    "        'content': chunk.content,\n",
    "        'chunk_index': chunk.chunk_index,\n",
    "        'semantic_type': chunk.semantic_type\n",
    "    } for i, chunk in enumerate(chunks_data)]\n",
    "    \n",
    "    # Ê∑ªÂä†ÂêëÈáèÂµåÂÖ•\n",
    "    vector_chunk_ids = vector_manager.add_document_embeddings(document_id, chunk_dicts)\n",
    "    \n",
    "    processed_documents.append({\n",
    "        'document_id': document_id,\n",
    "        'file': doc_info['file'],\n",
    "        'chunk_count': len(chunk_ids),\n",
    "        'vector_count': len(vector_chunk_ids)\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Â∑≤ËôïÁêÜ: {doc_info['file']} (ÊñáÊ™îID: {document_id}, ÂàÜÂ°ä: {len(chunk_ids)})\\n\")\n",
    "\n",
    "print(f\"üéâ ÊâÄÊúâÊñáÊ™îËôïÁêÜÂÆåÊàêÔºÅ\")\n",
    "for doc in processed_documents:\n",
    "    print(f\"üìÑ {doc['file']}: {doc['chunk_count']} ÂÄãÂàÜÂ°ä, {doc['vector_count']} ÂÄãÂêëÈáè\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ê∏¨Ë©¶ÊêúÁ¥¢ÂäüËÉΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ê∏¨Ë©¶ÊêúÁ¥¢ÂäüËÉΩ\n",
      "\n",
      "üîé Êü•Ë©¢: 'attention mechanism in neural networks'\n",
      "üîç Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ 3 ÂÄãÁµêÊûú\n",
      "ÊâæÂà∞ 3 ÂÄãÁõ∏ÈóúÁµêÊûú:\n",
      "\n",
      "  1. Áõ∏‰ººÂ∫¶: 0.568\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_7\n",
      "     ÂÖßÂÆπ: 4. Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the rec...\n",
      "\n",
      "  2. Áõ∏‰ººÂ∫¶: 0.521\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_1\n",
      "     ÂÖßÂÆπ: Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional n...\n",
      "\n",
      "  3. Áõ∏‰ººÂ∫¶: 0.390\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_6\n",
      "     ÂÖßÂÆπ: 3. Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder s...\n",
      "\n",
      "--------------------------------------------------\n",
      "üîé Êü•Ë©¢: 'transformer architecture and self-attention'\n",
      "üîç Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ 3 ÂÄãÁµêÊûú\n",
      "ÊâæÂà∞ 3 ÂÄãÁõ∏ÈóúÁµêÊûú:\n",
      "\n",
      "  1. Áõ∏‰ººÂ∫¶: 0.607\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_4\n",
      "     ÂÖßÂÆπ: In this work, we propose the Transformer, a model architecture eschewing recurrence and instead rely...\n",
      "\n",
      "  2. Áõ∏‰ººÂ∫¶: 0.547\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_7\n",
      "     ÂÖßÂÆπ: 4. Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the rec...\n",
      "\n",
      "  3. Áõ∏‰ººÂ∫¶: 0.489\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_1\n",
      "     ÂÖßÂÆπ: Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional n...\n",
      "\n",
      "--------------------------------------------------\n",
      "üîé Êü•Ë©¢: 'BERT bidirectional language model'\n",
      "üîç Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ 3 ÂÄãÁµêÊûú\n",
      "ÊâæÂà∞ 3 ÂÄãÁõ∏ÈóúÁµêÊûú:\n",
      "\n",
      "  1. Áõ∏‰ººÂ∫¶: 0.723\n",
      "     ÊñáÊ™î: f088c9e9011861a7\n",
      "     ÂàÜÂ°ä: f088c9e9011861a7_chunk_1\n",
      "     ÂÖßÂÆπ: Abstract\n",
      "We introduce a new language representation model called BERT, which stands for Bidirectiona...\n",
      "\n",
      "  2. Áõ∏‰ººÂ∫¶: 0.650\n",
      "     ÊñáÊ™î: f088c9e9011861a7\n",
      "     ÂàÜÂ°ä: f088c9e9011861a7_chunk_7\n",
      "     ÂÖßÂÆπ: 3. BERT\n",
      "We introduce BERT and its detailed implementation in this section. There are two steps in ou...\n",
      "\n",
      "  3. Áõ∏‰ººÂ∫¶: 0.471\n",
      "     ÊñáÊ™î: f088c9e9011861a7\n",
      "     ÂàÜÂ°ä: f088c9e9011861a7_chunk_4\n",
      "     ÂÖßÂÆπ: 2. Related Work\n",
      "There has been a long history of pre-training general language representations, and ...\n",
      "\n",
      "--------------------------------------------------\n",
      "üîé Êü•Ë©¢: 'artificial intelligence market trends'\n",
      "üîç Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ 3 ÂÄãÁµêÊûú\n",
      "ÊâæÂà∞ 3 ÂÄãÁõ∏ÈóúÁµêÊûú:\n",
      "\n",
      "  1. Áõ∏‰ººÂ∫¶: 0.692\n",
      "     ÊñáÊ™î: 19b7b90a4bb6d87c\n",
      "     ÂàÜÂ°ä: 19b7b90a4bb6d87c_chunk_2\n",
      "     ÂÖßÂÆπ: Market Overview\n",
      "The AI market is characterized by rapid technological advancement and increasing ent...\n",
      "\n",
      "  2. Áõ∏‰ººÂ∫¶: 0.675\n",
      "     ÊñáÊ™î: 19b7b90a4bb6d87c\n",
      "     ÂàÜÂ°ä: 19b7b90a4bb6d87c_chunk_1\n",
      "     ÂÖßÂÆπ: Executive Summary\n",
      "The artificial intelligence market has experienced unprecedented growth in 2024, w...\n",
      "\n",
      "  3. Áõ∏‰ººÂ∫¶: 0.619\n",
      "     ÊñáÊ™î: 19b7b90a4bb6d87c\n",
      "     ÂàÜÂ°ä: 19b7b90a4bb6d87c_chunk_5\n",
      "     ÂÖßÂÆπ: Market Projections\n",
      "We project the AI market to reach $300 billion by 2026, driven by enterprise AI a...\n",
      "\n",
      "--------------------------------------------------\n",
      "üîé Êü•Ë©¢: 'machine learning applications in healthcare'\n",
      "üîç Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ 3 ÂÄãÁµêÊûú\n",
      "ÊâæÂà∞ 3 ÂÄãÁõ∏ÈóúÁµêÊûú:\n",
      "\n",
      "  1. Áõ∏‰ººÂ∫¶: 0.525\n",
      "     ÊñáÊ™î: 19b7b90a4bb6d87c\n",
      "     ÂàÜÂ°ä: 19b7b90a4bb6d87c_chunk_4\n",
      "     ÂÖßÂÆπ: Industry Applications\n",
      "Healthcare: AI-powered diagnostic tools have shown 95% accuracy in medical ima...\n",
      "\n",
      "  2. Áõ∏‰ººÂ∫¶: 0.333\n",
      "     ÊñáÊ™î: 19b7b90a4bb6d87c\n",
      "     ÂàÜÂ°ä: 19b7b90a4bb6d87c_chunk_2\n",
      "     ÂÖßÂÆπ: Market Overview\n",
      "The AI market is characterized by rapid technological advancement and increasing ent...\n",
      "\n",
      "  3. Áõ∏‰ººÂ∫¶: 0.298\n",
      "     ÊñáÊ™î: 5e2046078b2618a9\n",
      "     ÂàÜÂ°ä: 5e2046078b2618a9_chunk_2\n",
      "     ÂÖßÂÆπ: 1. Introduction\n",
      "Recurrent neural networks, long short-term memory and gated recurrent units in parti...\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Ê∏¨Ë©¶ÊêúÁ¥¢ÂäüËÉΩ\\n\")\n",
    "\n",
    "# Ê∏¨Ë©¶Ë™ûÁæ©ÊêúÁ¥¢\n",
    "test_queries = [\n",
    "    \"attention mechanism in neural networks\",\n",
    "    \"transformer architecture and self-attention\",\n",
    "    \"BERT bidirectional language model\",\n",
    "    \"artificial intelligence market trends\",\n",
    "    \"machine learning applications in healthcare\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"üîé Êü•Ë©¢: '{query}'\")\n",
    "    \n",
    "    # Âü∑Ë°åË™ûÁæ©ÊêúÁ¥¢\n",
    "    results = vector_manager.semantic_search(\n",
    "        query=query,\n",
    "        top_k=3,\n",
    "        min_similarity=0.2\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(f\"ÊâæÂà∞ {len(results)} ÂÄãÁõ∏ÈóúÁµêÊûú:\\n\")\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. Áõ∏‰ººÂ∫¶: {result['similarity_score']:.3f}\")\n",
    "            print(f\"     ÊñáÊ™î: {result['document_id']}\")\n",
    "            print(f\"     ÂàÜÂ°ä: {result['chunk_id']}\")\n",
    "            print(f\"     ÂÖßÂÆπ: {result['content'][:100]}...\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"‚ùå Ê≤íÊúâÊâæÂà∞Áõ∏ÈóúÁµêÊûú\\n\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ê∏¨Ë©¶ÂÖÉË≥áÊñôÊü•Ë©¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ê∏¨Ë©¶ÂÖÉË≥áÊñôÊü•Ë©¢ÂäüËÉΩ\n",
      "\n",
      "üìö ÂÖ±Êúâ 4 ÂÄãÊñáÊ™î:\n",
      "  ‚Ä¢ Artificial Intelligence Market Analysis Report 2024 (business_report)\n",
      "    Â≠óÊï∏: 195, ÂìÅË≥™: 0.000\n",
      "  ‚Ä¢ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (academic_paper)\n",
      "    Â≠óÊï∏: 220, ÂìÅË≥™: 0.000\n",
      "  ‚Ä¢ Attention Is All You Need (academic_paper)\n",
      "    Â≠óÊï∏: 264, ÂìÅË≥™: 0.000\n",
      "  ‚Ä¢ Attention Is All You Need (academic_paper)\n",
      "    Â≠óÊï∏: 264, ÂìÅË≥™: 0.000\n",
      "\n",
      "============================================================\n",
      "üìñ Â≠∏Ë°ìË´ñÊñá: 3 ÁØá\n",
      "üìà ÂïÜÊ•≠Â†±Âëä: 1 ‰ªΩ\n",
      "\n",
      "============================================================\n",
      "üìÑ Ë©≥Á¥∞ÊñáÊ™îË≥áË®ä - Attention Is All You Need:\n",
      "  ‰ΩúËÄÖ: Ashish Vaswani, Noam Shazeer, Niki Parmar\n",
      "  ÁµÑÁπî: Google Research\n",
      "  È°ûÂûã: academic_paper\n",
      "  ÂàÜÈ°û: deep_learning\n",
      "  ÈóúÈçµÂ≠ó: transformer, attention mechanism, neural networks, why self-attention, attention mechanisms\n",
      "  ÂØ¶È´î: ['ORG', 'CARDINAL', 'PERSON']\n",
      "  ÂàÜÂ°äÊï∏Èáè: 7\n",
      "  Ëá™Ë®ÇÂÖÉË≥áÊñô: {'venue': 'NIPS 2017', 'citation_count': 50000, 'field': 'Natural Language Processing'}\n",
      "\n",
      "============================================================\n",
      "üìä Á≥ªÁµ±Áµ±Ë®àÂ†±Âëä:\n",
      "  total_documents: 4\n",
      "  total_chunks: 20\n",
      "  avg_chunks_per_doc: 5.0\n",
      "  document_types: {'academic_paper': 3, 'business_report': 1}\n",
      "  processing_status: {'completed': 4}\n",
      "  average_quality_score: 0.0\n",
      "\n",
      "============================================================\n",
      "üîç ÂêëÈáèÁ¥¢ÂºïÁµ±Ë®à:\n",
      "  total_vectors: 20\n",
      "  embedding_model: all-MiniLM-L6-v2\n",
      "  collection_name: document_chunks\n",
      "  unique_documents: 3\n",
      "  avg_chunks_per_doc: 6.666666666666667\n",
      "  semantic_type_distribution: {'text': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Ê∏¨Ë©¶ÂÖÉË≥áÊñôÊü•Ë©¢ÂäüËÉΩ\\n\")\n",
    "\n",
    "# ÂàóÂá∫ÊâÄÊúâÊñáÊ™î\n",
    "all_docs = metadata_manager.list_documents()\n",
    "print(f\"üìö ÂÖ±Êúâ {len(all_docs)} ÂÄãÊñáÊ™î:\")\n",
    "for doc in all_docs:\n",
    "    print(f\"  ‚Ä¢ {doc['title']} ({doc['document_type']})\")\n",
    "    print(f\"    Â≠óÊï∏: {doc['word_count']}, ÂìÅË≥™: {doc['quality_score']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ÊåâÈ°ûÂûãÊü•Ë©¢\n",
    "academic_papers = metadata_manager.list_documents(document_type=\"academic_paper\")\n",
    "business_reports = metadata_manager.list_documents(document_type=\"business_report\")\n",
    "\n",
    "print(f\"üìñ Â≠∏Ë°ìË´ñÊñá: {len(academic_papers)} ÁØá\")\n",
    "print(f\"üìà ÂïÜÊ•≠Â†±Âëä: {len(business_reports)} ‰ªΩ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Áç≤ÂèñË©≥Á¥∞ÊñáÊ™îË≥áË®ä\n",
    "if processed_documents:\n",
    "    sample_doc_id = processed_documents[0]['document_id']\n",
    "    detailed_doc = metadata_manager.get_document(sample_doc_id, include_chunks=True)\n",
    "    \n",
    "    print(f\"üìÑ Ë©≥Á¥∞ÊñáÊ™îË≥áË®ä - {detailed_doc['title']}:\")\n",
    "    print(f\"  ‰ΩúËÄÖ: {', '.join(detailed_doc['authors'])}\")\n",
    "    print(f\"  ÁµÑÁπî: {detailed_doc['organization']}\")\n",
    "    print(f\"  È°ûÂûã: {detailed_doc['document_type']}\")\n",
    "    print(f\"  ÂàÜÈ°û: {detailed_doc['category']}\")\n",
    "    print(f\"  ÈóúÈçµÂ≠ó: {', '.join(detailed_doc['keywords'][:5])}\")\n",
    "    print(f\"  ÂØ¶È´î: {list(detailed_doc['entities'].keys())}\")\n",
    "    print(f\"  ÂàÜÂ°äÊï∏Èáè: {len(detailed_doc.get('chunks', []))}\")\n",
    "    print(f\"  Ëá™Ë®ÇÂÖÉË≥áÊñô: {detailed_doc['custom_metadata']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ÁîüÊàêÁµ±Ë®àÂ†±Âëä\n",
    "stats = metadata_manager.generate_statistics()\n",
    "print(\"üìä Á≥ªÁµ±Áµ±Ë®àÂ†±Âëä:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ÂêëÈáèÁ¥¢ÂºïÁµ±Ë®à\n",
    "vector_stats = vector_manager.get_collection_stats()\n",
    "print(\"üîç ÂêëÈáèÁ¥¢ÂºïÁµ±Ë®à:\")\n",
    "for key, value in vector_stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ê∏¨Ë©¶Ë≥áÊñôË°ÄÁ∑£ËøΩËπ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Ê∏¨Ë©¶Ë≥áÊñôË°ÄÁ∑£ËøΩËπ§\n",
      "\n",
      "üìã ÊñáÊ™î 5e2046078b2618a9 ÁöÑËôïÁêÜÊ≠∑Á®ã:\n",
      "\n",
      "1. ÈöéÊÆµ: metadata_creation\n",
      "   Êìç‰Ωú: create_document\n",
      "   ËôïÁêÜÂô®: EnterpriseMetadataManager\n",
      "   ÊôÇÈñì: 2025-11-11 16:19:38.924093\n",
      "   ÊàêÂäü: ‚úÖ\n",
      "   Ëº∏ÂÖ•: {'input': {'title': 'Attention Is All You Need', 'subtitle': None, 'document_type': 'academic_paper'...\n",
      "   Ëº∏Âá∫: {'document_id': '5e2046078b2618a9'}...\n",
      "\n",
      "2. ÈöéÊÆµ: chunking\n",
      "   Êìç‰Ωú: add_chunks\n",
      "   ËôïÁêÜÂô®: EnterpriseMetadataManager\n",
      "   ÊôÇÈñì: 2025-11-11 16:19:38.996664\n",
      "   ÊàêÂäü: ‚úÖ\n",
      "   Ëº∏ÂÖ•: {'chunk_count': 7}...\n",
      "   Ëº∏Âá∫: {'chunk_ids': ['5e2046078b2618a9_chunk_1', '5e2046078b2618a9_chunk_2', '5e2046078b2618a9_chunk_3', '...\n",
      "\n",
      "============================================================\n",
      "üîç Êü•ÊâæËàáÂàÜÂ°ä 5e2046078b2618a9_chunk_1 Áõ∏‰ººÁöÑÂÖßÂÆπ:\n",
      "\n",
      "1. Áõ∏‰ººÂ∫¶: 0.729\n",
      "   ‰æÜÊ∫êÊñáÊ™î: 5e2046078b2618a9\n",
      "   ÂÖßÂÆπ: 3. Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence...\n",
      "\n",
      "2. Áõ∏‰ººÂ∫¶: 0.691\n",
      "   ‰æÜÊ∫êÊñáÊ™î: 5e2046078b2618a9\n",
      "   ÂÖßÂÆπ: 1. Introduction\n",
      "Recurrent neural networks, long short-term memory and gated recurrent units in particular, have been firmly established as state of th...\n",
      "\n",
      "3. Áõ∏‰ººÂ∫¶: 0.636\n",
      "   ‰æÜÊ∫êÊñáÊ™î: 5e2046078b2618a9\n",
      "   ÂÖßÂÆπ: In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw glo...\n",
      "\n",
      "üéâ ÊâÄÊúâÊ∏¨Ë©¶ÂÆåÊàêÔºÅ\n"
     ]
    }
   ],
   "source": [
    "print(\"üîó Ê∏¨Ë©¶Ë≥áÊñôË°ÄÁ∑£ËøΩËπ§\\n\")\n",
    "\n",
    "# Êü•ÁúãÊüêÂÄãÊñáÊ™îÁöÑËôïÁêÜÊ≠∑Á®ã\n",
    "if processed_documents:\n",
    "    sample_doc_id = processed_documents[0]['document_id']\n",
    "    lineage_history = metadata_manager.get_lineage_history(sample_doc_id)\n",
    "    \n",
    "    print(f\"üìã ÊñáÊ™î {sample_doc_id} ÁöÑËôïÁêÜÊ≠∑Á®ã:\")\n",
    "    \n",
    "    for i, record in enumerate(lineage_history, 1):\n",
    "        print(f\"\\n{i}. ÈöéÊÆµ: {record['stage']}\")\n",
    "        print(f\"   Êìç‰Ωú: {record['operation']}\")\n",
    "        print(f\"   ËôïÁêÜÂô®: {record['processor']}\")\n",
    "        print(f\"   ÊôÇÈñì: {record['timestamp']}\")\n",
    "        print(f\"   ÊàêÂäü: {'‚úÖ' if record['success'] else '‚ùå'}\")\n",
    "        \n",
    "        if record['input_data']:\n",
    "            print(f\"   Ëº∏ÂÖ•: {str(record['input_data'])[:100]}...\")\n",
    "        \n",
    "        if record['output_data']:\n",
    "            print(f\"   Ëº∏Âá∫: {str(record['output_data'])[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Ê∏¨Ë©¶Áõ∏‰ººÂàÜÂ°äÊü•Êâæ\n",
    "if processed_documents:\n",
    "    # Áç≤ÂèñÁ¨¨‰∏ÄÂÄãÊñáÊ™îÁöÑÁ¨¨‰∏ÄÂÄãÂàÜÂ°ä\n",
    "    doc_id = processed_documents[0]['document_id']\n",
    "    doc_detail = metadata_manager.get_document(doc_id, include_chunks=True)\n",
    "    \n",
    "    if doc_detail and doc_detail.get('chunks'):\n",
    "        first_chunk_id = doc_detail['chunks'][0]['chunk_id']\n",
    "        \n",
    "        print(f\"üîç Êü•ÊâæËàáÂàÜÂ°ä {first_chunk_id} Áõ∏‰ººÁöÑÂÖßÂÆπ:\")\n",
    "        \n",
    "        similar_chunks = vector_manager.get_similar_chunks(first_chunk_id, top_k=3)\n",
    "        \n",
    "        if similar_chunks:\n",
    "            for i, chunk in enumerate(similar_chunks, 1):\n",
    "                print(f\"\\n{i}. Áõ∏‰ººÂ∫¶: {chunk['similarity_score']:.3f}\")\n",
    "                print(f\"   ‰æÜÊ∫êÊñáÊ™î: {chunk['document_id']}\")\n",
    "                print(f\"   ÂÖßÂÆπ: {chunk['content'][:150]}...\")\n",
    "        else:\n",
    "            print(\"‚ùå Ê≤íÊúâÊâæÂà∞Áõ∏‰ººÁöÑÂàÜÂ°ä\")\n",
    "\n",
    "print(\"\\nüéâ ÊâÄÊúâÊ∏¨Ë©¶ÂÆåÊàêÔºÅ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
