{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡çµ„ 1: æ–‡æª”æ”å–èˆ‡è™•ç†åŸºç¤\n",
    "## Module 1: Document Ingestion and Processing Fundamentals\n",
    "\n",
    "> **æ•™å­¸ç›®æ¨™**: æŒæ¡ä¼æ¥­ç´šæ–‡æª”æ”å–æµç¨‹ã€äº†è§£ Docling æ·±åº¦æ•´åˆã€å¯¦ä½œèªç¾©åˆ†å¡Šæ¼”ç®—æ³•\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’æˆæœ (Learning Outcomes)\n",
    "\n",
    "å®Œæˆæœ¬æ¨¡çµ„å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š\n",
    "1. **è¨­è¨ˆæ–‡æª”æ”å–æ¶æ§‹** - å»ºç«‹å¯æ“´å±•çš„æ–‡æª”è™•ç†ç®¡ç·š\n",
    "2. **æ‡‰ç”¨ Docling é€²è¡Œæ–‡æª”è§£æ** - æŒæ¡ PDF/DOCX ç²¾ç¢ºæå–\n",
    "3. **å¯¦ä½œèªç¾©åˆ†å¡Šæ¼”ç®—æ³•** - åŸºæ–¼ embeddings çš„æ™ºèƒ½åˆ†å¡Š\n",
    "4. **å»ºç«‹å“è³ªæª¢æ¸¬æ©Ÿåˆ¶** - è‡ªå‹•åŒ–çš„æ–‡æª”å“è³ªè©•ä¼°\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ (Core Concepts)\n",
    "\n",
    "### 1. ä¼æ¥­ç´šæ–‡æª”æ”å–æŒ‘æˆ°\n",
    "- å¤šæ ¼å¼æ”¯æ´ (PDF, DOCX, PPTX, HTML)\n",
    "- å¤§è¦æ¨¡ä¸¦ç™¼è™•ç†\n",
    "- éŒ¯èª¤è™•ç†èˆ‡æ¢å¾©æ©Ÿåˆ¶\n",
    "- å“è³ªä¿è­‰èˆ‡é©—è­‰\n",
    "\n",
    "### 2. é‹ç®—æ€ç¶­æ‡‰ç”¨\n",
    "- **åˆ†è§£**: æ–‡æª”è™•ç†æµç¨‹æ¨¡çµ„åŒ–\n",
    "- **æ¨¡å¼è­˜åˆ¥**: æ–‡æª”çµæ§‹æ¨¡å¼\n",
    "- **æŠ½è±¡åŒ–**: çµ±ä¸€æ–‡æª”è™•ç†ä»‹é¢\n",
    "- **æ¼”ç®—æ³•**: è‡ªé©æ‡‰åˆ†å¡Šç­–ç•¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç’°å¢ƒè¨­å®šèˆ‡ä¾è³´å®‰è£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ä¿®æ­£ä¾è³´ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ\n",
    "# !pip install \"numpy<2\" --upgrade\n",
    "\n",
    "# # å®‰è£æ ¸å¿ƒä¾è³´å¥—ä»¶ (æŒ‡å®šå…¼å®¹ç‰ˆæœ¬)\n",
    "# !pip install docling python-docx PyPDF2 sentence-transformers textstat\n",
    "# !pip install plotly pandas \"numpy<2\" scikit-learn\n",
    "# !pip install spacy language-tool-python\n",
    "\n",
    "# # å¦‚æœ spaCy æ¨¡å‹æœªå®‰è£ï¼Œå‰‡å®‰è£\n",
    "# !python -m spacy download en_core_web_sm || echo \"spaCy æ¨¡å‹ä¸‹è¼‰å¤±æ•—ï¼Œå°‡ä½¿ç”¨é™ç´šè™•ç†æ–¹æ³•\"\n",
    "\n",
    "# print(\"âœ… ä¾è³´å¥—ä»¶å®‰è£å®Œæˆï¼ˆä¿®æ­£ç‰ˆæœ¬å…¼å®¹æ€§ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Docling å¯ç”¨\n",
      "âœ… SentenceTransformers å¯ç”¨\n",
      "âœ… spaCy æ¨¡å‹å¯ç”¨\n",
      "âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\n",
      "ğŸ“‚ å°ˆæ¡ˆæ ¹ç›®éŒ„: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance\n",
      "ğŸ”§ å¯ç”¨çµ„ä»¶: Docling=True, Embeddings=True, spaCy=True\n"
     ]
    }
   ],
   "source": [
    "# å°å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# åŸºç¤æ–‡æª”è™•ç† (å…ˆä¸å°å…¥ Doclingï¼Œé¿å…ç›¸å®¹æ€§å•é¡Œ)\n",
    "try:\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    DOCLING_AVAILABLE = True\n",
    "    print(\"âœ… Docling å¯ç”¨\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Docling å°å…¥å¤±æ•—: {e}\")\n",
    "    print(\"ğŸ“ å°‡ä½¿ç”¨æ›¿ä»£çš„æ–‡æª”è™•ç†æ–¹æ³•\")\n",
    "    DOCLING_AVAILABLE = False\n",
    "    \n",
    "# æ›¿ä»£æ–‡æª”è™•ç†\n",
    "import PyPDF2\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# NLP èˆ‡èªç¾©è™•ç†\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "    print(\"âœ… SentenceTransformers å¯ç”¨\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  SentenceTransformers å°å…¥å¤±æ•—: {e}\")\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    # å˜—è©¦è¼‰å…¥æ¨¡å‹\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        SPACY_AVAILABLE = True\n",
    "        print(\"âœ… spaCy æ¨¡å‹å¯ç”¨\")\n",
    "    except OSError:\n",
    "        print(\"âš ï¸  spaCy è‹±æ–‡æ¨¡å‹æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: python -m spacy download en_core_web_sm\")\n",
    "        SPACY_AVAILABLE = False\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textstat\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# è¨­å®šå°ˆæ¡ˆè·¯å¾‘\n",
    "PROJECT_ROOT = Path('/home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance')\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"ğŸ“‚ å°ˆæ¡ˆæ ¹ç›®éŒ„: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ”§ å¯ç”¨çµ„ä»¶: Docling={DOCLING_AVAILABLE}, Embeddings={EMBEDDINGS_AVAILABLE}, spaCy={SPACY_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ è³‡æ–™çµæ§‹è¨­è¨ˆ\n",
    "\n",
    "### æ¨™æº–åŒ–æ–‡æª”å…ƒè³‡æ–™æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ–‡æª”å…ƒè³‡æ–™ï¼š {'document_id': 'DOC-001', 'title': 'AI å ±å‘Šç¯„ä¾‹', 'authors': ['Sunny', 'Alice'], 'created_date': datetime.datetime(2025, 11, 11, 10, 0), 'modified_date': datetime.datetime(2025, 11, 11, 12, 0), 'document_type': 'report', 'file_path': '/data/docs/ai_report.pdf', 'file_size': 204800, 'page_count': 12, 'language': 'zh-TW', 'keywords': ['AI', 'NLP', 'RAG'], 'category': 'æŠ€è¡“æ–‡ä»¶', 'quality_score': None, 'processing_status': 'pending'}\n",
      "ğŸ§© æ–‡æª”åˆ†å¡Šå…§å®¹ï¼š ProcessedChunk(chunk_id='CHUNK-001', document_id='DOC-001', content='ç”Ÿæˆå¼ AI æ˜¯ 2020 å¹´ä»£æœ€å…·å½±éŸ¿åŠ›çš„æŠ€è¡“ä¹‹ä¸€...', chunk_index=0, start_page=1, end_page=1, word_count=15, embedding=[0.12, 0.56, 0.88], semantic_type='text')\n",
      "ğŸ“Š å“è³ªè©•ä¼°æŒ‡æ¨™ï¼š QualityMetrics(accuracy_score=0.95, completeness_score=0.9, readability_score=0.85, structure_score=0.88, overall_score=0.89)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "# ==========================================================\n",
    "# ä¸€ã€DocumentMetadataï¼šæ–‡æª”æ¨™æº–åŒ–å…ƒè³‡æ–™çµæ§‹\n",
    "# ==========================================================\n",
    "@dataclass\n",
    "class DocumentMetadata:\n",
    "    \"\"\"\n",
    "    ğŸ“˜ æ–‡æª”çš„æ¨™æº–åŒ–å…ƒè³‡æ–™çµæ§‹\n",
    "    --------------------------------------------------\n",
    "    ç”¨æ–¼æè¿°ä¸€ä»½æ–‡æª”çš„æ•´é«”å±¬æ€§ï¼Œä¾‹å¦‚ï¼š\n",
    "    - æª”åã€ä½œè€…ã€å»ºç«‹æ™‚é–“ã€å¤§å°ã€èªè¨€ã€åˆ†é¡ç­‰ã€‚\n",
    "    - é€šå¸¸ä½œç‚ºæ–‡ä»¶ç®¡ç†ç³»çµ±æˆ–çŸ¥è­˜åº«çš„ä¸Šå±¤æè¿°ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    document_id: str               # æ–‡æª”å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆUUIDæˆ–ç³»çµ±ç·¨è™Ÿï¼‰\n",
    "    title: str                     # æ–‡æª”æ¨™é¡Œ\n",
    "    authors: List[str]             # ä½œè€…æ¸…å–®\n",
    "    created_date: Optional[datetime]  # å»ºç«‹æ—¥æœŸ\n",
    "    modified_date: Optional[datetime] # æœ€å¾Œä¿®æ”¹æ—¥æœŸ\n",
    "    document_type: str             # æ–‡ä»¶é¡å‹ï¼ˆå¦‚ 'paper', 'report', 'policy'ï¼‰\n",
    "    file_path: str                 # æª”æ¡ˆå„²å­˜è·¯å¾‘\n",
    "    file_size: int                 # æª”æ¡ˆå¤§å°ï¼ˆå–®ä½ï¼šbytesï¼‰\n",
    "    page_count: int                # é æ•¸\n",
    "    language: str                  # èªè¨€ï¼ˆå¦‚ 'zh-TW', 'en'ï¼‰\n",
    "    keywords: List[str]            # é—œéµå­—åˆ—è¡¨\n",
    "    category: str                  # æ–‡æª”åˆ†é¡\n",
    "    quality_score: Optional[float] = None  # æ–‡ä»¶å“è³ªåˆ†æ•¸ï¼ˆ0ï½1ï¼‰\n",
    "    processing_status: str = 'pending'     # è™•ç†ç‹€æ…‹ï¼š'pending' | 'processing' | 'completed' | 'failed'\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼åºåˆ—åŒ–æˆ–APIè¼¸å‡º\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# äºŒã€ProcessedChunkï¼šæ–‡æª”åˆ†å¡Šçµæ§‹\n",
    "# ==========================================================\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    \"\"\"\n",
    "    ğŸ§± æ–‡æª”åˆ†å¡Šå¾Œçš„èªç¾©å–®å…ƒ\n",
    "    --------------------------------------------------\n",
    "    æ–‡æª”åˆ†å‰²ï¼ˆchunkingï¼‰å¾Œçš„æœ€å°èªç¾©ç‰‡æ®µã€‚\n",
    "    æ¯å€‹åˆ†å¡Šå¯é€²è¡Œå‘é‡åµŒå…¥ï¼Œç”¨æ–¼ RAG æˆ–æª¢ç´¢ä»»å‹™ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_id: str                  # åˆ†å¡Šå”¯ä¸€è­˜åˆ¥ç¢¼\n",
    "    document_id: str               # æ‰€å±¬æ–‡æª”IDï¼ˆå°æ‡‰ DocumentMetadata.document_idï¼‰\n",
    "    content: str                   # è©²åˆ†å¡Šçš„æ–‡å­—å…§å®¹\n",
    "    chunk_index: int               # åœ¨æ•´ä»½æ–‡æª”çš„é †åºï¼ˆ0èµ·ç®—ï¼‰\n",
    "    start_page: int                # èµ·å§‹é ç¢¼\n",
    "    end_page: int                  # çµæŸé ç¢¼\n",
    "    word_count: int                # è©²åˆ†å¡Šçš„å­—æ•¸\n",
    "    embedding: Optional[List[float]] = None  # è©²åˆ†å¡Šçš„å‘é‡åµŒå…¥ï¼ˆèªç¾©è¡¨ç¤ºï¼‰\n",
    "    semantic_type: str = 'text'    # åˆ†å¡Šé¡å‹ï¼š'text' | 'table' | 'figure' | 'header'\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ä¸‰ã€QualityMetricsï¼šæ–‡æª”å“è³ªæŒ‡æ¨™\n",
    "# ==========================================================\n",
    "@dataclass  \n",
    "class QualityMetrics:\n",
    "    \"\"\"\n",
    "    ğŸ“Š æ–‡æª”å“è³ªè©•ä¼°æŒ‡æ¨™\n",
    "    --------------------------------------------------\n",
    "    ç”¨æ–¼é‡åŒ–æ–‡æª”æˆ–åˆ†å¡Šçš„å…§å®¹å“è³ªï¼š\n",
    "    - å¹«åŠ©è‡ªå‹•è©•ä¼°OCRæ­£ç¢ºç‡ã€çµæ§‹å®Œæ•´æ€§èˆ‡å¯è®€æ€§ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy_score: float          # æ–‡æœ¬æº–ç¢ºæ€§ï¼ˆæ‹¼å­—æˆ–è¾¨è­˜æ­£ç¢ºç‡ï¼‰[0~1]\n",
    "    completeness_score: float      # å®Œæ•´æ€§ï¼ˆæ˜¯å¦æœ‰ç¼ºæ¼ï¼‰[0~1]\n",
    "    readability_score: float       # å¯è®€æ€§ï¼ˆèªæ³•èˆ‡è¡¨é”æµæš¢åº¦ï¼‰[0~1]\n",
    "    structure_score: float         # çµæ§‹å®Œæ•´æ€§ï¼ˆæ¨™é¡Œ/æ®µè½/è¡¨æ ¼ï¼‰[0~1]\n",
    "    overall_score: float           # ç¶œåˆå“è³ªåˆ†æ•¸ï¼ˆé€šå¸¸ç‚ºåŠ æ¬Šå¹³å‡ï¼‰[0~1]\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ç¯„ä¾‹ä½¿ç”¨æƒ…å¢ƒï¼ˆæ–‡ä»¶ â†’ åˆ†å¡Š â†’ å“è³ªè©•ä¼°ï¼‰\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # å»ºç«‹æ–‡ä»¶å…ƒè³‡æ–™\n",
    "    doc_meta = DocumentMetadata(\n",
    "        document_id=\"DOC-001\",\n",
    "        title=\"AI å ±å‘Šç¯„ä¾‹\",\n",
    "        authors=[\"Sunny\", \"Alice\"],\n",
    "        created_date=datetime(2025, 11, 11, 10, 0),\n",
    "        modified_date=datetime(2025, 11, 11, 12, 0),\n",
    "        document_type=\"report\",\n",
    "        file_path=\"/data/docs/ai_report.pdf\",\n",
    "        file_size=204800,\n",
    "        page_count=12,\n",
    "        language=\"zh-TW\",\n",
    "        keywords=[\"AI\", \"NLP\", \"RAG\"],\n",
    "        category=\"æŠ€è¡“æ–‡ä»¶\",\n",
    "        quality_score=None,\n",
    "        processing_status=\"pending\"\n",
    "    )\n",
    "\n",
    "    # æ¨¡æ“¬ä¸€å€‹åˆ†å¡Š\n",
    "    chunk = ProcessedChunk(\n",
    "        chunk_id=\"CHUNK-001\",\n",
    "        document_id=\"DOC-001\",\n",
    "        content=\"ç”Ÿæˆå¼ AI æ˜¯ 2020 å¹´ä»£æœ€å…·å½±éŸ¿åŠ›çš„æŠ€è¡“ä¹‹ä¸€...\",\n",
    "        chunk_index=0,\n",
    "        start_page=1,\n",
    "        end_page=1,\n",
    "        word_count=15,\n",
    "        embedding=[0.12, 0.56, 0.88],  # ç°¡åŒ–ç¯„ä¾‹\n",
    "        semantic_type=\"text\"\n",
    "    )\n",
    "\n",
    "    # æ¨¡æ“¬å“è³ªè©•ä¼°\n",
    "    metrics = QualityMetrics(\n",
    "        accuracy_score=0.95,\n",
    "        completeness_score=0.9,\n",
    "        readability_score=0.85,\n",
    "        structure_score=0.88,\n",
    "        overall_score=0.89\n",
    "    )\n",
    "\n",
    "    print(\"âœ… æ–‡æª”å…ƒè³‡æ–™ï¼š\", doc_meta.to_dict())\n",
    "    print(\"ğŸ§© æ–‡æª”åˆ†å¡Šå…§å®¹ï¼š\", chunk)\n",
    "    print(\"ğŸ“Š å“è³ªè©•ä¼°æŒ‡æ¨™ï¼š\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ æ ¸å¿ƒæ–‡æª”è™•ç†å™¨å¯¦ä½œ\n",
    "\n",
    "### ä¼æ¥­ç´šæ–‡æª”è™•ç†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EnterpriseDocumentProcessor å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class EnterpriseDocumentProcessor:\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ç´šæ–‡æª”è™•ç†å™¨\n",
    "    æ•´åˆ Doclingã€èªç¾©åˆ†å¡Šã€å“è³ªè©•ä¼°\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        \"\"\"åˆå§‹åŒ–è™•ç†å™¨\"\"\"\n",
    "        self.config = config or self._default_config()\n",
    "        \n",
    "        # åˆå§‹åŒ– Docling è½‰æ›å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        if DOCLING_AVAILABLE:\n",
    "            try:\n",
    "                self.doc_converter = DocumentConverter()\n",
    "                print(\"âœ… Docling è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Docling åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
    "                self.doc_converter = None\n",
    "        else:\n",
    "            self.doc_converter = None\n",
    "        \n",
    "        # è¼‰å…¥èªç¾©æ¨¡å‹ï¼ˆå„ªåŒ–CUDAè¨­å®šï¼‰\n",
    "        if EMBEDDINGS_AVAILABLE:\n",
    "            try:\n",
    "                print(\"ğŸ”„ è¼‰å…¥èªç¾©æ¨¡å‹ (å„ªåŒ–CUDAè¨­å®š)...\")\n",
    "                \n",
    "                # CUDAè¨˜æ†¶é«”å„ªåŒ–è¨­å®š\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    # æ¸…ç†GPUè¨˜æ†¶é«”\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # æª¢æŸ¥å¯ç”¨GPU\n",
    "                    gpu_count = torch.cuda.device_count()\n",
    "                    print(f\"ç™¼ç¾ {gpu_count} å¼µGPU\")\n",
    "                    \n",
    "                    # é¸æ“‡è¨˜æ†¶é«”ä½¿ç”¨è¼ƒå°‘çš„GPU\n",
    "                    best_gpu = 1 if gpu_count > 1 else 0  # å„ªå…ˆä½¿ç”¨GPU 1\n",
    "                    device_name = f\"cuda:{best_gpu}\"\n",
    "                    \n",
    "                    # è¨­å®šè¨˜æ†¶é«”åˆ†é…ç­–ç•¥\n",
    "                    torch.cuda.set_per_process_memory_fraction(0.7, device=best_gpu)\n",
    "                    print(f\"ä½¿ç”¨è¨­å‚™: {device_name}\")\n",
    "                else:\n",
    "                    device_name = \"cpu\"\n",
    "                    print(\"CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU\")\n",
    "                \n",
    "                self.semantic_model = SentenceTransformer(\n",
    "                    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                    device=device_name,\n",
    "                    trust_remote_code=False\n",
    "                )\n",
    "                print(f\"âœ… èªç¾©æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè¨­å‚™: {self.semantic_model.device}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  èªç¾©æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨CPU: {e}\")\n",
    "                try:\n",
    "                    # é™ç´šåˆ°CPU\n",
    "                    self.semantic_model = SentenceTransformer(\n",
    "                        'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                        device='cpu',\n",
    "                        trust_remote_code=False\n",
    "                    )\n",
    "                    print(\"âœ… èªç¾©æ¨¡å‹è¼‰å…¥å®Œæˆ (CPUæ¨¡å¼)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"âŒ CPUæ¨¡å¼ä¹Ÿå¤±æ•—: {e2}\")\n",
    "                    self.semantic_model = None\n",
    "        else:\n",
    "            self.semantic_model = None\n",
    "        \n",
    "        # è¼‰å…¥ spaCy æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        if SPACY_AVAILABLE:\n",
    "            self.nlp = nlp\n",
    "            print(\"âœ… spaCy æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "        else:\n",
    "            self.nlp = None\n",
    "        \n",
    "        # åˆå§‹åŒ–å“è³ªé–¾å€¼\n",
    "        self.quality_thresholds = {\n",
    "            'min_overall_score': 0.7,\n",
    "            'min_readability': 0.6,\n",
    "            'min_completeness': 0.8\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… DocumentProcessor åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _default_config(self) -> Dict:\n",
    "        \"\"\"é»˜èªé…ç½®\"\"\"\n",
    "        return {\n",
    "            'chunk_size': 512,\n",
    "            'chunk_overlap': 50,\n",
    "            'similarity_threshold': 0.3,\n",
    "            'min_chunk_words': 20,\n",
    "            'max_chunk_words': 1000\n",
    "        }\n",
    "    \n",
    "    def extract_metadata(self, file_path: str) -> DocumentMetadata:\n",
    "        \"\"\"\n",
    "        æå–æ–‡æª”åŸºæœ¬å…ƒè³‡æ–™\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # åŸºæœ¬æª”æ¡ˆè³‡è¨Š\n",
    "        file_stats = file_path.stat()\n",
    "        \n",
    "        # ç”Ÿæˆæ–‡æª” ID\n",
    "        document_id = f\"doc_{file_path.stem}_{int(datetime.now().timestamp())}\"\n",
    "        \n",
    "        # å˜—è©¦è§£ææ–‡æª”ç²å–è©³ç´°è³‡è¨Š\n",
    "        try:\n",
    "            title, page_count = self._extract_basic_info(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  åŸºæœ¬è³‡è¨Šæå–å¤±æ•—: {e}\")\n",
    "            title = file_path.stem\n",
    "            page_count = 1\n",
    "        \n",
    "        return DocumentMetadata(\n",
    "            document_id=document_id,\n",
    "            title=title,\n",
    "            authors=[],  # éœ€è¦é€²ä¸€æ­¥ NER æå–\n",
    "            created_date=datetime.fromtimestamp(file_stats.st_ctime),\n",
    "            modified_date=datetime.fromtimestamp(file_stats.st_mtime),\n",
    "            document_type=self._classify_document_type(file_path),\n",
    "            file_path=str(file_path),\n",
    "            file_size=file_stats.st_size,\n",
    "            page_count=page_count,\n",
    "            language='en',  # å¯æ“´å±•ç‚ºè‡ªå‹•æª¢æ¸¬\n",
    "            keywords=[],  # éœ€è¦é€²ä¸€æ­¥ NLP æå–\n",
    "            category=self._extract_category_from_path(file_path),\n",
    "            processing_status='pending'\n",
    "        )\n",
    "    \n",
    "    def _extract_basic_info(self, file_path: Path) -> Tuple[str, int]:\n",
    "        \"\"\"æå–åŸºæœ¬æª”æ¡ˆè³‡è¨Š\"\"\"\n",
    "        title = file_path.stem\n",
    "        page_count = 1\n",
    "        \n",
    "        # å¦‚æœæ˜¯æ–‡æœ¬æª”æ¡ˆï¼Œç›´æ¥è®€å–\n",
    "        if file_path.suffix.lower() == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                # å˜—è©¦æå–ç¬¬ä¸€è¡Œä½œç‚ºæ¨™é¡Œ\n",
    "                lines = content.split('\\n')\n",
    "                first_line = lines[0].strip() if lines else \"\"\n",
    "                if first_line and len(first_line) < 100:\n",
    "                    title = first_line\n",
    "                # ä¼°ç®—é æ•¸ï¼ˆæ¯é ç´„500å­—ï¼‰\n",
    "                word_count = len(content.split())\n",
    "                page_count = max(1, word_count // 500)\n",
    "        \n",
    "        elif file_path.suffix.lower() == '.pdf' and self.doc_converter:\n",
    "            # ä½¿ç”¨ Docling è™•ç† PDF\n",
    "            try:\n",
    "                converted_doc = self.doc_converter.convert(str(file_path)).document\n",
    "                page_count = len(converted_doc.pages) if hasattr(converted_doc, 'pages') else 1\n",
    "                raw_text = converted_doc.export_to_text()\n",
    "                first_line = raw_text.split('\\n')[0].strip()\n",
    "                if first_line and len(first_line) < 200:\n",
    "                    title = first_line\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Docling PDF è™•ç†å¤±æ•—: {e}\")\n",
    "        \n",
    "        return title, page_count\n",
    "    \n",
    "    def _classify_document_type(self, file_path: Path) -> str:\n",
    "        \"\"\"æ ¹æ“šè·¯å¾‘å’Œæª”ååˆ†é¡æ–‡æª”é¡å‹\"\"\"\n",
    "        path_str = str(file_path).lower()\n",
    "        \n",
    "        if 'paper' in path_str or 'arxiv' in path_str:\n",
    "            return 'paper'\n",
    "        elif 'report' in path_str:\n",
    "            return 'report'\n",
    "        elif 'policy' in path_str or 'guideline' in path_str:\n",
    "            return 'policy'\n",
    "        else:\n",
    "            return 'document'\n",
    "    \n",
    "    def _extract_category_from_path(self, file_path: Path) -> str:\n",
    "        \"\"\"å¾æª”æ¡ˆè·¯å¾‘æå–åˆ†é¡\"\"\"\n",
    "        path_parts = file_path.parts\n",
    "        \n",
    "        # å°‹æ‰¾é¡ä¼¼åˆ†é¡çš„ç›®éŒ„åç¨±\n",
    "        for part in path_parts:\n",
    "            if any(keyword in part.lower() for keyword in \n",
    "                   ['model', 'infrastructure', 'language', 'multimodal']):\n",
    "                return part\n",
    "        \n",
    "        return 'general'\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Tuple[DocumentMetadata, List[ProcessedChunk], QualityMetrics]:\n",
    "        \"\"\"\n",
    "        å®Œæ•´çš„æ–‡æª”è™•ç†æµç¨‹\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[DocumentMetadata, List[ProcessedChunk], QualityMetrics]\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”„ é–‹å§‹è™•ç†æ–‡æª”: {Path(file_path).name}\")\n",
    "        \n",
    "        # 1. æå–å…ƒè³‡æ–™\n",
    "        metadata = self.extract_metadata(file_path)\n",
    "        metadata.processing_status = 'processing'\n",
    "        \n",
    "        # 2. æ–‡æª”å…§å®¹æå–\n",
    "        try:\n",
    "            raw_text = self._extract_text_content(file_path)\n",
    "            \n",
    "            # 3. æ™ºèƒ½åˆ†å¡Š\n",
    "            chunks = self._semantic_chunking(raw_text, metadata.document_id)\n",
    "            \n",
    "            # 4. å“è³ªè©•ä¼°\n",
    "            quality_metrics = self._assess_quality(raw_text, chunks)\n",
    "            metadata.quality_score = quality_metrics.overall_score\n",
    "            metadata.processing_status = 'completed'\n",
    "            \n",
    "            print(f\"âœ… æ–‡æª”è™•ç†å®Œæˆ - å“è³ªåˆ†æ•¸: {quality_metrics.overall_score:.3f}\")\n",
    "            \n",
    "            return metadata, chunks, quality_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ–‡æª”è™•ç†å¤±æ•—: {e}\")\n",
    "            metadata.processing_status = 'failed'\n",
    "            return metadata, [], QualityMetrics(0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    \n",
    "    def _extract_text_content(self, file_path: str) -> str:\n",
    "        \"\"\"æå–æ–‡æœ¬å…§å®¹ï¼ˆå…¼å®¹å¤šç¨®æ–¹æ³•ï¼‰\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # æ–‡æœ¬æª”æ¡ˆç›´æ¥è®€å–\n",
    "        if file_path.suffix.lower() == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        # å¦‚æœ Docling å¯ç”¨ï¼Œå„ªå…ˆä½¿ç”¨\n",
    "        elif self.doc_converter and file_path.suffix.lower() in ['.pdf', '.docx']:\n",
    "            try:\n",
    "                converted_doc = self.doc_converter.convert(str(file_path)).document\n",
    "                return converted_doc.export_to_text()\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Docling è™•ç†å¤±æ•—ï¼Œå˜—è©¦æ›¿ä»£æ–¹æ³•: {e}\")\n",
    "        \n",
    "        # PDF æ›¿ä»£è™•ç†\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            try:\n",
    "                text = \"\"\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    pdf_reader = PyPDF2.PdfReader(f)\n",
    "                    for page in pdf_reader.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  PDF æ›¿ä»£è™•ç†å¤±æ•—: {e}\")\n",
    "        \n",
    "        # DOCX æ›¿ä»£è™•ç†\n",
    "        elif file_path.suffix.lower() == '.docx':\n",
    "            try:\n",
    "                doc = DocxDocument(file_path)\n",
    "                text = \"\"\n",
    "                for paragraph in doc.paragraphs:\n",
    "                    text += paragraph.text + \"\\n\"\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  DOCX æ›¿ä»£è™•ç†å¤±æ•—: {e}\")\n",
    "        \n",
    "        # å¦‚æœéƒ½å¤±æ•—ï¼Œå›å‚³æª”æ¡ˆåä½œç‚ºå…§å®¹\n",
    "        return f\"Unable to extract content from {file_path.name}\"\n",
    "\n",
    "print(\"âœ… EnterpriseDocumentProcessor å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### èªç¾©åˆ†å¡Šæ¼”ç®—æ³•å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… èªç¾©åˆ†å¡Šèˆ‡å“è³ªè©•ä¼°æ–¹æ³•å®šç¾©å®Œæˆï¼ˆå…¼å®¹ç‰ˆæœ¬ï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ç¹¼çºŒ EnterpriseDocumentProcessor é¡åˆ¥çš„æ–¹æ³•å¯¦ä½œ\n",
    "\n",
    "def _semantic_chunking(self, text: str, document_id: str) -> List[ProcessedChunk]:\n",
    "    \"\"\"\n",
    "    èªç¾©åˆ†å¡Šæ¼”ç®—æ³•\n",
    "    åŸºæ–¼å¥å­ç›¸ä¼¼åº¦çš„å‹•æ…‹åˆ†å¡Šç­–ç•¥ï¼ˆå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§  åŸ·è¡Œåˆ†å¡Š...\")\n",
    "    \n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # å¦‚æœå¯ç”¨ï¼Œä½¿ç”¨èªç¾©åˆ†å¡Š\n",
    "    if self.semantic_model and self.nlp:\n",
    "        return self._advanced_semantic_chunking(text, document_id)\n",
    "    else:\n",
    "        # é™ç´šåˆ°åŸºæ–¼æ®µè½çš„åˆ†å¡Š\n",
    "        return self._paragraph_based_chunking(text, document_id)\n",
    "\n",
    "def _advanced_semantic_chunking(self, text: str, document_id: str) -> List[ProcessedChunk]:\n",
    "    \"\"\"é€²éšèªç¾©åˆ†å¡Šï¼ˆéœ€è¦ spaCy å’Œ SentenceTransformersï¼‰\"\"\"\n",
    "    # 1. æ–‡æœ¬é è™•ç†èˆ‡å¥å­åˆ†å‰²\n",
    "    doc = self.nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) < 2:\n",
    "        # å¦‚æœå¥å­å¤ªå°‘ï¼Œç›´æ¥è¿”å›å–®ä¸€åˆ†å¡Š\n",
    "        return [ProcessedChunk(\n",
    "            chunk_id=f\"{document_id}_chunk_0\",\n",
    "            document_id=document_id,\n",
    "            content=text,\n",
    "            chunk_index=0,\n",
    "            start_page=1,\n",
    "            end_page=1,\n",
    "            word_count=len(text.split()),\n",
    "            semantic_type='text'\n",
    "        )]\n",
    "    \n",
    "    # 2. è¨ˆç®—å¥å­åµŒå…¥å‘é‡\n",
    "    embeddings = self.semantic_model.encode(sentences)\n",
    "    \n",
    "    # 3. è¨ˆç®—ç›¸é„°å¥å­ç›¸ä¼¼åº¦\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    # 4. è­˜åˆ¥èªç¾©é‚Šç•Œï¼ˆç›¸ä¼¼åº¦å±€éƒ¨æœ€å°å€¼ï¼‰\n",
    "    threshold = self.config['similarity_threshold']\n",
    "    boundaries = [0]  # èµ·å§‹é‚Šç•Œ\n",
    "    \n",
    "    for i in range(1, len(similarities) - 1):\n",
    "        # å¦‚æœç•¶å‰ç›¸ä¼¼åº¦æ˜¯å±€éƒ¨æœ€å°å€¼ä¸”ä½æ–¼é–¾å€¼\n",
    "        if (similarities[i] < similarities[i-1] and \n",
    "            similarities[i] < similarities[i+1] and \n",
    "            similarities[i] < threshold):\n",
    "            boundaries.append(i + 1)\n",
    "    \n",
    "    boundaries.append(len(sentences))  # çµæŸé‚Šç•Œ\n",
    "    \n",
    "    # 5. åŸºæ–¼é‚Šç•Œç”Ÿæˆåˆ†å¡Š\n",
    "    chunks = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        start_idx = boundaries[i]\n",
    "        end_idx = boundaries[i + 1]\n",
    "        \n",
    "        chunk_sentences = sentences[start_idx:end_idx]\n",
    "        chunk_text = ' '.join(chunk_sentences)\n",
    "        word_count = len(chunk_text.split())\n",
    "        \n",
    "        # æª¢æŸ¥åˆ†å¡Šå¤§å°\n",
    "        if (word_count >= self.config['min_chunk_words'] and \n",
    "            word_count <= self.config['max_chunk_words']):\n",
    "            \n",
    "            chunk = ProcessedChunk(\n",
    "                chunk_id=f\"{document_id}_chunk_{len(chunks)}\",\n",
    "                document_id=document_id,\n",
    "                content=chunk_text,\n",
    "                chunk_index=len(chunks),\n",
    "                start_page=1,  # ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è¨ˆç®—é ç¢¼\n",
    "                end_page=1,\n",
    "                word_count=word_count,\n",
    "                semantic_type='text'\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    print(f\"ğŸ“ ç”Ÿæˆ {len(chunks)} å€‹èªç¾©åˆ†å¡Š\")\n",
    "    return chunks\n",
    "\n",
    "def _paragraph_based_chunking(self, text: str, document_id: str) -> List[ProcessedChunk]:\n",
    "    \"\"\"åŸºæ–¼æ®µè½çš„åˆ†å¡Šï¼ˆé™ç´šæ–¹æ¡ˆï¼‰\"\"\"\n",
    "    print(\"ğŸ“ ä½¿ç”¨æ®µè½åˆ†å¡Šï¼ˆé™ç´šæ–¹æ¡ˆï¼‰\")\n",
    "    \n",
    "    # æŒ‰æ®µè½åˆ†å‰²\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    if not paragraphs:\n",
    "        # å¦‚æœæ²’æœ‰æ®µè½ï¼ŒæŒ‰å¥å­åˆ†å‰²\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        paragraphs = sentences\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_word_count = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        para_words = len(paragraph.split())\n",
    "        \n",
    "        # å¦‚æœç•¶å‰åˆ†å¡ŠåŠ ä¸Šé€™å€‹æ®µè½æœƒè¶…éæœ€å¤§é•·åº¦\n",
    "        if current_word_count + para_words > self.config['max_chunk_words'] and current_chunk:\n",
    "            # ä¿å­˜ç•¶å‰åˆ†å¡Š\n",
    "            if current_word_count >= self.config['min_chunk_words']:\n",
    "                chunk = ProcessedChunk(\n",
    "                    chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                    document_id=document_id,\n",
    "                    content=current_chunk.strip(),\n",
    "                    chunk_index=chunk_index,\n",
    "                    start_page=1,\n",
    "                    end_page=1,\n",
    "                    word_count=current_word_count,\n",
    "                    semantic_type='text'\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                chunk_index += 1\n",
    "            \n",
    "            # é–‹å§‹æ–°åˆ†å¡Š\n",
    "            current_chunk = paragraph\n",
    "            current_word_count = para_words\n",
    "        else:\n",
    "            # æ·»åŠ åˆ°ç•¶å‰åˆ†å¡Š\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + paragraph\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "            current_word_count += para_words\n",
    "    \n",
    "    # è™•ç†æœ€å¾Œä¸€å€‹åˆ†å¡Š\n",
    "    if current_chunk and current_word_count >= self.config['min_chunk_words']:\n",
    "        chunk = ProcessedChunk(\n",
    "            chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "            document_id=document_id,\n",
    "            content=current_chunk.strip(),\n",
    "            chunk_index=chunk_index,\n",
    "            start_page=1,\n",
    "            end_page=1,\n",
    "            word_count=current_word_count,\n",
    "            semantic_type='text'\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    print(f\"ğŸ“ ç”Ÿæˆ {len(chunks)} å€‹æ®µè½åˆ†å¡Š\")\n",
    "    return chunks\n",
    "\n",
    "def _assess_quality(self, text: str, chunks: List[ProcessedChunk]) -> QualityMetrics:\n",
    "    \"\"\"\n",
    "    å¤šç¶­åº¦æ–‡æª”å“è³ªè©•ä¼°\n",
    "    åŸºæ–¼ ISO 25012 æ¨™æº–\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š åŸ·è¡Œå“è³ªè©•ä¼°...\")\n",
    "    \n",
    "    # 1. æº–ç¢ºæ€§è©•ä¼°ï¼ˆåŸºæ–¼èªè¨€å“è³ªï¼‰\n",
    "    accuracy_score = self._calculate_accuracy(text)\n",
    "    \n",
    "    # 2. å®Œæ•´æ€§è©•ä¼°ï¼ˆåŸºæ–¼çµæ§‹å®Œæ•´æ€§ï¼‰\n",
    "    completeness_score = self._calculate_completeness(text, chunks)\n",
    "    \n",
    "    # 3. å¯è®€æ€§è©•ä¼°\n",
    "    readability_score = self._calculate_readability(text)\n",
    "    \n",
    "    # 4. çµæ§‹è©•ä¼°\n",
    "    structure_score = self._calculate_structure_score(text)\n",
    "    \n",
    "    # 5. ç¶œåˆå“è³ªåˆ†æ•¸ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰\n",
    "    weights = {'accuracy': 0.25, 'completeness': 0.25, \n",
    "               'readability': 0.25, 'structure': 0.25}\n",
    "    \n",
    "    overall_score = (\n",
    "        weights['accuracy'] * accuracy_score +\n",
    "        weights['completeness'] * completeness_score +\n",
    "        weights['readability'] * readability_score +\n",
    "        weights['structure'] * structure_score\n",
    "    )\n",
    "    \n",
    "    return QualityMetrics(\n",
    "        accuracy_score=accuracy_score,\n",
    "        completeness_score=completeness_score,\n",
    "        readability_score=readability_score,\n",
    "        structure_score=structure_score,\n",
    "        overall_score=overall_score\n",
    "    )\n",
    "\n",
    "def _calculate_accuracy(self, text: str) -> float:\n",
    "    \"\"\"è¨ˆç®—æ–‡æœ¬æº–ç¢ºæ€§ï¼ˆèªè¨€å“è³ªï¼‰\"\"\"\n",
    "    # ç°¡åŒ–å¯¦ä½œï¼šåŸºæ–¼æ–‡æœ¬é•·åº¦å’Œå­—ç¬¦åˆ†å¸ƒ\n",
    "    if len(text.strip()) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # åŸºæœ¬èªè¨€ç‰¹å¾µæª¢æŸ¥\n",
    "    if self.nlp:\n",
    "        # ä½¿ç”¨ spaCy é€²è¡Œç²¾ç¢ºåˆ†æ\n",
    "        doc = self.nlp(text[:1000])  # å–å‰1000å­—ç¬¦é€²è¡Œåˆ†æ\n",
    "        \n",
    "        # è¨ˆç®—æœ‰æ•ˆè©å½™æ¯”ä¾‹\n",
    "        valid_tokens = sum(1 for token in doc if token.is_alpha and not token.is_stop)\n",
    "        total_tokens = len([token for token in doc if token.is_alpha])\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        validity_ratio = valid_tokens / total_tokens\n",
    "        return min(validity_ratio * 1.2, 1.0)  # ç¨å¾®æå‡åˆ†æ•¸\n",
    "    else:\n",
    "        # é™ç´šåˆ†æï¼šåŸºæ–¼åŸºæœ¬çµ±è¨ˆ\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return 0.0\n",
    "        \n",
    "        # æª¢æŸ¥åŸºæœ¬ç‰¹å¾µ\n",
    "        alpha_ratio = sum(1 for word in words if word.isalpha()) / len(words)\n",
    "        avg_word_length = np.mean([len(word) for word in words])\n",
    "        \n",
    "        # ç°¡å–®è©•åˆ†\n",
    "        score = alpha_ratio * (min(avg_word_length / 5, 1.0))\n",
    "        return min(score, 1.0)\n",
    "\n",
    "def _calculate_completeness(self, text: str, chunks: List[ProcessedChunk]) -> float:\n",
    "    \"\"\"è¨ˆç®—å…§å®¹å®Œæ•´æ€§\"\"\"\n",
    "    if not chunks:\n",
    "        return 0.0\n",
    "    \n",
    "    # åŸºæ–¼åˆ†å¡Šåˆ†å¸ƒçš„å‡å‹»æ€§\n",
    "    chunk_sizes = [chunk.word_count for chunk in chunks]\n",
    "    if not chunk_sizes:\n",
    "        return 0.0\n",
    "    \n",
    "    # è¨ˆç®—åˆ†å¡Šå¤§å°çš„è®Šç•°ä¿‚æ•¸\n",
    "    mean_size = np.mean(chunk_sizes)\n",
    "    std_size = np.std(chunk_sizes)\n",
    "    \n",
    "    if mean_size == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    cv = std_size / mean_size\n",
    "    completeness = max(1.0 - cv, 0.0)  # è®Šç•°ä¿‚æ•¸è¶Šå°ï¼Œå®Œæ•´æ€§è¶Šé«˜\n",
    "    return min(completeness, 1.0)\n",
    "\n",
    "def _calculate_readability(self, text: str) -> float:\n",
    "    \"\"\"è¨ˆç®—æ–‡æœ¬å¯è®€æ€§\"\"\"\n",
    "    try:\n",
    "        # ä½¿ç”¨ Flesch Reading Ease\n",
    "        flesch_score = textstat.flesch_reading_ease(text)\n",
    "        # è½‰æ›ç‚º 0-1 ç¯„åœ\n",
    "        normalized_score = max(0, min(100, flesch_score)) / 100\n",
    "        return normalized_score\n",
    "    except:\n",
    "        # é™ç´šè©•ä¼°\n",
    "        sentences = text.split('.')\n",
    "        if not sentences:\n",
    "            return 0.5\n",
    "        \n",
    "        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])\n",
    "        # ç†æƒ³å¥å­é•·åº¦ç‚º15-25å­—\n",
    "        if 15 <= avg_sentence_length <= 25:\n",
    "            return 0.8\n",
    "        elif 10 <= avg_sentence_length <= 35:\n",
    "            return 0.6\n",
    "        else:\n",
    "            return 0.4\n",
    "\n",
    "def _calculate_structure_score(self, text: str) -> float:\n",
    "    \"\"\"è¨ˆç®—æ–‡æª”çµæ§‹åˆ†æ•¸\"\"\"\n",
    "    # æª¢æŸ¥åŸºæœ¬çµæ§‹å…ƒç´ \n",
    "    structure_indicators = {\n",
    "        'has_paragraphs': '\\n\\n' in text,\n",
    "        'has_sentences': '.' in text,\n",
    "        'reasonable_length': 100 < len(text) < 50000,\n",
    "        'has_capitalization': any(c.isupper() for c in text[:500]),\n",
    "        'has_sections': any(pattern in text.lower() for pattern in ['abstract', 'introduction', 'conclusion']),\n",
    "        'has_numbering': bool(re.search(r'^\\s*\\d+\\.', text, re.MULTILINE))\n",
    "    }\n",
    "    \n",
    "    score = sum(structure_indicators.values()) / len(structure_indicators)\n",
    "    return score\n",
    "\n",
    "# å°‡æ–¹æ³•æ·»åŠ åˆ° EnterpriseDocumentProcessor é¡åˆ¥\n",
    "import re\n",
    "\n",
    "EnterpriseDocumentProcessor._semantic_chunking = _semantic_chunking\n",
    "EnterpriseDocumentProcessor._advanced_semantic_chunking = _advanced_semantic_chunking\n",
    "EnterpriseDocumentProcessor._paragraph_based_chunking = _paragraph_based_chunking\n",
    "EnterpriseDocumentProcessor._assess_quality = _assess_quality\n",
    "EnterpriseDocumentProcessor._calculate_accuracy = _calculate_accuracy\n",
    "EnterpriseDocumentProcessor._calculate_completeness = _calculate_completeness\n",
    "EnterpriseDocumentProcessor._calculate_readability = _calculate_readability\n",
    "EnterpriseDocumentProcessor._calculate_structure_score = _calculate_structure_score\n",
    "\n",
    "print(\"âœ… èªç¾©åˆ†å¡Šèˆ‡å“è³ªè©•ä¼°æ–¹æ³•å®šç¾©å®Œæˆï¼ˆå…¼å®¹ç‰ˆæœ¬ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª å¯¦ä½œç·´ç¿’ 1: å–®ä¸€æ–‡æª”è™•ç†\n",
    "\n",
    "### æº–å‚™æ¸¬è©¦è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹æ¸¬è©¦ç”¨çš„æ¨£æœ¬æ–‡æª”\nsample_text = \"\"\"\nAttention Is All You Need\n\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\n1. Introduction\nRecurrent neural networks, long short-term memory and gated recurrent units in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n\nSelf-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.\n\n2. Background\nThe goal of reducing sequential computation also motivated the creation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions.\n\"\"\"\n\n# å»ºç«‹æ¸¬è©¦æ–‡ä»¶\ntest_dir = PROJECT_ROOT / 'notebooks' / '01_document_ingestion' / 'test_data'\ntest_dir.mkdir(parents=True, exist_ok=True)\n\ntest_file = test_dir / 'transformer_paper_sample.txt'\nwith open(test_file, 'w', encoding='utf-8') as f:\n    f.write(sample_text)\n\nprint(f\"âœ… æ¸¬è©¦æ–‡ä»¶å·²å»ºç«‹: {test_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œæ–‡æª”è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SentenceTransformer CUDA åŠ é€Ÿæ¸¬è©¦ ===\n",
      "âœ… Docling è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ”„ è¼‰å…¥èªç¾©æ¨¡å‹ (å„ªåŒ–CUDAè¨­å®š)...\n",
      "ç™¼ç¾ 2 å¼µGPU\n",
      "ä½¿ç”¨è¨­å‚™: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:02:48,301 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… èªç¾©æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè¨­å‚™: cuda:1\n",
      "âœ… spaCy æ¨¡å‹è¼‰å…¥å®Œæˆ\n",
      "âœ… DocumentProcessor åˆå§‹åŒ–å®Œæˆ\n",
      "æ¸¬è©¦æ–‡æœ¬æ•¸é‡: 100\n",
      "ä½¿ç”¨è¨­å‚™: cuda:1\n",
      "âœ… ç·¨ç¢¼å®Œæˆ\n",
      "è™•ç†æ™‚é–“: 0.06 ç§’\n",
      "è™•ç†é€Ÿåº¦: 1783.7 å¥å­/ç§’\n",
      "å‘é‡ç¶­åº¦: (100, 384)\n",
      "GPUè¨˜æ†¶é«”ä½¿ç”¨: 0.09 GB\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ CUDAåŠ é€Ÿæ•ˆèƒ½æ¸¬è©¦\n",
    "import time\n",
    "\n",
    "print(\"=== SentenceTransformer CUDA åŠ é€Ÿæ¸¬è©¦ ===\")\n",
    "\n",
    "# åˆå§‹åŒ–è™•ç†å™¨ä¸¦æ¸¬è©¦\n",
    "processor = EnterpriseDocumentProcessor()\n",
    "\n",
    "if processor.semantic_model:\n",
    "    # æº–å‚™æ¸¬è©¦æ–‡æœ¬\n",
    "    test_texts = [\n",
    "        \"This is a test sentence for performance evaluation.\",\n",
    "        \"We are testing the CUDA acceleration capabilities.\",\n",
    "        \"Natural language processing with deep learning models.\",\n",
    "        \"Transformer architectures have revolutionized NLP.\",\n",
    "        \"GPU acceleration provides significant speedup for inference.\"\n",
    "    ] * 20  # 100å€‹å¥å­é€²è¡Œæ¸¬è©¦\n",
    "    \n",
    "    print(f\"æ¸¬è©¦æ–‡æœ¬æ•¸é‡: {len(test_texts)}\")\n",
    "    print(f\"ä½¿ç”¨è¨­å‚™: {processor.semantic_model.device}\")\n",
    "    \n",
    "    # åŸ·è¡Œç·¨ç¢¼ä¸¦æ¸¬é‡æ™‚é–“\n",
    "    start_time = time.time()\n",
    "    embeddings = processor.semantic_model.encode(test_texts, show_progress_bar=False)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    processing_time = end_time - start_time\n",
    "    throughput = len(test_texts) / processing_time\n",
    "    \n",
    "    print(f\"âœ… ç·¨ç¢¼å®Œæˆ\")\n",
    "    print(f\"è™•ç†æ™‚é–“: {processing_time:.2f} ç§’\")\n",
    "    print(f\"è™•ç†é€Ÿåº¦: {throughput:.1f} å¥å­/ç§’\")\n",
    "    print(f\"å‘é‡ç¶­åº¦: {embeddings.shape}\")\n",
    "    \n",
    "    # GPUè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\n",
    "    if 'cuda' in str(processor.semantic_model.device):\n",
    "        import torch\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPUè¨˜æ†¶é«”ä½¿ç”¨: {gpu_memory:.2f} GB\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ SentenceTransformer æ¨¡å‹æœªè¼‰å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:02:51,299 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Docling è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ”„ è¼‰å…¥èªç¾©æ¨¡å‹ (å„ªåŒ–CUDAè¨­å®š)...\n",
      "ç™¼ç¾ 2 å¼µGPU\n",
      "ä½¿ç”¨è¨­å‚™: cuda:1\n",
      "âœ… èªç¾©æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè¨­å‚™: cuda:1\n",
      "âœ… spaCy æ¨¡å‹è¼‰å…¥å®Œæˆ\n",
      "âœ… DocumentProcessor åˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ”„ é–‹å§‹è™•ç†æ–‡æª”: transformer_paper_sample.txt\n",
      "ğŸ§  åŸ·è¡Œåˆ†å¡Š...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0a88818b4a4121bb97c3cd7e18a4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆ 2 å€‹èªç¾©åˆ†å¡Š\n",
      "ğŸ“Š åŸ·è¡Œå“è³ªè©•ä¼°...\n",
      "âœ… æ–‡æª”è™•ç†å®Œæˆ - å“è³ªåˆ†æ•¸: 0.690\n",
      "\n",
      "ğŸ“‹ æ–‡æª”å…ƒè³‡æ–™:\n",
      "æ–‡æª” ID: doc_transformer_paper_sample_1762844574\n",
      "æ¨™é¡Œ: transformer_paper_sample\n",
      "æ–‡æª”é¡å‹: paper\n",
      "æª”æ¡ˆå¤§å°: 1444 bytes\n",
      "è™•ç†ç‹€æ…‹: completed\n",
      "å“è³ªåˆ†æ•¸: 0.690\n",
      "\n",
      "ğŸ“ åˆ†å¡Šè³‡è¨Š:\n",
      "\n",
      "åˆ†å¡Š 1:\n",
      "  ID: doc_transformer_paper_sample_1762844574_chunk_0\n",
      "  å­—æ•¸: 106\n",
      "  å…§å®¹é è¦½: Attention Is All You Need Abstract\n",
      "The dominant sequence transduction models are based on complex re...\n",
      "\n",
      "åˆ†å¡Š 2:\n",
      "  ID: doc_transformer_paper_sample_1762844574_chunk_1\n",
      "  å­—æ•¸: 87\n",
      "  å…§å®¹é è¦½: Self-attention, sometimes called intra-attention, is an attention mechanism relating different posit...\n",
      "\n",
      "ç¸½å…±ç”Ÿæˆ 2 å€‹åˆ†å¡Š\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–è™•ç†å™¨\n",
    "processor = EnterpriseDocumentProcessor()\n",
    "\n",
    "# è™•ç†æ¸¬è©¦æ–‡æª”\n",
    "metadata, chunks, quality_metrics = processor.process_document(str(test_file))\n",
    "\n",
    "print(\"\\nğŸ“‹ æ–‡æª”å…ƒè³‡æ–™:\")\n",
    "print(f\"æ–‡æª” ID: {metadata.document_id}\")\n",
    "print(f\"æ¨™é¡Œ: {metadata.title}\")\n",
    "print(f\"æ–‡æª”é¡å‹: {metadata.document_type}\")\n",
    "print(f\"æª”æ¡ˆå¤§å°: {metadata.file_size} bytes\")\n",
    "print(f\"è™•ç†ç‹€æ…‹: {metadata.processing_status}\")\n",
    "print(f\"å“è³ªåˆ†æ•¸: {metadata.quality_score:.3f}\")\n",
    "\n",
    "print(\"\\nğŸ“ åˆ†å¡Šè³‡è¨Š:\")\n",
    "for i, chunk in enumerate(chunks[:3]):  # é¡¯ç¤ºå‰3å€‹åˆ†å¡Š\n",
    "    print(f\"\\nåˆ†å¡Š {i+1}:\")\n",
    "    print(f\"  ID: {chunk.chunk_id}\")\n",
    "    print(f\"  å­—æ•¸: {chunk.word_count}\")\n",
    "    print(f\"  å…§å®¹é è¦½: {chunk.content[:100]}...\")\n",
    "\n",
    "print(f\"\\nç¸½å…±ç”Ÿæˆ {len(chunks)} å€‹åˆ†å¡Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å“è³ªè©•ä¼°çµæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "lightblue"
         },
         "name": "å¯¦éš›åˆ†æ•¸",
         "type": "bar",
         "x": [
          "æº–ç¢ºæ€§",
          "å®Œæ•´æ€§",
          "å¯è®€æ€§",
          "çµæ§‹æ€§",
          "ç¶œåˆåˆ†æ•¸"
         ],
         "y": [
          0.7574468085106383,
          0.9015544041450777,
          0.10291298076923112,
          1,
          0.6904785483562368
         ]
        },
        {
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "marker": {
          "color": "red",
          "size": 8
         },
         "mode": "markers+lines",
         "name": "å“è³ªé–¾å€¼",
         "type": "scatter",
         "x": [
          "æº–ç¢ºæ€§",
          "å®Œæ•´æ€§",
          "å¯è®€æ€§",
          "çµæ§‹æ€§",
          "ç¶œåˆåˆ†æ•¸"
         ],
         "y": [
          0.7,
          0.8,
          0.6,
          0.7,
          0.7
         ]
        }
       ],
       "layout": {
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "æ–‡æª”å“è³ªè©•ä¼°çµæœ"
        },
        "xaxis": {
         "title": {
          "text": "å“è³ªç¶­åº¦"
         }
        },
        "yaxis": {
         "range": [
          0,
          1
         ],
         "title": {
          "text": "åˆ†æ•¸ [0-1]"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š å“è³ªè©•ä¼°è©³ç´°çµæœ:\n",
      "æº–ç¢ºæ€§: 0.757 (é–¾å€¼: 0.7) âœ… é€šé\n",
      "å®Œæ•´æ€§: 0.902 (é–¾å€¼: 0.8) âœ… é€šé\n",
      "å¯è®€æ€§: 0.103 (é–¾å€¼: 0.6) âŒ æœªé€šé\n",
      "çµæ§‹æ€§: 1.000 (é–¾å€¼: 0.7) âœ… é€šé\n",
      "ç¶œåˆåˆ†æ•¸: 0.690 (é–¾å€¼: 0.7) âŒ æœªé€šé\n"
     ]
    }
   ],
   "source": [
    "# å“è³ªæŒ‡æ¨™å¯è¦–åŒ–\n",
    "quality_data = {\n",
    "    'ç¶­åº¦': ['æº–ç¢ºæ€§', 'å®Œæ•´æ€§', 'å¯è®€æ€§', 'çµæ§‹æ€§', 'ç¶œåˆåˆ†æ•¸'],\n",
    "    'åˆ†æ•¸': [quality_metrics.accuracy_score, \n",
    "            quality_metrics.completeness_score,\n",
    "            quality_metrics.readability_score, \n",
    "            quality_metrics.structure_score,\n",
    "            quality_metrics.overall_score],\n",
    "    'é–¾å€¼': [0.7, 0.8, 0.6, 0.7, 0.7]\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# å¯¦éš›åˆ†æ•¸\n",
    "fig.add_trace(go.Bar(\n",
    "    name='å¯¦éš›åˆ†æ•¸',\n",
    "    x=quality_data['ç¶­åº¦'],\n",
    "    y=quality_data['åˆ†æ•¸'],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "# é–¾å€¼ç·š\n",
    "fig.add_trace(go.Scatter(\n",
    "    name='å“è³ªé–¾å€¼',\n",
    "    x=quality_data['ç¶­åº¦'],\n",
    "    y=quality_data['é–¾å€¼'],\n",
    "    mode='markers+lines',\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    marker=dict(size=8, color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='æ–‡æª”å“è³ªè©•ä¼°çµæœ',\n",
    "    xaxis_title='å“è³ªç¶­åº¦',\n",
    "    yaxis_title='åˆ†æ•¸ [0-1]',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ“Š å“è³ªè©•ä¼°è©³ç´°çµæœ:\")\n",
    "for dim, score, threshold in zip(quality_data['ç¶­åº¦'], quality_data['åˆ†æ•¸'], quality_data['é–¾å€¼']):\n",
    "    status = \"âœ… é€šé\" if score >= threshold else \"âŒ æœªé€šé\"\n",
    "    print(f\"{dim}: {score:.3f} (é–¾å€¼: {threshold}) {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª å¯¦ä½œç·´ç¿’ 2: æ‰¹æ¬¡æ–‡æª”è™•ç†\n",
    "\n",
    "### æ‰¹æ¬¡è™•ç†ç®¡ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BatchDocumentProcessor å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class BatchDocumentProcessor:\n",
    "    \"\"\"\n",
    "    æ‰¹æ¬¡æ–‡æª”è™•ç†å™¨\n",
    "    æ”¯æ´ç›®éŒ„ç´šåˆ¥çš„æ–‡æª”è™•ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor: EnterpriseDocumentProcessor):\n",
    "        self.processor = processor\n",
    "        self.processing_results = []\n",
    "    \n",
    "    def process_directory(self, directory_path: str, file_patterns: List[str] = ['*.pdf', '*.txt', '*.docx']) -> Dict:\n",
    "        \"\"\"\n",
    "        è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„æ–‡æª”\n",
    "        \"\"\"\n",
    "        directory = Path(directory_path)\n",
    "        if not directory.exists():\n",
    "            raise ValueError(f\"ç›®éŒ„ä¸å­˜åœ¨: {directory_path}\")\n",
    "        \n",
    "        # æ”¶é›†éœ€è¦è™•ç†çš„æ–‡ä»¶\n",
    "        files_to_process = []\n",
    "        for pattern in file_patterns:\n",
    "            files_to_process.extend(directory.glob(pattern))\n",
    "        \n",
    "        print(f\"ğŸ” æ‰¾åˆ° {len(files_to_process)} å€‹æ–‡ä»¶å¾…è™•ç†\")\n",
    "        \n",
    "        # æ‰¹æ¬¡è™•ç†\n",
    "        results = {\n",
    "            'processed_files': [],\n",
    "            'failed_files': [],\n",
    "            'total_chunks': 0,\n",
    "            'average_quality': 0.0\n",
    "        }\n",
    "        \n",
    "        quality_scores = []\n",
    "        \n",
    "        for i, file_path in enumerate(files_to_process):\n",
    "            print(f\"\\nğŸ“„ è™•ç† {i+1}/{len(files_to_process)}: {file_path.name}\")\n",
    "            \n",
    "            try:\n",
    "                metadata, chunks, quality = self.processor.process_document(str(file_path))\n",
    "                \n",
    "                if metadata.processing_status == 'completed':\n",
    "                    results['processed_files'].append({\n",
    "                        'file_path': str(file_path),\n",
    "                        'metadata': metadata.to_dict(),\n",
    "                        'chunk_count': len(chunks),\n",
    "                        'quality_score': quality.overall_score\n",
    "                    })\n",
    "                    results['total_chunks'] += len(chunks)\n",
    "                    quality_scores.append(quality.overall_score)\n",
    "                else:\n",
    "                    results['failed_files'].append(str(file_path))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è™•ç†å¤±æ•—: {e}\")\n",
    "                results['failed_files'].append(str(file_path))\n",
    "        \n",
    "        # è¨ˆç®—å¹³å‡å“è³ª\n",
    "        if quality_scores:\n",
    "            results['average_quality'] = np.mean(quality_scores)\n",
    "        \n",
    "        self.processing_results = results\n",
    "        return results\n",
    "    \n",
    "    def generate_report(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆè™•ç†å ±å‘Š\n",
    "        \"\"\"\n",
    "        if not self.processing_results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        report_data = []\n",
    "        for file_info in self.processing_results['processed_files']:\n",
    "            metadata = file_info['metadata']\n",
    "            report_data.append({\n",
    "                'æ–‡ä»¶å': Path(file_info['file_path']).name,\n",
    "                'æ–‡æª”é¡å‹': metadata['document_type'],\n",
    "                'æª”æ¡ˆå¤§å°(KB)': round(metadata['file_size'] / 1024, 2),\n",
    "                'åˆ†å¡Šæ•¸é‡': file_info['chunk_count'],\n",
    "                'å“è³ªåˆ†æ•¸': round(file_info['quality_score'], 3),\n",
    "                'è™•ç†ç‹€æ…‹': metadata['processing_status']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(report_data)\n",
    "\n",
    "print(\"âœ… BatchDocumentProcessor å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œæ‰¹æ¬¡è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å»ºç«‹äº† 2 å€‹é¡å¤–æ¸¬è©¦æ–‡ä»¶\n",
      "ğŸ” æ‰¾åˆ° 3 å€‹æ–‡ä»¶å¾…è™•ç†\n",
      "\n",
      "ğŸ“„ è™•ç† 1/3: transformer_paper_sample.txt\n",
      "ğŸ”„ é–‹å§‹è™•ç†æ–‡æª”: transformer_paper_sample.txt\n",
      "ğŸ§  åŸ·è¡Œåˆ†å¡Š...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38223799a8914f4480417c6705d51753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆ 2 å€‹èªç¾©åˆ†å¡Š\n",
      "ğŸ“Š åŸ·è¡Œå“è³ªè©•ä¼°...\n",
      "âœ… æ–‡æª”è™•ç†å®Œæˆ - å“è³ªåˆ†æ•¸: 0.690\n",
      "\n",
      "ğŸ“„ è™•ç† 2/3: bert_paper_sample.txt\n",
      "ğŸ”„ é–‹å§‹è™•ç†æ–‡æª”: bert_paper_sample.txt\n",
      "ğŸ§  åŸ·è¡Œåˆ†å¡Š...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f5ccf718474ebe9aef3a3856b4feff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆ 1 å€‹èªç¾©åˆ†å¡Š\n",
      "ğŸ“Š åŸ·è¡Œå“è³ªè©•ä¼°...\n",
      "âœ… æ–‡æª”è™•ç†å®Œæˆ - å“è³ªåˆ†æ•¸: 0.731\n",
      "\n",
      "ğŸ“„ è™•ç† 3/3: gpt3_paper_sample.txt\n",
      "ğŸ”„ é–‹å§‹è™•ç†æ–‡æª”: gpt3_paper_sample.txt\n",
      "ğŸ§  åŸ·è¡Œåˆ†å¡Š...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724bd490d8864abaa6271939976f6816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆ 1 å€‹èªç¾©åˆ†å¡Š\n",
      "ğŸ“Š åŸ·è¡Œå“è³ªè©•ä¼°...\n",
      "âœ… æ–‡æª”è™•ç†å®Œæˆ - å“è³ªåˆ†æ•¸: 0.735\n",
      "\n",
      "ğŸ“Š æ‰¹æ¬¡è™•ç†çµæœ:\n",
      "æˆåŠŸè™•ç†: 3 å€‹æ–‡ä»¶\n",
      "è™•ç†å¤±æ•—: 0 å€‹æ–‡ä»¶\n",
      "ç¸½åˆ†å¡Šæ•¸: 4\n",
      "å¹³å‡å“è³ª: 0.719\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹æ›´å¤šæ¸¬è©¦æ–‡ä»¶\n",
    "test_docs = [\n",
    "    (\"bert_paper_sample.txt\", \"\"\"\n",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "Abstract\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
    "\n",
    "1. Introduction\n",
    "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence classification tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically.\n",
    "\"\"\"),\n",
    "    (\"gpt3_paper_sample.txt\", \"\"\"\n",
    "Language Models are Few-Shot Learners\n",
    "\n",
    "Abstract\n",
    "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples.\n",
    "\n",
    "By contrast, humans can generally perform a new language task from only a few examples or from simple instructions. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦æ–‡ä»¶\n",
    "for filename, content in test_docs:\n",
    "    test_file = test_dir / filename\n",
    "    with open(test_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"âœ… å»ºç«‹äº† {len(test_docs)} å€‹é¡å¤–æ¸¬è©¦æ–‡ä»¶\")\n",
    "\n",
    "# åŸ·è¡Œæ‰¹æ¬¡è™•ç†\n",
    "batch_processor = BatchDocumentProcessor(processor)\n",
    "batch_results = batch_processor.process_directory(str(test_dir))\n",
    "\n",
    "print(\"\\nğŸ“Š æ‰¹æ¬¡è™•ç†çµæœ:\")\n",
    "print(f\"æˆåŠŸè™•ç†: {len(batch_results['processed_files'])} å€‹æ–‡ä»¶\")\n",
    "print(f\"è™•ç†å¤±æ•—: {len(batch_results['failed_files'])} å€‹æ–‡ä»¶\")\n",
    "print(f\"ç¸½åˆ†å¡Šæ•¸: {batch_results['total_chunks']}\")\n",
    "print(f\"å¹³å‡å“è³ª: {batch_results['average_quality']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰¹æ¬¡è™•ç†å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ æ‰¹æ¬¡è™•ç†å ±å‘Š:\n",
      "                         æ–‡ä»¶å  æ–‡æª”é¡å‹  æª”æ¡ˆå¤§å°(KB)  åˆ†å¡Šæ•¸é‡  å“è³ªåˆ†æ•¸      è™•ç†ç‹€æ…‹\n",
      "transformer_paper_sample.txt paper      1.41     2 0.690 completed\n",
      "       bert_paper_sample.txt paper      0.73     1 0.731 completed\n",
      "       gpt3_paper_sample.txt paper      0.57     1 0.735 completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "æ–‡ä»¶å=%{x}<br>å“è³ªåˆ†æ•¸=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "FK5H4XoU5j8xCKwcWmTnP4XrUbgehec/",
           "dtype": "f8"
          },
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "transformer_paper_sample.txt",
          "bert_paper_sample.txt",
          "gpt3_paper_sample.txt"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "FK5H4XoU5j8xCKwcWmTnP4XrUbgehec/",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "å“è³ªé–¾å€¼ (0.7)",
          "x": 1,
          "xanchor": "right",
          "xref": "x domain",
          "y": 0.7,
          "yanchor": "bottom",
          "yref": "y"
         }
        ],
        "barmode": "relative",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "å“è³ªåˆ†æ•¸"
          }
         },
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ]
        },
        "height": 500,
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 0.7,
          "y1": 0.7,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "å„æ–‡æª”å“è³ªåˆ†æ•¸æ¯”è¼ƒ"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickangle": -45,
         "title": {
          "text": "æ–‡ä»¶å"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "å“è³ªåˆ†æ•¸"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ çµ±è¨ˆæ‘˜è¦:\n",
      "å¹³å‡å“è³ªåˆ†æ•¸: 0.719\n",
      "å“è³ªåˆ†æ•¸æ¨™æº–å·®: 0.025\n",
      "é€šéå“è³ªé–¾å€¼(0.7)çš„æ–‡æª”: 2/3\n"
     ]
    }
   ],
   "source": [
    "# ç”Ÿæˆè™•ç†å ±å‘Š\n",
    "report_df = batch_processor.generate_report()\n",
    "\n",
    "print(\"ğŸ“‹ æ‰¹æ¬¡è™•ç†å ±å‘Š:\")\n",
    "print(report_df.to_string(index=False))\n",
    "\n",
    "# å“è³ªåˆ†ä½ˆå¯è¦–åŒ–\n",
    "if not report_df.empty:\n",
    "    fig = px.bar(\n",
    "        report_df, \n",
    "        x='æ–‡ä»¶å', \n",
    "        y='å“è³ªåˆ†æ•¸',\n",
    "        title='å„æ–‡æª”å“è³ªåˆ†æ•¸æ¯”è¼ƒ',\n",
    "        color='å“è³ªåˆ†æ•¸',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig.add_hline(y=0.7, line_dash=\"dash\", line_color=\"red\", \n",
    "                  annotation_text=\"å“è³ªé–¾å€¼ (0.7)\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    # çµ±è¨ˆæ‘˜è¦\n",
    "    print(\"\\nğŸ“ˆ çµ±è¨ˆæ‘˜è¦:\")\n",
    "    print(f\"å¹³å‡å“è³ªåˆ†æ•¸: {report_df['å“è³ªåˆ†æ•¸'].mean():.3f}\")\n",
    "    print(f\"å“è³ªåˆ†æ•¸æ¨™æº–å·®: {report_df['å“è³ªåˆ†æ•¸'].std():.3f}\")\n",
    "    print(f\"é€šéå“è³ªé–¾å€¼(0.7)çš„æ–‡æª”: {(report_df['å“è³ªåˆ†æ•¸'] >= 0.7).sum()}/{len(report_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å­¸ç¿’æª¢æ ¸èˆ‡ä¸‹ä¸€æ­¥\n",
    "\n",
    "### å­¸ç¿’æˆæœæª¢æ ¸\n",
    "\n",
    "å®Œæˆæœ¬æ¨¡çµ„å¾Œï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š\n",
    "\n",
    "âœ… **æ–‡æª”æ”å–æ¶æ§‹è¨­è¨ˆ**\n",
    "- æ¨™æº–åŒ–å…ƒè³‡æ–™æ¨¡å‹\n",
    "- å¯æ“´å±•çš„è™•ç†å™¨æ¶æ§‹\n",
    "- éŒ¯èª¤è™•ç†æ©Ÿåˆ¶\n",
    "\n",
    "âœ… **Docling æ•´åˆæ‡‰ç”¨**\n",
    "- å¤šæ ¼å¼æ–‡æª”è§£æ\n",
    "- å…§å®¹æå–èˆ‡çµæ§‹åŒ–\n",
    "- å…ƒè³‡æ–™è‡ªå‹•æå–\n",
    "\n",
    "âœ… **èªç¾©åˆ†å¡Šæ¼”ç®—æ³•**\n",
    "- åŸºæ–¼ç›¸ä¼¼åº¦çš„é‚Šç•Œæª¢æ¸¬\n",
    "- å‹•æ…‹åˆ†å¡Šç­–ç•¥\n",
    "- èªç¾©é€£è²«æ€§ä¿æŒ\n",
    "\n",
    "âœ… **å“è³ªè©•ä¼°æ©Ÿåˆ¶**\n",
    "- å¤šç¶­åº¦å“è³ªæŒ‡æ¨™\n",
    "- è‡ªå‹•åŒ–è©•åˆ†ç®—æ³•\n",
    "- é–¾å€¼æª¢æŸ¥èˆ‡å ±å‘Š\n",
    "\n",
    "### é€²éšæŒ‘æˆ°ç·´ç¿’\n",
    "\n",
    "1. **å¤šèªè¨€æ”¯æ´**: æ“´å±•è™•ç†å™¨æ”¯æ´ä¸­æ–‡æ–‡æª”\n",
    "2. **è¡¨æ ¼è™•ç†**: åŠ å¼·è¡¨æ ¼å…§å®¹çš„çµæ§‹åŒ–æå–\n",
    "3. **åœ–åƒè™•ç†**: æ•´åˆ OCR è™•ç†æƒææ–‡æª”\n",
    "4. **ä¸¦ç™¼è™•ç†**: å¯¦ä½œ async ä¸¦ç™¼è™•ç†èƒ½åŠ›\n",
    "\n",
    "### ä¸‹ä¸€æ¨¡çµ„é å‘Š\n",
    "\n",
    "**æ¨¡çµ„ 2: å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹**\n",
    "- ä¼æ¥­ç´šå…ƒè³‡æ–™ schema è¨­è¨ˆ\n",
    "- å‘é‡ç´¢å¼•å»ºç«‹èˆ‡ç®¡ç†\n",
    "- æ··åˆæª¢ç´¢ç­–ç•¥å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç·´ç¿’çµæœå·²å„²å­˜\n",
      "ğŸ“‚ çµæœä½ç½®: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance/notebooks/01_document_ingestion/exercises\n",
      "\n",
      "ğŸ‰ æ¨¡çµ„ 1 å®Œæˆï¼æº–å‚™é€²å…¥æ¨¡çµ„ 2: å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹\n"
     ]
    }
   ],
   "source": [
    "# å„²å­˜æœ¬æ¬¡ç·´ç¿’çµæœ\n",
    "results_dir = PROJECT_ROOT / 'notebooks' / '01_document_ingestion' / 'exercises'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å„²å­˜è™•ç†çµæœ\n",
    "import pickle\n",
    "\n",
    "with open(results_dir / 'exercise_results.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'metadata': metadata,\n",
    "        'chunks': chunks,\n",
    "        'quality_metrics': quality_metrics,\n",
    "        'batch_results': batch_results\n",
    "    }, f)\n",
    "\n",
    "# å„²å­˜å ±å‘Š\n",
    "report_df.to_csv(results_dir / 'processing_report.csv', index=False)\n",
    "\n",
    "print(\"âœ… ç·´ç¿’çµæœå·²å„²å­˜\")\n",
    "print(f\"ğŸ“‚ çµæœä½ç½®: {results_dir}\")\n",
    "print(\"\\nğŸ‰ æ¨¡çµ„ 1 å®Œæˆï¼æº–å‚™é€²å…¥æ¨¡çµ„ 2: å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ou8ufievzu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç·´ç¿’çµæœå·²å„²å­˜\n",
      "ğŸ“‚ çµæœä½ç½®: /home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance/notebooks/01_document_ingestion/exercises\n",
      "\n",
      "ğŸ‰ æ¨¡çµ„ 1 å®Œæˆï¼æº–å‚™é€²å…¥æ¨¡çµ„ 2: å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹\n"
     ]
    }
   ],
   "source": [
    "# å„²å­˜æœ¬æ¬¡ç·´ç¿’çµæœ\n",
    "results_dir = PROJECT_ROOT / 'notebooks' / '01_document_ingestion' / 'exercises'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å„²å­˜è™•ç†çµæœ\n",
    "import pickle\n",
    "\n",
    "with open(results_dir / 'exercise_results.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'metadata': metadata,\n",
    "        'chunks': chunks,\n",
    "        'quality_metrics': quality_metrics,\n",
    "        'batch_results': batch_results\n",
    "    }, f)\n",
    "\n",
    "# å„²å­˜å ±å‘Š\n",
    "report_df.to_csv(results_dir / 'processing_report.csv', index=False)\n",
    "\n",
    "print(\"âœ… ç·´ç¿’çµæœå·²å„²å­˜\")\n",
    "print(f\"ğŸ“‚ çµæœä½ç½®: {results_dir}\")\n",
    "print(\"\\nğŸ‰ æ¨¡çµ„ 1 å®Œæˆï¼æº–å‚™é€²å…¥æ¨¡çµ„ 2: å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}