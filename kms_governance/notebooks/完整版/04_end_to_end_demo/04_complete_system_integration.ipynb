{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡çµ„ 4: ç«¯åˆ°ç«¯ç³»çµ±æ•´åˆæ¼”ç¤º\n",
    "## Module 4: End-to-End System Integration Demo\n",
    "\n",
    "> **æ•™å­¸ç›®æ¨™**: æ•´åˆå‰ä¸‰å€‹æ¨¡çµ„ï¼Œå»ºç«‹å®Œæ•´çš„ä¼æ¥­ç´šè³‡æ–™æ²»ç†ç³»çµ±ï¼Œå±•ç¤ºç”Ÿç”¢ç´šéƒ¨ç½²èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’æˆæœ (Learning Outcomes)\n",
    "\n",
    "å®Œæˆæœ¬æ¨¡çµ„å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š\n",
    "1. **ç³»çµ±æ•´åˆè¨­è¨ˆ** - å°‡æ–‡æª”è™•ç†ã€å…ƒè³‡æ–™ç®¡ç†ã€å“è³ªæ§åˆ¶æ•´åˆç‚ºçµ±ä¸€ç³»çµ±\n",
    "2. **ç”Ÿç”¢ç´šéƒ¨ç½²** - å¯¦ç¾ Docker å®¹å™¨åŒ–ã€API æœå‹™åŒ–ã€ç›£æ§å‘Šè­¦\n",
    "3. **æ€§èƒ½å„ªåŒ–** - å¤§è¦æ¨¡æ–‡æª”è™•ç†çš„ä¸¦ç™¼å„ªåŒ–å’Œè³‡æºç®¡ç†\n",
    "4. **é‹ç¶­ç®¡ç†** - å»ºç«‹å¥åº·æª¢æŸ¥ã€æ—¥èªŒåˆ†æã€ç•°å¸¸æ¢å¾©æ©Ÿåˆ¶\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ æ•´åˆç³»çµ±æ¶æ§‹\n",
    "\n",
    "### å®Œæ•´è³‡æ–™æµç¨‹\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[åŸå§‹æ–‡æª”] --> B[æ–‡æª”æ”å–æœå‹™]\n",
    "    B --> C[æ–‡æª”è™•ç†å¼•æ“]\n",
    "    C --> D[å“è³ªè©•ä¼°å¼•æ“]\n",
    "    D --> E{å“è³ªæª¢æŸ¥}\n",
    "    E -->|é€šé| F[å…ƒè³‡æ–™ç®¡ç†]\n",
    "    E -->|å¤±æ•—| G[ç•°å¸¸è™•ç†]\n",
    "    F --> H[å‘é‡ç´¢å¼•]\n",
    "    H --> I[æœç´¢å¼•æ“]\n",
    "    I --> J[RAG ç³»çµ±]\n",
    "    \n",
    "    G --> K[äººå·¥å¯©æ ¸]\n",
    "    K --> C\n",
    "    \n",
    "    L[ç›£æ§ç³»çµ±] --> M[å“è³ªå„€è¡¨æ¿]\n",
    "    L --> N[ç•°å¸¸å‘Šè­¦]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style J fill:#e8f5e9\n",
    "    style M fill:#fff3e0\n",
    "    style N fill:#ffebee\n",
    "```\n",
    "\n",
    "### æ ¸å¿ƒçµ„ä»¶\n",
    "\n",
    "1. **æ–‡æª”æ”å–å±¤** - å¤šæ ¼å¼æ–‡æª”çš„è‡ªå‹•åŒ–æ”å–å’Œé è™•ç†\n",
    "2. **è™•ç†å¼•æ“å±¤** - èªç¾©åˆ†å¡Šã€å…§å®¹æå–ã€ç‰¹å¾µç”Ÿæˆ  \n",
    "3. **å“è³ªç®¡æ§å±¤** - å¤šç¶­åº¦å“è³ªè©•ä¼°ã€ç•°å¸¸æª¢æ¸¬ã€è‡ªå‹•ä¿®å¾©\n",
    "4. **å­˜å„²ç®¡ç†å±¤** - å…ƒè³‡æ–™å­˜å„²ã€å‘é‡ç´¢å¼•ã€ç‰ˆæœ¬æ§åˆ¶\n",
    "5. **æœå‹™ä»‹é¢å±¤** - RESTful APIã€GraphQLã€WebSocket\n",
    "6. **ç›£æ§é‹ç¶­å±¤** - å¥åº·æª¢æŸ¥ã€æ€§èƒ½ç›£æ§ã€æ—¥èªŒåˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç’°å¢ƒè¨­å®šèˆ‡ä¾è³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å®Œæ•´ç³»çµ±ä¾è³´\n",
    "!pip install fastapi uvicorn websockets\n",
    "!pip install redis celery flower\n",
    "!pip install prometheus-client grafana-api\n",
    "!pip install docker python-multipart\n",
    "!pip install aiofiles asyncpg\n",
    "!pip install structlog rich\n",
    "\n",
    "# ç³»çµ±ç›£æ§\n",
    "!pip install psutil py-cpuinfo\n",
    "!pip install healthcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å®Œæ•´ç³»çµ±ä¾è³´\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple, Any, AsyncGenerator\n",
    "from dataclasses import dataclass, asdict\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Web æ¡†æ¶èˆ‡ API\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, File, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "import uvicorn\n",
    "\n",
    "# éåŒæ­¥è™•ç†\n",
    "import aiofiles\n",
    "import aioredis\n",
    "from celery import Celery\n",
    "\n",
    "# è³‡æ–™è™•ç†èˆ‡åˆ†æ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# ç›£æ§èˆ‡æ—¥èªŒ\n",
    "import structlog\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "import psutil\n",
    "import py_cpuinfo\n",
    "\n",
    "# åŒ¯å…¥å‰é¢æ¨¡çµ„çš„é¡åˆ¥\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path('/home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance')\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# è¨­å®šçµæ§‹åŒ–æ—¥èªŒ\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.stdlib.filter_by_level,\n",
    "        structlog.stdlib.add_logger_name,\n",
    "        structlog.stdlib.add_log_level,\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.dev.ConsoleRenderer()\n",
    "    ],\n",
    "    wrapper_class=structlog.stdlib.BoundLogger,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "    cache_logger_on_first_use=True,\n",
    ")\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "console = Console()\n",
    "\n",
    "print(\"âœ… å®Œæ•´ç³»çµ±ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"ğŸ“‚ å°ˆæ¡ˆæ ¹ç›®éŒ„: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ’» CPU æ ¸å¿ƒæ•¸: {multiprocessing.cpu_count()}\")\n",
    "print(f\"ğŸ’¾ ç³»çµ±è¨˜æ†¶é«”: {psutil.virtual_memory().total / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ çµ±ä¸€ç³»çµ±ç®¡ç†å™¨\n",
    "\n",
    "### ä¼æ¥­ç´šæ–‡æª”æ²»ç†ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfig:\n",
    "    \"\"\"ç³»çµ±é…ç½®\"\"\"\n",
    "    # è³‡æ–™åº«é…ç½®\n",
    "    database_url: str = \"sqlite+aiosqlite:///./kms_governance.db\"\n",
    "    vector_db_path: str = \"./vector_index\"\n",
    "    \n",
    "    # è™•ç†é…ç½®\n",
    "    max_workers: int = 4\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 50\n",
    "    \n",
    "    # å“è³ªé…ç½®\n",
    "    min_quality_threshold: float = 0.7\n",
    "    enable_auto_correction: bool = True\n",
    "    \n",
    "    # API é…ç½®\n",
    "    api_host: str = \"0.0.0.0\"\n",
    "    api_port: int = 8000\n",
    "    enable_cors: bool = True\n",
    "    \n",
    "    # ç›£æ§é…ç½®\n",
    "    enable_metrics: bool = True\n",
    "    log_level: str = \"INFO\"\n",
    "    health_check_interval: int = 30\n",
    "\n",
    "class EnterpriseDocumentGovernanceSystem:\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ç´šæ–‡æª”æ²»ç†ç³»çµ±\n",
    "    æ•´åˆæ–‡æª”è™•ç†ã€å…ƒè³‡æ–™ç®¡ç†ã€å“è³ªæ§åˆ¶çš„å®Œæ•´è§£æ±ºæ–¹æ¡ˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: SystemConfig):\n",
    "        \"\"\"åˆå§‹åŒ–ç³»çµ±\"\"\"\n",
    "        self.config = config\n",
    "        self.is_initialized = False\n",
    "        self.is_running = False\n",
    "        \n",
    "        # ç³»çµ±çµ„ä»¶\n",
    "        self.document_processor = None\n",
    "        self.metadata_manager = None\n",
    "        self.vector_manager = None\n",
    "        self.quality_assessor = None\n",
    "        self.anomaly_detector = None\n",
    "        \n",
    "        # åŸ·è¡Œå™¨\n",
    "        self.thread_executor = ThreadPoolExecutor(max_workers=config.max_workers)\n",
    "        self.process_executor = ProcessPoolExecutor(max_workers=min(4, config.max_workers))\n",
    "        \n",
    "        # æŒ‡æ¨™æ”¶é›†\n",
    "        self._init_metrics()\n",
    "        \n",
    "        # ç³»çµ±ç‹€æ…‹\n",
    "        self.system_stats = {\n",
    "            'start_time': datetime.now(),\n",
    "            'documents_processed': 0,\n",
    "            'quality_assessments': 0,\n",
    "            'anomalies_detected': 0,\n",
    "            'api_requests': 0\n",
    "        }\n",
    "        \n",
    "        logger.info(\"ä¼æ¥­ç´šæ–‡æª”æ²»ç†ç³»çµ±åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _init_metrics(self):\n",
    "        \"\"\"åˆå§‹åŒ– Prometheus æŒ‡æ¨™\"\"\"\n",
    "        if not self.config.enable_metrics:\n",
    "            return\n",
    "            \n",
    "        self.metrics = {\n",
    "            'documents_processed': Counter(\n",
    "                'documents_processed_total', \n",
    "                'Total number of documents processed',\n",
    "                ['status', 'type']\n",
    "            ),\n",
    "            'processing_duration': Histogram(\n",
    "                'document_processing_duration_seconds',\n",
    "                'Time spent processing documents',\n",
    "                ['stage']\n",
    "            ),\n",
    "            'quality_score': Histogram(\n",
    "                'document_quality_score',\n",
    "                'Document quality scores',\n",
    "                buckets=[0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "            ),\n",
    "            'system_health': Gauge(\n",
    "                'system_health_score',\n",
    "                'Overall system health score'\n",
    "            ),\n",
    "            'active_connections': Gauge(\n",
    "                'active_api_connections',\n",
    "                'Number of active API connections'\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    async def initialize_system(self) -> bool:\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ‰€æœ‰ç³»çµ±çµ„ä»¶\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"æ­£åœ¨åˆå§‹åŒ–ç³»çµ±çµ„ä»¶...\")\n",
    "            \n",
    "            with Progress(\n",
    "                SpinnerColumn(),\n",
    "                TextColumn(\"[progress.description]{task.description}\"),\n",
    "                console=console\n",
    "            ) as progress:\n",
    "                \n",
    "                # 1. åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨\n",
    "                task1 = progress.add_task(\"åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨...\", total=None)\n",
    "                # é€™è£¡éœ€è¦å¯¦éš›çš„å¯¦ä½œï¼Œç°¡åŒ–ç‚ºæ¨¡æ“¬\n",
    "                await asyncio.sleep(1)\n",
    "                self.document_processor = \"DocumentProcessor(config)\"\n",
    "                progress.update(task1, description=\"âœ… æ–‡æª”è™•ç†å™¨å·²å°±ç·’\")\n",
    "                \n",
    "                # 2. åˆå§‹åŒ–å…ƒè³‡æ–™ç®¡ç†å™¨\n",
    "                task2 = progress.add_task(\"åˆå§‹åŒ–å…ƒè³‡æ–™ç®¡ç†å™¨...\", total=None)\n",
    "                await asyncio.sleep(1)\n",
    "                self.metadata_manager = \"MetadataManager(config.database_url)\"\n",
    "                progress.update(task2, description=\"âœ… å…ƒè³‡æ–™ç®¡ç†å™¨å·²å°±ç·’\")\n",
    "                \n",
    "                # 3. åˆå§‹åŒ–å‘é‡ç´¢å¼•\n",
    "                task3 = progress.add_task(\"åˆå§‹åŒ–å‘é‡ç´¢å¼•...\", total=None)\n",
    "                await asyncio.sleep(1)\n",
    "                self.vector_manager = \"VectorManager(config.vector_db_path)\"\n",
    "                progress.update(task3, description=\"âœ… å‘é‡ç´¢å¼•å·²å°±ç·’\")\n",
    "                \n",
    "                # 4. åˆå§‹åŒ–å“è³ªè©•ä¼°å™¨\n",
    "                task4 = progress.add_task(\"åˆå§‹åŒ–å“è³ªè©•ä¼°å™¨...\", total=None)\n",
    "                await asyncio.sleep(1)\n",
    "                self.quality_assessor = \"QualityAssessor()\"\n",
    "                progress.update(task4, description=\"âœ… å“è³ªè©•ä¼°å™¨å·²å°±ç·’\")\n",
    "                \n",
    "                # 5. åˆå§‹åŒ–ç•°å¸¸æª¢æ¸¬å™¨\n",
    "                task5 = progress.add_task(\"åˆå§‹åŒ–ç•°å¸¸æª¢æ¸¬å™¨...\", total=None)\n",
    "                await asyncio.sleep(1)\n",
    "                self.anomaly_detector = \"AnomalyDetector(config)\"\n",
    "                progress.update(task5, description=\"âœ… ç•°å¸¸æª¢æ¸¬å™¨å·²å°±ç·’\")\n",
    "            \n",
    "            self.is_initialized = True\n",
    "            logger.info(\"æ‰€æœ‰ç³»çµ±çµ„ä»¶åˆå§‹åŒ–å®Œæˆ\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"ç³»çµ±åˆå§‹åŒ–å¤±æ•—\", error=str(e))\n",
    "            return False\n",
    "    \n",
    "    async def process_document_batch(self, \n",
    "                                   file_paths: List[str],\n",
    "                                   metadata_list: List[Dict] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        æ‰¹æ¬¡è™•ç†æ–‡æª”\n",
    "        \n",
    "        Args:\n",
    "            file_paths: æ–‡æª”è·¯å¾‘åˆ—è¡¨\n",
    "            metadata_list: å°æ‡‰çš„å…ƒè³‡æ–™åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            Dict: æ‰¹æ¬¡è™•ç†çµæœ\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise RuntimeError(\"ç³»çµ±å°šæœªåˆå§‹åŒ–\")\n",
    "        \n",
    "        logger.info(f\"é–‹å§‹æ‰¹æ¬¡è™•ç† {len(file_paths)} å€‹æ–‡æª”\")\n",
    "        \n",
    "        results = {\n",
    "            'processed': [],\n",
    "            'failed': [],\n",
    "            'quality_issues': [],\n",
    "            'anomalies': [],\n",
    "            'summary': {\n",
    "                'total': len(file_paths),\n",
    "                'success_count': 0,\n",
    "                'failure_count': 0,\n",
    "                'avg_quality_score': 0.0,\n",
    "                'processing_time': 0.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # ä¸¦ç™¼è™•ç†æ–‡æª”\n",
    "        tasks = []\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            metadata = metadata_list[i] if metadata_list else {}\n",
    "            task = self._process_single_document(file_path, metadata)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ\n",
    "        with Progress(console=console) as progress:\n",
    "            task_id = progress.add_task(\"è™•ç†æ–‡æª”ä¸­...\", total=len(tasks))\n",
    "            \n",
    "            for i, task in enumerate(asyncio.as_completed(tasks)):\n",
    "                try:\n",
    "                    result = await task\n",
    "                    \n",
    "                    if result['status'] == 'success':\n",
    "                        results['processed'].append(result)\n",
    "                        results['summary']['success_count'] += 1\n",
    "                        \n",
    "                        # å“è³ªæª¢æŸ¥\n",
    "                        if result.get('quality_score', 1.0) < self.config.min_quality_threshold:\n",
    "                            results['quality_issues'].append(result)\n",
    "                            \n",
    "                    else:\n",
    "                        results['failed'].append(result)\n",
    "                        results['summary']['failure_count'] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"æ–‡æª”è™•ç†å¤±æ•—: {e}\")\n",
    "                    results['failed'].append({\n",
    "                        'file_path': file_paths[i] if i < len(file_paths) else 'unknown',\n",
    "                        'error': str(e),\n",
    "                        'status': 'error'\n",
    "                    })\n",
    "                    results['summary']['failure_count'] += 1\n",
    "                \n",
    "                progress.update(task_id, advance=1)\n",
    "        \n",
    "        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š\n",
    "        end_time = datetime.now()\n",
    "        processing_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if results['processed']:\n",
    "            avg_quality = np.mean([r.get('quality_score', 0) for r in results['processed']])\n",
    "            results['summary']['avg_quality_score'] = avg_quality\n",
    "        \n",
    "        results['summary']['processing_time'] = processing_time\n",
    "        \n",
    "        # æ›´æ–°ç³»çµ±çµ±è¨ˆ\n",
    "        self.system_stats['documents_processed'] += results['summary']['success_count']\n",
    "        \n",
    "        # æ›´æ–°æŒ‡æ¨™\n",
    "        if self.config.enable_metrics:\n",
    "            self.metrics['documents_processed'].labels(\n",
    "                status='success', type='batch'\n",
    "            ).inc(results['summary']['success_count'])\n",
    "            \n",
    "            self.metrics['processing_duration'].labels(\n",
    "                stage='batch_processing'\n",
    "            ).observe(processing_time)\n",
    "        \n",
    "        logger.info(f\"æ‰¹æ¬¡è™•ç†å®Œæˆ: {results['summary']}\")\n",
    "        return results\n",
    "    \n",
    "    async def _process_single_document(self, file_path: str, metadata: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        è™•ç†å–®ä¸€æ–‡æª”\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # æ¨¡æ“¬æ–‡æª”è™•ç†éç¨‹\n",
    "            await asyncio.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“\n",
    "            \n",
    "            # æ¨¡æ“¬è™•ç†çµæœ\n",
    "            result = {\n",
    "                'file_path': file_path,\n",
    "                'document_id': f\"doc_{hash(file_path) % 10000}\",\n",
    "                'status': 'success',\n",
    "                'quality_score': np.random.uniform(0.3, 0.95),\n",
    "                'chunk_count': np.random.randint(3, 15),\n",
    "                'processing_time': (datetime.now() - start_time).total_seconds(),\n",
    "                'metadata': metadata\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'file_path': file_path,\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time': (datetime.now() - start_time).total_seconds()\n",
    "            }\n",
    "    \n",
    "    async def get_system_health(self) -> Dict:\n",
    "        \"\"\"\n",
    "        ç²å–ç³»çµ±å¥åº·ç‹€æ…‹\n",
    "        \"\"\"\n",
    "        # ç²å–ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³\n",
    "        cpu_usage = psutil.cpu_percent(interval=1)\n",
    "        memory = psutil.virtual_memory()\n",
    "        disk = psutil.disk_usage('/')\n",
    "        \n",
    "        # è¨ˆç®—ç³»çµ±å¥åº·åˆ†æ•¸\n",
    "        health_score = self._calculate_health_score(cpu_usage, memory.percent, disk.percent)\n",
    "        \n",
    "        uptime = datetime.now() - self.system_stats['start_time']\n",
    "        \n",
    "        health_status = {\n",
    "            'status': 'healthy' if health_score > 0.8 else 'warning' if health_score > 0.6 else 'critical',\n",
    "            'health_score': health_score,\n",
    "            'uptime_seconds': uptime.total_seconds(),\n",
    "            'system_resources': {\n",
    "                'cpu_usage_percent': cpu_usage,\n",
    "                'memory_usage_percent': memory.percent,\n",
    "                'disk_usage_percent': disk.percent,\n",
    "                'available_memory_gb': memory.available / (1024**3)\n",
    "            },\n",
    "            'component_status': {\n",
    "                'document_processor': 'ready' if self.document_processor else 'not_initialized',\n",
    "                'metadata_manager': 'ready' if self.metadata_manager else 'not_initialized',\n",
    "                'vector_manager': 'ready' if self.vector_manager else 'not_initialized',\n",
    "                'quality_assessor': 'ready' if self.quality_assessor else 'not_initialized',\n",
    "                'anomaly_detector': 'ready' if self.anomaly_detector else 'not_initialized'\n",
    "            },\n",
    "            'statistics': self.system_stats.copy(),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # æ›´æ–°å¥åº·æŒ‡æ¨™\n",
    "        if self.config.enable_metrics:\n",
    "            self.metrics['system_health'].set(health_score)\n",
    "        \n",
    "        return health_status\n",
    "    \n",
    "    def _calculate_health_score(self, cpu: float, memory: float, disk: float) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—ç³»çµ±å¥åº·åˆ†æ•¸\n",
    "        \"\"\"\n",
    "        # ç°¡å–®çš„å¥åº·è©•åˆ†ç®—æ³•\n",
    "        cpu_score = max(0, (100 - cpu) / 100)\n",
    "        memory_score = max(0, (100 - memory) / 100) \n",
    "        disk_score = max(0, (100 - disk) / 100)\n",
    "        \n",
    "        # åŠ æ¬Šå¹³å‡\n",
    "        health_score = 0.4 * cpu_score + 0.4 * memory_score + 0.2 * disk_score\n",
    "        \n",
    "        # å¦‚æœç³»çµ±æœªåˆå§‹åŒ–ï¼Œé™ä½å¥åº·åˆ†æ•¸\n",
    "        if not self.is_initialized:\n",
    "            health_score *= 0.5\n",
    "        \n",
    "        return health_score\n",
    "    \n",
    "    async def shutdown(self):\n",
    "        \"\"\"å„ªé›…é—œé–‰ç³»çµ±\"\"\"\n",
    "        logger.info(\"æ­£åœ¨é—œé–‰ç³»çµ±...\")\n",
    "        \n",
    "        self.is_running = False\n",
    "        \n",
    "        # é—œé–‰åŸ·è¡Œå™¨\n",
    "        self.thread_executor.shutdown(wait=True)\n",
    "        self.process_executor.shutdown(wait=True)\n",
    "        \n",
    "        logger.info(\"ç³»çµ±å·²å®‰å…¨é—œé–‰\")\n",
    "\n",
    "print(\"âœ… ä¼æ¥­ç´šæ–‡æª”æ²»ç†ç³»çµ±å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ RESTful API æœå‹™\n",
    "\n",
    "### FastAPI æ‡‰ç”¨å»ºç«‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ FastAPI æ‡‰ç”¨\n",
    "def create_app(system: EnterpriseDocumentGovernanceSystem) -> FastAPI:\n",
    "    \"\"\"\n",
    "    å»ºç«‹ FastAPI æ‡‰ç”¨\n",
    "    \"\"\"\n",
    "    app = FastAPI(\n",
    "        title=\"ä¼æ¥­ KMS è³‡æ–™æ²»ç†ç³»çµ±\",\n",
    "        description=\"Enterprise Knowledge Management System Data Governance API\",\n",
    "        version=\"1.0.0\",\n",
    "        docs_url=\"/docs\",\n",
    "        redoc_url=\"/redoc\"\n",
    "    )\n",
    "    \n",
    "    # CORS è¨­å®š\n",
    "    if system.config.enable_cors:\n",
    "        app.add_middleware(\n",
    "            CORSMiddleware,\n",
    "            allow_origins=[\"*\"],\n",
    "            allow_credentials=True,\n",
    "            allow_methods=[\"*\"],\n",
    "            allow_headers=[\"*\"],\n",
    "        )\n",
    "    \n",
    "    # å¥åº·æª¢æŸ¥ç«¯é»\n",
    "    @app.get(\"/health\")\n",
    "    async def health_check():\n",
    "        \"\"\"ç³»çµ±å¥åº·æª¢æŸ¥\"\"\"\n",
    "        health_status = await system.get_system_health()\n",
    "        \n",
    "        status_code = 200\n",
    "        if health_status['status'] == 'critical':\n",
    "            status_code = 503\n",
    "        elif health_status['status'] == 'warning':\n",
    "            status_code = 200\n",
    "            \n",
    "        return JSONResponse(\n",
    "            status_code=status_code,\n",
    "            content=health_status\n",
    "        )\n",
    "    \n",
    "    # ç³»çµ±åˆå§‹åŒ–ç«¯é»\n",
    "    @app.post(\"/system/initialize\")\n",
    "    async def initialize_system():\n",
    "        \"\"\"åˆå§‹åŒ–ç³»çµ±\"\"\"\n",
    "        if system.is_initialized:\n",
    "            return {\"status\": \"already_initialized\", \"message\": \"ç³»çµ±å·²ç¶“åˆå§‹åŒ–\"}\n",
    "        \n",
    "        success = await system.initialize_system()\n",
    "        \n",
    "        if success:\n",
    "            return {\"status\": \"success\", \"message\": \"ç³»çµ±åˆå§‹åŒ–æˆåŠŸ\"}\n",
    "        else:\n",
    "            raise HTTPException(status_code=500, detail=\"ç³»çµ±åˆå§‹åŒ–å¤±æ•—\")\n",
    "    \n",
    "    # æ–‡æª”ä¸Šå‚³èˆ‡è™•ç†ç«¯é»\n",
    "    @app.post(\"/documents/upload\")\n",
    "    async def upload_documents(files: List[UploadFile] = File(...)):\n",
    "        \"\"\"ä¸Šå‚³ä¸¦è™•ç†æ–‡æª”\"\"\"\n",
    "        if not system.is_initialized:\n",
    "            raise HTTPException(status_code=503, detail=\"ç³»çµ±å°šæœªåˆå§‹åŒ–\")\n",
    "        \n",
    "        # å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆ\n",
    "        upload_dir = Path(\"./uploads\")\n",
    "        upload_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        saved_files = []\n",
    "        for file in files:\n",
    "            file_path = upload_dir / file.filename\n",
    "            \n",
    "            async with aiofiles.open(file_path, 'wb') as f:\n",
    "                content = await file.read()\n",
    "                await f.write(content)\n",
    "            \n",
    "            saved_files.append(str(file_path))\n",
    "        \n",
    "        # è™•ç†æ–‡æª”\n",
    "        results = await system.process_document_batch(saved_files)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"å·²è™•ç† {len(files)} å€‹æ–‡æª”\",\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    # æ‰¹æ¬¡è™•ç†ç«¯é»\n",
    "    @app.post(\"/documents/batch-process\")\n",
    "    async def batch_process_documents(request: Dict):\n",
    "        \"\"\"æ‰¹æ¬¡è™•ç†æ–‡æª”\"\"\"\n",
    "        if not system.is_initialized:\n",
    "            raise HTTPException(status_code=503, detail=\"ç³»çµ±å°šæœªåˆå§‹åŒ–\")\n",
    "        \n",
    "        file_paths = request.get('file_paths', [])\n",
    "        metadata_list = request.get('metadata_list', [])\n",
    "        \n",
    "        if not file_paths:\n",
    "            raise HTTPException(status_code=400, detail=\"æœªæä¾›æª”æ¡ˆè·¯å¾‘\")\n",
    "        \n",
    "        results = await system.process_document_batch(file_paths, metadata_list)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    # æœç´¢ç«¯é»\n",
    "    @app.get(\"/search\")\n",
    "    async def search_documents(q: str, limit: int = 10, min_score: float = 0.0):\n",
    "        \"\"\"æœç´¢æ–‡æª”\"\"\"\n",
    "        if not system.is_initialized:\n",
    "            raise HTTPException(status_code=503, detail=\"ç³»çµ±å°šæœªåˆå§‹åŒ–\")\n",
    "        \n",
    "        # æ¨¡æ“¬æœç´¢çµæœ\n",
    "        mock_results = [\n",
    "            {\n",
    "                \"document_id\": f\"doc_{i}\",\n",
    "                \"title\": f\"Document {i} related to {q}\",\n",
    "                \"score\": np.random.uniform(min_score, 1.0),\n",
    "                \"snippet\": f\"This document contains information about {q}...\"\n",
    "            }\n",
    "            for i in range(min(limit, 5))\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"query\": q,\n",
    "            \"results\": mock_results,\n",
    "            \"total_count\": len(mock_results)\n",
    "        }\n",
    "    \n",
    "    # æŒ‡æ¨™ç«¯é»\n",
    "    @app.get(\"/metrics\")\n",
    "    async def get_metrics():\n",
    "        \"\"\"Prometheus æŒ‡æ¨™\"\"\"\n",
    "        if not system.config.enable_metrics:\n",
    "            raise HTTPException(status_code=404, detail=\"æŒ‡æ¨™æ”¶é›†æœªå•Ÿç”¨\")\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            iter([generate_latest().decode('utf-8')]),\n",
    "            media_type=\"text/plain\"\n",
    "        )\n",
    "    \n",
    "    # ç³»çµ±çµ±è¨ˆç«¯é»\n",
    "    @app.get(\"/system/stats\")\n",
    "    async def get_system_stats():\n",
    "        \"\"\"ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        health_status = await system.get_system_health()\n",
    "        \n",
    "        return {\n",
    "            \"system_health\": health_status,\n",
    "            \"performance_stats\": {\n",
    "                \"documents_processed_per_minute\": system.system_stats['documents_processed'] / max(1, health_status['uptime_seconds'] / 60),\n",
    "                \"average_processing_time\": \"N/A\",  # éœ€è¦å¯¦éš›è¨ˆç®—\n",
    "                \"success_rate\": \"N/A\"  # éœ€è¦å¯¦éš›è¨ˆç®—\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return app\n",
    "\n",
    "print(\"âœ… FastAPI æ‡‰ç”¨å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª å®Œæ•´ç³»çµ±æ¼”ç¤º\n",
    "\n",
    "### ç³»çµ±å•Ÿå‹•èˆ‡æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_system_demo():\n",
    "    \"\"\"\n",
    "    é‹è¡Œå®Œæ•´ç³»çµ±æ¼”ç¤º\n",
    "    \"\"\"\n",
    "    console.print(\"\\nğŸš€ [bold blue]ä¼æ¥­ç´š KMS è³‡æ–™æ²»ç†ç³»çµ±æ¼”ç¤º[/bold blue]\\n\")\n",
    "    \n",
    "    # 1. ç³»çµ±é…ç½®\n",
    "    config = SystemConfig(\n",
    "        max_workers=4,\n",
    "        min_quality_threshold=0.6,\n",
    "        enable_metrics=True\n",
    "    )\n",
    "    \n",
    "    # 2. åˆå§‹åŒ–ç³»çµ±\n",
    "    console.print(\"ğŸ“‹ [yellow]æ­¥é©Ÿ 1: åˆå§‹åŒ–ä¼æ¥­æ²»ç†ç³»çµ±[/yellow]\")\n",
    "    governance_system = EnterpriseDocumentGovernanceSystem(config)\n",
    "    \n",
    "    init_success = await governance_system.initialize_system()\n",
    "    if not init_success:\n",
    "        console.print(\"âŒ [red]ç³»çµ±åˆå§‹åŒ–å¤±æ•—[/red]\")\n",
    "        return\n",
    "    \n",
    "    # 3. å»ºç«‹æ¸¬è©¦æ–‡æª”\n",
    "    console.print(\"\\nğŸ“‹ [yellow]æ­¥é©Ÿ 2: å»ºç«‹æ¸¬è©¦æ–‡æª”[/yellow]\")\n",
    "    \n",
    "    test_dir = Path(\"./demo_documents\")\n",
    "    test_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    demo_documents = {\n",
    "        \"enterprise_ai_strategy.txt\": \"\"\"\n",
    "Enterprise AI Strategy 2024-2026\n",
    "\n",
    "Executive Summary\n",
    "This document outlines our comprehensive artificial intelligence strategy for the next three years. We aim to leverage AI technologies to enhance operational efficiency, improve customer experience, and drive innovation across all business units.\n",
    "\n",
    "Strategic Objectives\n",
    "1. Implement AI-powered automation in 70% of routine processes by 2025\n",
    "2. Deploy machine learning models for predictive analytics\n",
    "3. Establish enterprise-wide data governance framework\n",
    "4. Build AI talent pool through training and recruitment\n",
    "\n",
    "Implementation Roadmap\n",
    "Phase 1 (2024): Foundation building and pilot projects\n",
    "Phase 2 (2025): Scaling successful initiatives\n",
    "Phase 3 (2026): Full enterprise integration and optimization\n",
    "\n",
    "Risk Mitigation\n",
    "We acknowledge potential risks including data privacy concerns, algorithmic bias, and workforce displacement. Our mitigation strategies include comprehensive governance policies, regular audits, and employee retraining programs.\n",
    "\"\"\",\n",
    "        \"technical_architecture.txt\": \"\"\"\n",
    "Microservices Architecture Design Document\n",
    "\n",
    "Overview\n",
    "This document describes the technical architecture for our new microservices-based platform. The architecture follows domain-driven design principles and implements event-driven communication patterns.\n",
    "\n",
    "Core Services\n",
    "- User Management Service: Handles authentication and authorization\n",
    "- Document Processing Service: Manages file upload and content extraction\n",
    "- Search Service: Provides full-text and semantic search capabilities\n",
    "- Notification Service: Handles all system notifications and alerts\n",
    "\n",
    "Technology Stack\n",
    "- Container Platform: Docker + Kubernetes\n",
    "- API Gateway: Kong\n",
    "- Message Queue: Apache Kafka\n",
    "- Databases: PostgreSQL, Redis, Elasticsearch\n",
    "- Monitoring: Prometheus + Grafana\n",
    "\n",
    "Security Considerations\n",
    "All services implement OAuth 2.0 authentication, API rate limiting, and comprehensive audit logging. Data encryption is enforced both at rest and in transit.\n",
    "\"\"\",\n",
    "        \"quality_metrics_report.txt\": \"\"\"\n",
    "Q3 2024 Quality Metrics Report\n",
    "\n",
    "Document Processing Quality\n",
    "During Q3 2024, our document governance system processed 15,847 documents with an average quality score of 0.847. This represents a 12% improvement over Q2 2024.\n",
    "\n",
    "Key Performance Indicators\n",
    "- Processing Success Rate: 98.2%\n",
    "- Average Processing Time: 2.3 seconds per document\n",
    "- Quality Threshold Compliance: 92%\n",
    "- Anomaly Detection Accuracy: 94.5%\n",
    "\n",
    "Quality Dimensions Analysis\n",
    "Accuracy: 0.89 (excellent)\n",
    "Completeness: 0.85 (good)\n",
    "Consistency: 0.82 (good)\n",
    "Currency: 0.78 (fair)\n",
    "Understandability: 0.91 (excellent)\n",
    "Traceability: 0.84 (good)\n",
    "\n",
    "Recommendations\n",
    "1. Focus on improving document currency through automated update detection\n",
    "2. Enhance consistency checking algorithms for technical documentation\n",
    "3. Implement proactive quality monitoring for real-time feedback\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    file_paths = []\n",
    "    for filename, content in demo_documents.items():\n",
    "        file_path = test_dir / filename\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        file_paths.append(str(file_path))\n",
    "    \n",
    "    console.print(f\"âœ… å»ºç«‹äº† {len(demo_documents)} å€‹æ¸¬è©¦æ–‡æª”\")\n",
    "    \n",
    "    # 4. æ‰¹æ¬¡è™•ç†æ¼”ç¤º\n",
    "    console.print(\"\\nğŸ“‹ [yellow]æ­¥é©Ÿ 3: æ‰¹æ¬¡è™•ç†æ–‡æª”[/yellow]\")\n",
    "    \n",
    "    # æº–å‚™å…ƒè³‡æ–™\n",
    "    metadata_list = [\n",
    "        {\n",
    "            \"title\": \"Enterprise AI Strategy 2024-2026\",\n",
    "            \"authors\": [\"Strategy Team\"],\n",
    "            \"document_type\": \"strategic_plan\",\n",
    "            \"department\": \"Corporate Strategy\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Microservices Architecture Design Document\",\n",
    "            \"authors\": [\"Architecture Team\"],\n",
    "            \"document_type\": \"technical_spec\",\n",
    "            \"department\": \"Engineering\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Q3 2024 Quality Metrics Report\",\n",
    "            \"authors\": [\"Quality Assurance Team\"],\n",
    "            \"document_type\": \"quality_report\",\n",
    "            \"department\": \"Operations\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # åŸ·è¡Œæ‰¹æ¬¡è™•ç†\n",
    "    batch_results = await governance_system.process_document_batch(file_paths, metadata_list)\n",
    "    \n",
    "    # 5. å±•ç¤ºè™•ç†çµæœ\n",
    "    console.print(\"\\nğŸ“‹ [yellow]æ­¥é©Ÿ 4: è™•ç†çµæœåˆ†æ[/yellow]\")\n",
    "    \n",
    "    # å»ºç«‹çµæœè¡¨æ ¼\n",
    "    results_table = Table(\n",
    "        title=\"æ‰¹æ¬¡è™•ç†çµæœ\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold magenta\"\n",
    "    )\n",
    "    \n",
    "    results_table.add_column(\"æ–‡æª”\", style=\"cyan\")\n",
    "    results_table.add_column(\"ç‹€æ…‹\", style=\"green\")\n",
    "    results_table.add_column(\"å“è³ªåˆ†æ•¸\", style=\"yellow\")\n",
    "    results_table.add_column(\"åˆ†å¡Šæ•¸\", style=\"blue\")\n",
    "    results_table.add_column(\"è™•ç†æ™‚é–“\", style=\"magenta\")\n",
    "    \n",
    "    for result in batch_results['processed']:\n",
    "        filename = Path(result['file_path']).name\n",
    "        status_icon = \"âœ…\" if result['status'] == 'success' else \"âŒ\"\n",
    "        quality_score = f\"{result['quality_score']:.3f}\"\n",
    "        chunk_count = str(result['chunk_count'])\n",
    "        processing_time = f\"{result['processing_time']:.2f}s\"\n",
    "        \n",
    "        results_table.add_row(\n",
    "            filename, \n",
    "            f\"{status_icon} {result['status']}\", \n",
    "            quality_score,\n",
    "            chunk_count,\n",
    "            processing_time\n",
    "        )\n",
    "    \n",
    "    console.print(results_table)\n",
    "    \n",
    "    # 6. ç³»çµ±å¥åº·ç‹€æ…‹\n",
    "    console.print(\"\\nğŸ“‹ [yellow]æ­¥é©Ÿ 5: ç³»çµ±å¥åº·æª¢æŸ¥[/yellow]\")\n",
    "    \n",
    "    health_status = await governance_system.get_system_health()\n",
    "    \n",
    "    health_table = Table(\n",
    "        title=\"ç³»çµ±å¥åº·ç‹€æ…‹\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold blue\"\n",
    "    )\n",
    "    \n",
    "    health_table.add_column(\"é …ç›®\", style=\"cyan\")\n",
    "    health_table.add_column(\"ç‹€æ…‹/æ•¸å€¼\", style=\"green\")\n",
    "    \n",
    "    health_table.add_row(\"æ•´é«”ç‹€æ…‹\", health_status['status'].upper())\n",
    "    health_table.add_row(\"å¥åº·åˆ†æ•¸\", f\"{health_status['health_score']:.3f}\")\n",
    "    health_table.add_row(\"é‹è¡Œæ™‚é–“\", f\"{health_status['uptime_seconds']:.0f} ç§’\")\n",
    "    health_table.add_row(\"CPU ä½¿ç”¨ç‡\", f\"{health_status['system_resources']['cpu_usage_percent']:.1f}%\")\n",
    "    health_table.add_row(\"è¨˜æ†¶é«”ä½¿ç”¨ç‡\", f\"{health_status['system_resources']['memory_usage_percent']:.1f}%\")\n",
    "    health_table.add_row(\"å·²è™•ç†æ–‡æª”\", str(health_status['statistics']['documents_processed']))\n",
    "    \n",
    "    console.print(health_table)\n",
    "    \n",
    "    # 7. æ‘˜è¦çµ±è¨ˆ\n",
    "    console.print(\"\\nğŸ“Š [bold green]æ¼”ç¤ºæ‘˜è¦[/bold green]\")\n",
    "    summary = batch_results['summary']\n",
    "    \n",
    "    console.print(f\"â€¢ ç¸½è™•ç†æ–‡æª”æ•¸: {summary['total']}\")\n",
    "    console.print(f\"â€¢ æˆåŠŸè™•ç†: {summary['success_count']}\")\n",
    "    console.print(f\"â€¢ è™•ç†å¤±æ•—: {summary['failure_count']}\")\n",
    "    console.print(f\"â€¢ å¹³å‡å“è³ªåˆ†æ•¸: {summary['avg_quality_score']:.3f}\")\n",
    "    console.print(f\"â€¢ ç¸½è™•ç†æ™‚é–“: {summary['processing_time']:.2f} ç§’\")\n",
    "    console.print(f\"â€¢ å¹³å‡è™•ç†é€Ÿåº¦: {summary['total']/summary['processing_time']:.1f} æ–‡æª”/ç§’\")\n",
    "    \n",
    "    if batch_results['quality_issues']:\n",
    "        console.print(f\"â€¢ âš ï¸  å“è³ªå•é¡Œæ–‡æª”: {len(batch_results['quality_issues'])} å€‹\")\n",
    "    \n",
    "    console.print(\"\\nğŸ‰ [bold green]ä¼æ¥­ç´š KMS è³‡æ–™æ²»ç†ç³»çµ±æ¼”ç¤ºå®Œæˆï¼[/bold green]\")\n",
    "    \n",
    "    # 8. é—œé–‰ç³»çµ±\n",
    "    await governance_system.shutdown()\n",
    "\n",
    "# é‹è¡Œæ¼”ç¤º\n",
    "print(\"ğŸš€ æº–å‚™é‹è¡Œå®Œæ•´ç³»çµ±æ¼”ç¤º...\")\n",
    "await run_system_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API æœå‹™å•Ÿå‹•æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_api_service():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤º API æœå‹™åŠŸèƒ½\n",
    "    \"\"\"\n",
    "    console.print(\"\\nğŸŒ [bold blue]API æœå‹™æ¼”ç¤º[/bold blue]\\n\")\n",
    "    \n",
    "    # åˆå§‹åŒ–ç³»çµ±\n",
    "    config = SystemConfig(api_port=8001)  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…è¡çª\n",
    "    governance_system = EnterpriseDocumentGovernanceSystem(config)\n",
    "    await governance_system.initialize_system()\n",
    "    \n",
    "    # å»ºç«‹ FastAPI æ‡‰ç”¨\n",
    "    app = create_app(governance_system)\n",
    "    \n",
    "    console.print(\"ğŸ“‹ API ç«¯é»æ¸…å–®:\")\n",
    "    \n",
    "    api_endpoints = [\n",
    "        (\"GET\", \"/health\", \"ç³»çµ±å¥åº·æª¢æŸ¥\"),\n",
    "        (\"POST\", \"/system/initialize\", \"åˆå§‹åŒ–ç³»çµ±\"),\n",
    "        (\"POST\", \"/documents/upload\", \"ä¸Šå‚³æ–‡æª”\"),\n",
    "        (\"POST\", \"/documents/batch-process\", \"æ‰¹æ¬¡è™•ç†\"),\n",
    "        (\"GET\", \"/search\", \"æœç´¢æ–‡æª”\"),\n",
    "        (\"GET\", \"/metrics\", \"Prometheus æŒ‡æ¨™\"),\n",
    "        (\"GET\", \"/system/stats\", \"ç³»çµ±çµ±è¨ˆ\")\n",
    "    ]\n",
    "    \n",
    "    endpoints_table = Table(\n",
    "        title=\"API ç«¯é»\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold cyan\"\n",
    "    )\n",
    "    \n",
    "    endpoints_table.add_column(\"æ–¹æ³•\", style=\"green\")\n",
    "    endpoints_table.add_column(\"è·¯å¾‘\", style=\"blue\")\n",
    "    endpoints_table.add_column(\"èªªæ˜\", style=\"yellow\")\n",
    "    \n",
    "    for method, path, description in api_endpoints:\n",
    "        endpoints_table.add_row(method, path, description)\n",
    "    \n",
    "    console.print(endpoints_table)\n",
    "    \n",
    "    # æ¨¡æ“¬ API æ¸¬è©¦\n",
    "    console.print(\"\\nğŸ“‹ [yellow]æ¨¡æ“¬ API æ¸¬è©¦[/yellow]\")\n",
    "    \n",
    "    # æ¨¡æ“¬å¥åº·æª¢æŸ¥\n",
    "    health_response = await governance_system.get_system_health()\n",
    "    console.print(f\"âœ… GET /health - ç‹€æ…‹: {health_response['status']}\")\n",
    "    \n",
    "    # æ¨¡æ“¬æ‰¹æ¬¡è™•ç†è«‹æ±‚\n",
    "    mock_request = {\n",
    "        \"file_paths\": [\"./demo_documents/enterprise_ai_strategy.txt\"],\n",
    "        \"metadata_list\": [{\n",
    "            \"title\": \"Enterprise AI Strategy\",\n",
    "            \"department\": \"Strategy\"\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    batch_response = await governance_system.process_document_batch(\n",
    "        mock_request['file_paths'],\n",
    "        mock_request['metadata_list']\n",
    "    )\n",
    "    \n",
    "    console.print(f\"âœ… POST /documents/batch-process - è™•ç†äº† {batch_response['summary']['success_count']} å€‹æ–‡æª”\")\n",
    "    \n",
    "    # é¡¯ç¤º API é…ç½®\n",
    "    console.print(\"\\nğŸ“‹ API æœå‹™é…ç½®:\")\n",
    "    console.print(f\"â€¢ ä¸»æ©Ÿ: {config.api_host}\")\n",
    "    console.print(f\"â€¢ ç«¯å£: {config.api_port}\")\n",
    "    console.print(f\"â€¢ CORS: {'å•Ÿç”¨' if config.enable_cors else 'ç¦ç”¨'}\")\n",
    "    console.print(f\"â€¢ æŒ‡æ¨™æ”¶é›†: {'å•Ÿç”¨' if config.enable_metrics else 'ç¦ç”¨'}\")\n",
    "    console.print(f\"â€¢ æ–‡æª”: http://{config.api_host}:{config.api_port}/docs\")\n",
    "    \n",
    "    console.print(\"\\nğŸ’¡ [bold yellow]å•Ÿå‹• API æœå‹™å‘½ä»¤:[/bold yellow]\")\n",
    "    console.print(f\"uvicorn main:app --host {config.api_host} --port {config.api_port} --reload\")\n",
    "    \n",
    "    await governance_system.shutdown()\n",
    "\n",
    "# é‹è¡Œ API æ¼”ç¤º\n",
    "await demo_api_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ€§èƒ½åˆ†æèˆ‡ç›£æ§\n",
    "\n",
    "### ç³»çµ±æ€§èƒ½è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def performance_benchmark():\n",
    "    \"\"\"\n",
    "    ç³»çµ±æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "    \"\"\"\n",
    "    console.print(\"\\nâš¡ [bold blue]ç³»çµ±æ€§èƒ½åŸºæº–æ¸¬è©¦[/bold blue]\\n\")\n",
    "    \n",
    "    # å»ºç«‹æ¸¬è©¦é…ç½®\n",
    "    test_configs = [\n",
    "        SystemConfig(max_workers=1, chunk_size=256),\n",
    "        SystemConfig(max_workers=2, chunk_size=512),\n",
    "        SystemConfig(max_workers=4, chunk_size=512),\n",
    "        SystemConfig(max_workers=8, chunk_size=1024)\n",
    "    ]\n",
    "    \n",
    "    # å»ºç«‹æ¸¬è©¦æ–‡æª”\n",
    "    test_docs = [f\"test_doc_{i}.txt\" for i in range(20)]\n",
    "    test_dir = Path(\"./benchmark_docs\")\n",
    "    test_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ç”Ÿæˆæ¸¬è©¦å…§å®¹\n",
    "    for doc_name in test_docs:\n",
    "        content = f\"\"\"\n",
    "Performance Test Document {doc_name}\n",
    "\n",
    "This is a test document for performance benchmarking. It contains multiple paragraphs to simulate real-world document processing scenarios.\n",
    "\n",
    "Introduction\n",
    "Performance testing is crucial for ensuring system reliability and scalability. This document serves as a representative sample for measuring processing throughput and latency.\n",
    "\n",
    "Content Section 1\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris.\n",
    "\n",
    "Content Section 2\n",
    "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "\n",
    "Conclusion\n",
    "This document demonstrates the system's ability to process structured content with multiple sections and varying complexity levels.\n",
    "\"\"\"\n",
    "        \n",
    "        file_path = test_dir / doc_name\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    test_file_paths = [str(test_dir / doc) for doc in test_docs]\n",
    "    \n",
    "    # æ€§èƒ½æ¸¬è©¦çµæœ\n",
    "    performance_results = []\n",
    "    \n",
    "    for i, config in enumerate(test_configs):\n",
    "        console.print(f\"ğŸ“Š æ¸¬è©¦é…ç½® {i+1}: {config.max_workers} workers, {config.chunk_size} chunk size\")\n",
    "        \n",
    "        # åˆå§‹åŒ–ç³»çµ±\n",
    "        governance_system = EnterpriseDocumentGovernanceSystem(config)\n",
    "        await governance_system.initialize_system()\n",
    "        \n",
    "        # åŸ·è¡Œæ€§èƒ½æ¸¬è©¦\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        results = await governance_system.process_document_batch(test_file_paths)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        total_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # æ”¶é›†æ€§èƒ½æŒ‡æ¨™\n",
    "        performance_data = {\n",
    "            'config': f\"{config.max_workers}w_{config.chunk_size}c\",\n",
    "            'max_workers': config.max_workers,\n",
    "            'chunk_size': config.chunk_size,\n",
    "            'total_documents': len(test_file_paths),\n",
    "            'total_time': total_time,\n",
    "            'throughput': len(test_file_paths) / total_time,\n",
    "            'avg_processing_time': total_time / len(test_file_paths),\n",
    "            'success_count': results['summary']['success_count'],\n",
    "            'avg_quality_score': results['summary']['avg_quality_score']\n",
    "        }\n",
    "        \n",
    "        performance_results.append(performance_data)\n",
    "        \n",
    "        console.print(f\"  â±ï¸  ç¸½æ™‚é–“: {total_time:.2f}s\")\n",
    "        console.print(f\"  ğŸ“ˆ ååé‡: {performance_data['throughput']:.1f} æ–‡æª”/ç§’\")\n",
    "        console.print(f\"  âœ… æˆåŠŸç‡: {performance_data['success_count']}/{performance_data['total_documents']}\")\n",
    "        console.print()\n",
    "        \n",
    "        await governance_system.shutdown()\n",
    "    \n",
    "    # æ€§èƒ½çµæœè¡¨æ ¼\n",
    "    perf_table = Table(\n",
    "        title=\"æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœ\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold green\"\n",
    "    )\n",
    "    \n",
    "    perf_table.add_column(\"é…ç½®\", style=\"cyan\")\n",
    "    perf_table.add_column(\"Workeræ•¸\", style=\"blue\")\n",
    "    perf_table.add_column(\"åˆ†å¡Šå¤§å°\", style=\"yellow\")\n",
    "    perf_table.add_column(\"ç¸½æ™‚é–“(s)\", style=\"magenta\")\n",
    "    perf_table.add_column(\"ååé‡(doc/s)\", style=\"green\")\n",
    "    perf_table.add_column(\"å¹³å‡å“è³ª\", style=\"red\")\n",
    "    \n",
    "    for result in performance_results:\n",
    "        perf_table.add_row(\n",
    "            result['config'],\n",
    "            str(result['max_workers']),\n",
    "            str(result['chunk_size']),\n",
    "            f\"{result['total_time']:.2f}\",\n",
    "            f\"{result['throughput']:.1f}\",\n",
    "            f\"{result['avg_quality_score']:.3f}\"\n",
    "        )\n",
    "    \n",
    "    console.print(perf_table)\n",
    "    \n",
    "    # æ‰¾å‡ºæœ€ä½³é…ç½®\n",
    "    best_config = max(performance_results, key=lambda x: x['throughput'])\n",
    "    console.print(f\"\\nğŸ† [bold green]æœ€ä½³æ€§èƒ½é…ç½®:[/bold green] {best_config['config']}\")\n",
    "    console.print(f\"   ååé‡: {best_config['throughput']:.1f} æ–‡æª”/ç§’\")\n",
    "    console.print(f\"   å¹³å‡è™•ç†æ™‚é–“: {best_config['avg_processing_time']:.3f} ç§’/æ–‡æª”\")\n",
    "    \n",
    "    # æ€§èƒ½å»ºè­°\n",
    "    console.print(\"\\nğŸ’¡ [bold yellow]æ€§èƒ½å„ªåŒ–å»ºè­°:[/bold yellow]\")\n",
    "    \n",
    "    if best_config['max_workers'] >= 4:\n",
    "        console.print(\"â€¢ ç³»çµ±é©åˆé«˜ä¸¦ç™¼è™•ç†ï¼Œå»ºè­°åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨å¤š worker é…ç½®\")\n",
    "    else:\n",
    "        console.print(\"â€¢ ç³»çµ±åœ¨ä½ä¸¦ç™¼ä¸‹è¡¨ç¾æ›´ä½³ï¼Œå¯èƒ½å­˜åœ¨ I/O ç“¶é ¸\")\n",
    "    \n",
    "    avg_throughput = np.mean([r['throughput'] for r in performance_results])\n",
    "    if avg_throughput > 5:\n",
    "        console.print(\"â€¢ ç³»çµ±æ•´é«”æ€§èƒ½è‰¯å¥½ï¼Œé©åˆç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²\")\n",
    "    else:\n",
    "        console.print(\"â€¢ å»ºè­°é€²ä¸€æ­¥å„ªåŒ–ç®—æ³•æˆ–å‡ç´šç¡¬ä»¶é…ç½®\")\n",
    "    \n",
    "    console.print(\"\\nğŸ‰ æ€§èƒ½åŸºæº–æ¸¬è©¦å®Œæˆï¼\")\n",
    "\n",
    "# é‹è¡Œæ€§èƒ½æ¸¬è©¦\n",
    "await performance_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ èª²ç¨‹ç¸½çµèˆ‡å¾ŒçºŒå­¸ç¿’\n",
    "\n",
    "### å®Œæ•´å­¸ç¿’æˆæœæª¢æ ¸\n",
    "\n",
    "ğŸ‰ æ­å–œï¼æ‚¨å·²å®Œæˆä¼æ¥­ç´š KMS è³‡æ–™æ²»ç†ç³»çµ±çš„å®Œæ•´å­¸ç¿’æ—…ç¨‹ã€‚\n",
    "\n",
    "#### âœ… å››å¤§æ ¸å¿ƒæ¨¡çµ„æŒæ¡\n",
    "\n",
    "**æ¨¡çµ„ 1: æ–‡æª”æ”å–èˆ‡è™•ç†**\n",
    "- ä¼æ¥­ç´šæ–‡æª”è™•ç†æ¶æ§‹è¨­è¨ˆ\n",
    "- Docling æ·±åº¦æ•´åˆèˆ‡å¤šæ ¼å¼æ”¯æ´\n",
    "- èªç¾©åˆ†å¡Šæ¼”ç®—æ³•å¯¦ä½œ\n",
    "- è‡ªå‹•åŒ–å“è³ªæª¢æ¸¬æ©Ÿåˆ¶\n",
    "\n",
    "**æ¨¡çµ„ 2: å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•**\n",
    "- å¯æ“´å±•çš„å…ƒè³‡æ–™ schema è¨­è¨ˆ\n",
    "- ä¼æ¥­ç´šå‘é‡ç´¢å¼•ç³»çµ±\n",
    "- æ··åˆæª¢ç´¢ç­–ç•¥å¯¦ä½œ\n",
    "- è³‡æ–™è¡€ç·£è¿½è¹¤æ©Ÿåˆ¶\n",
    "\n",
    "**æ¨¡çµ„ 3: å“è³ªæ§åˆ¶èˆ‡ç›£æ§**\n",
    "- å¤šç¶­åº¦å“è³ªè©•ä¼°æ¡†æ¶ (ISO 25012)\n",
    "- æ™ºèƒ½ç•°å¸¸æª¢æ¸¬ç³»çµ±\n",
    "- è‡ªå‹•åŒ–å“è³ªç›£æ§\n",
    "- å“è³ªæ²»ç†æœ€ä½³å¯¦è¸\n",
    "\n",
    "**æ¨¡çµ„ 4: ç«¯åˆ°ç«¯ç³»çµ±æ•´åˆ**\n",
    "- å®Œæ•´ç³»çµ±æ¶æ§‹æ•´åˆ\n",
    "- RESTful API æœå‹™åŒ–\n",
    "- æ€§èƒ½å„ªåŒ–èˆ‡ç›£æ§\n",
    "- ç”Ÿç”¢ç´šéƒ¨ç½²æº–å‚™\n",
    "\n",
    "### ğŸš€ å¯¦éš›æ‡‰ç”¨èƒ½åŠ›\n",
    "\n",
    "å®Œæˆæœ¬èª²ç¨‹å¾Œï¼Œæ‚¨å…·å‚™äº†ï¼š\n",
    "\n",
    "1. **ç³»çµ±è¨­è¨ˆèƒ½åŠ›** - é‹ç”¨é‹ç®—æ€ç¶­è¨­è¨ˆå¯æ“´å±•çš„æ²»ç†æ¶æ§‹\n",
    "2. **æŠ€è¡“å¯¦ä½œèƒ½åŠ›** - æŒæ¡å¾æ–‡æª”è™•ç†åˆ°å‘é‡æª¢ç´¢çš„å®Œæ•´æŠ€è¡“æ£§\n",
    "3. **å“è³ªç®¡ç†èƒ½åŠ›** - å»ºç«‹ä¼æ¥­ç´šçš„è³‡æ–™å“è³ªä¿è­‰é«”ç³»\n",
    "4. **é‹ç¶­ç›£æ§èƒ½åŠ›** - å¯¦ç¾ç³»çµ±å¥åº·ç›£æ§èˆ‡æ€§èƒ½å„ªåŒ–\n",
    "\n",
    "### ğŸ’¼ è·æ¥­æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "- **ä¼æ¥­æ•¸æ“šæ²»ç†å°ˆå®¶** - å»ºç«‹çµ„ç¹”ç´šè³‡æ–™å“è³ªç®¡ç†åˆ¶åº¦\n",
    "- **RAG ç³»çµ±æ¶æ§‹å¸«** - è¨­è¨ˆé«˜å“è³ªçŸ¥è­˜åº«æ”¯æ’çš„ AI æ‡‰ç”¨\n",
    "- **æ–‡æª”ç®¡ç†ç³»çµ±é–‹ç™¼è€…** - é–‹ç™¼æ™ºèƒ½åŒ–æ–‡æª”è™•ç†å¹³å°\n",
    "- **AI å·¥ç¨‹å¸«** - ç‚º LLM æ‡‰ç”¨æä¾›å¯ä¿¡è³´çš„çŸ¥è­˜åŸºç¤\n",
    "\n",
    "### ğŸ”® ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°\n",
    "\n",
    "#### æ·±åº¦å°ˆæ¥­åŒ–æ–¹å‘\n",
    "\n",
    "1. **RAG ç³»çµ±å¯¦ä½œ** \n",
    "   - å­¸ç¿’ LangChainã€LlamaIndex æ¡†æ¶\n",
    "   - å¯¦ä½œæª¢ç´¢å¢å¼·ç”Ÿæˆæ‡‰ç”¨\n",
    "   - æŒæ¡ Prompt Engineering æŠ€è¡“\n",
    "\n",
    "2. **å¤§è¦æ¨¡éƒ¨ç½²**\n",
    "   - Kubernetes å®¹å™¨ç·¨æ’\n",
    "   - å¾®æœå‹™æ¶æ§‹é€²éš\n",
    "   - é›²åŸç”ŸæŠ€è¡“æ£§\n",
    "\n",
    "3. **AI/ML é€²éš**\n",
    "   - è‡ªç„¶èªè¨€è™•ç†æ·±åº¦å­¸ç¿’\n",
    "   - å‘é‡æ•¸æ“šåº«å„ªåŒ–\n",
    "   - MLOps å·¥ç¨‹å¯¦è¸\n",
    "\n",
    "#### æŠ€è¡“æ“´å±•é ˜åŸŸ\n",
    "\n",
    "- **å¤šæ¨¡æ…‹è™•ç†**: åœ–åƒã€éŸ³é »ã€è¦–é »å…§å®¹æ²»ç†\n",
    "- **å¯¦æ™‚æµè™•ç†**: Kafkaã€Flink æµå¼è³‡æ–™è™•ç†\n",
    "- **å®‰å…¨èˆ‡éš±ç§**: å·®åˆ†éš±ç§ã€è¯é‚¦å­¸ç¿’\n",
    "- **å¯è§£é‡‹ AI**: æ¨¡å‹è§£é‡‹æ€§èˆ‡é€æ˜åº¦\n",
    "\n",
    "### ğŸ“š æ¨è–¦è³‡æº\n",
    "\n",
    "#### æŠ€è¡“æ–‡æª”\n",
    "- [Docling å®˜æ–¹æ–‡æª”](https://github.com/DS4SD/docling)\n",
    "- [ChromaDB å‘é‡æ•¸æ“šåº«](https://www.trychroma.com/)\n",
    "- [FastAPI æ¡†æ¶](https://fastapi.tiangolo.com/)\n",
    "\n",
    "#### æ¨™æº–è¦ç¯„\n",
    "- ISO/IEC 25012:2008 - è³‡æ–™å“è³ªæ¨™æº–\n",
    "- DAMA-DMBOK - è³‡æ–™ç®¡ç†çŸ¥è­˜é«”ç³»\n",
    "- OpenAPI 3.0 è¦ç¯„\n",
    "\n",
    "#### é–‹æºå°ˆæ¡ˆ\n",
    "- [LangChain](https://github.com/langchain-ai/langchain) - LLM æ‡‰ç”¨æ¡†æ¶\n",
    "- [Weaviate](https://github.com/weaviate/weaviate) - å‘é‡æ•¸æ“šåº«\n",
    "- [Haystack](https://github.com/deepset-ai/haystack) - NLP ç®¡ç·š\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ™ çµèª\n",
    "\n",
    "æ„Ÿè¬æ‚¨å®Œæˆé€™å€‹æ·±å…¥çš„å­¸ç¿’æ—…ç¨‹ï¼æ‚¨ç¾åœ¨æ“æœ‰äº†å»ºç«‹ä¼æ¥­ç´šè³‡æ–™æ²»ç†ç³»çµ±çš„å®Œæ•´æŠ€èƒ½å’ŒçŸ¥è­˜ã€‚\n",
    "\n",
    "è¨˜ä½ï¼Œ**å“è³ªæ˜¯ AI ç³»çµ±æˆåŠŸçš„åŸºçŸ³**ã€‚åœ¨é€™å€‹è³‡è¨Šçˆ†ç‚¸çš„æ™‚ä»£ï¼Œèƒ½å¤ å¾æµ·é‡è³‡æ–™ä¸­æå–ã€ç®¡ç†å’Œåˆ©ç”¨é«˜å“è³ªçŸ¥è­˜çš„èƒ½åŠ›ï¼Œå°‡æˆç‚ºçµ„ç¹”ç«¶çˆ­åŠ›çš„é—œéµã€‚\n",
    "\n",
    "å¸Œæœ›æ‚¨èƒ½å°‡é€™äº›çŸ¥è­˜æ‡‰ç”¨åˆ°å¯¦éš›å°ˆæ¡ˆä¸­ï¼Œç‚ºçµ„ç¹”å‰µé€ åƒ¹å€¼ï¼Œç‚º AI çš„å¥åº·ç™¼å±•è²¢ç»åŠ›é‡ï¼\n",
    "\n",
    "ğŸš€ **ç¹¼çºŒå­¸ç¿’ï¼ŒæŒçºŒå‰µæ–°ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}