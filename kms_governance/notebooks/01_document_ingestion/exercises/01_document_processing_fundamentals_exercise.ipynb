{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç·´ç¿’ 1: åŸºç¤æ–‡æª”è™•ç†å¯¦ä½œ\n",
    "## Exercise 1: Basic Document Processing Implementation\n",
    "\n",
    "> **ç›®æ¨™**: å¯¦ä½œä¸€å€‹ç°¡åŒ–çš„æ–‡æª”è™•ç†å™¨ï¼ŒæŒæ¡æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ç·´ç¿’ç›®æ¨™\n",
    "\n",
    "1. **å¯¦ä½œå…ƒè³‡æ–™æå–å™¨** - å¾æ–‡æª”è·¯å¾‘å’Œå…§å®¹ä¸­æå–åŸºæœ¬è³‡è¨Š\n",
    "2. **å¯¦ä½œç°¡å–®åˆ†å¡Šå™¨** - åŸºæ–¼æ®µè½æˆ–å¥å­é€²è¡Œæ–‡æª”åˆ†å¡Š\n",
    "3. **å¯¦ä½œå“è³ªè©•ä¼°å™¨** - è¨ˆç®—åŸºæœ¬çš„å“è³ªæŒ‡æ¨™\n",
    "4. **æ•´åˆè™•ç†æµç¨‹** - å»ºç«‹ç«¯åˆ°ç«¯çš„è™•ç†ç®¡ç·š\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ä»»å‹™æ¸…å–®\n",
    "\n",
    "- [ ] ä»»å‹™ 1: å¯¦ä½œ `SimpleMetadataExtractor`\n",
    "- [ ] ä»»å‹™ 2: å¯¦ä½œ `BasicChunker`\n",
    "- [ ] ä»»å‹™ 3: å¯¦ä½œ `QualityAssessor`\n",
    "- [ ] ä»»å‹™ 4: æ•´åˆç‚º `SimpleDocumentProcessor`\n",
    "- [ ] ä»»å‹™ 5: æ¸¬è©¦èˆ‡é©—è­‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# è¨­å®šå°ˆæ¡ˆè·¯å¾‘\n",
    "PROJECT_ROOT = Path('/home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance')\n",
    "EXERCISE_DIR = PROJECT_ROOT / 'notebooks' / '01_document_ingestion' / 'exercises'\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ è³‡æ–™çµæ§‹å®šç¾©\n",
    "\n",
    "### ä»»å‹™å‰ç½®: å®šç¾©ç°¡åŒ–çš„è³‡æ–™çµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass DocumentMetadata:\n    \"\"\"æ¨™æº–åŒ–æ–‡æª”å…ƒè³‡æ–™çµæ§‹\"\"\"\n    document_id: str\n    title: str\n    authors: List[str]\n    created_date: Optional[datetime]\n    modified_date: Optional[datetime]\n    document_type: str\n    file_path: str\n    file_size: int\n    page_count: int\n    language: str\n    keywords: List[str]\n    category: str\n    quality_score: Optional[float] = None\n    processing_status: str = 'pending'\n\n@dataclass\nclass ProcessedChunk:\n    \"\"\"æ–‡æª”åˆ†å¡Šå¾Œçš„èªç¾©å–®å…ƒ\"\"\"\n    chunk_id: str\n    document_id: str\n    content: str\n    chunk_index: int\n    start_page: int\n    end_page: int\n    word_count: int\n    embedding: Optional[List[float]] = None\n    semantic_type: str = 'text'\n\n@dataclass  \nclass QualityMetrics:\n    \"\"\"æ–‡æª”å“è³ªè©•ä¼°æŒ‡æ¨™\"\"\"\n    accuracy_score: float\n    completeness_score: float\n    readability_score: float\n    structure_score: float\n    overall_score: float\n\nprint(\"âœ… è³‡æ–™çµæ§‹å®šç¾©å®Œæˆï¼ˆå°é½Šä¸»è¦è§£ç­”æ–‡ä»¶ï¼‰\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ ä»»å‹™ 1: å¯¦ä½œ SimpleMetadataExtractor\n",
    "\n",
    "**è¦æ±‚**:\n",
    "1. å¾æª”æ¡ˆè·¯å¾‘æå–åŸºæœ¬è³‡è¨Šï¼ˆæª”åã€å¤§å°ã€å»ºç«‹æ™‚é–“ï¼‰\n",
    "2. å¾å…§å®¹æ¨æ–·æ–‡æª”é¡å‹ï¼ˆpaper, report, documentï¼‰\n",
    "3. è¨ˆç®—å­—æ•¸çµ±è¨ˆ\n",
    "4. ç”Ÿæˆå”¯ä¸€çš„æ–‡æª” ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class EnterpriseDocumentProcessor:\n    \"\"\"ä¼æ¥­ç´šæ–‡æª”è™•ç†å™¨ - ç·´ç¿’ç‰ˆæœ¬\"\"\"\n    \n    def __init__(self, config: Dict = None):\n        \"\"\"åˆå§‹åŒ–è™•ç†å™¨\"\"\"\n        self.config = config or self._default_config()\n        \n        # TODO: åˆå§‹åŒ–å„ç¨®çµ„ä»¶\n        # æç¤º: åƒè€ƒä¸»è¦è§£ç­”æ–‡ä»¶ä¸­çš„åˆå§‹åŒ–æ–¹å¼\n        pass\n    \n    def _default_config(self) -> Dict:\n        \"\"\"é»˜èªé…ç½®\"\"\"\n        # TODO: å¯¦ä½œé»˜èªé…ç½®\n        # æç¤º: æ‡‰åŒ…å« chunk_size, chunk_overlap, similarity_threshold ç­‰\n        pass\n    \n    def extract_metadata(self, file_path: str) -> DocumentMetadata:\n        \"\"\"\n        æå–æ–‡æª”åŸºæœ¬å…ƒè³‡æ–™\n        \n        Args:\n            file_path: æª”æ¡ˆè·¯å¾‘\n            \n        Returns:\n            DocumentMetadata: æå–çš„å…ƒè³‡æ–™\n        \"\"\"\n        # TODO: å¯¦ä½œå…ƒè³‡æ–™æå–\n        # æç¤º:\n        # 1. ä½¿ç”¨ Path(file_path).stat() ç²å–æª”æ¡ˆè³‡è¨Š\n        # 2. ç”Ÿæˆå”¯ä¸€çš„ document_id\n        # 3. æå–æ¨™é¡Œã€åˆ†é¡æ–‡æª”é¡å‹\n        # 4. è¨ˆç®—é æ•¸å’Œå­—æ•¸\n        pass\n    \n    def _extract_basic_info(self, file_path: Path) -> Tuple[str, int]:\n        \"\"\"æå–åŸºæœ¬æª”æ¡ˆè³‡è¨Š\"\"\"\n        # TODO: å¯¦ä½œæ¨™é¡Œå’Œé æ•¸æå–\n        pass\n    \n    def _classify_document_type(self, file_path: Path) -> str:\n        \"\"\"æ ¹æ“šè·¯å¾‘å’Œæª”ååˆ†é¡æ–‡æª”é¡å‹\"\"\"\n        # TODO: å¯¦ä½œæ–‡æª”é¡å‹åˆ†é¡\n        # æç¤º: 'paper', 'report', 'document'\n        pass\n    \n    def _extract_category_from_path(self, file_path: Path) -> str:\n        \"\"\"å¾æª”æ¡ˆè·¯å¾‘æå–åˆ†é¡\"\"\"\n        # TODO: å¯¦ä½œåˆ†é¡æå–\n        pass\n\nprint(\"ğŸ“ è«‹åœ¨ä¸Šæ–¹å¯¦ä½œ EnterpriseDocumentProcessor çš„åŸºæœ¬æ–¹æ³•\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä»»å‹™ 1 æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ SimpleMetadataExtractor\n",
    "test_content = \"\"\"\n",
    "Deep Learning: A Comprehensive Overview\n",
    "\n",
    "Abstract\n",
    "This paper presents a comprehensive overview of deep learning techniques and their applications in various domains.\n",
    "\n",
    "1. Introduction\n",
    "Deep learning has revolutionized the field of artificial intelligence...\n",
    "\n",
    "2. Methodology\n",
    "We propose a novel architecture that combines...\n",
    "\n",
    "3. Conclusion\n",
    "Our experiments demonstrate significant improvements...\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦æª”æ¡ˆ\n",
    "test_file = EXERCISE_DIR / 'test_paper.txt'\n",
    "test_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(test_file, 'w') as f:\n",
    "    f.write(test_content)\n",
    "\n",
    "# TODO: å–æ¶ˆè¨»è§£ä¸¦æ¸¬è©¦æ‚¨çš„å¯¦ä½œ\n",
    "# extractor = SimpleMetadataExtractor()\n",
    "# metadata = extractor.extract_from_file(str(test_file), test_content)\n",
    "# print(f\"æ–‡æª” ID: {metadata.doc_id}\")\n",
    "# print(f\"æª”æ¡ˆå: {metadata.filename}\")\n",
    "# print(f\"æ–‡æª”é¡å‹: {metadata.doc_type}\")\n",
    "# print(f\"å­—æ•¸: {metadata.word_count}\")\n",
    "\n",
    "print(\"è«‹å…ˆå®Œæˆ SimpleMetadataExtractor çš„å¯¦ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ ä»»å‹™ 2: å¯¦ä½œ BasicChunker\n",
    "\n",
    "**è¦æ±‚**:\n",
    "1. æ”¯æ´æŒ‰æ®µè½åˆ†å¡Šï¼ˆä»¥é›™æ›è¡Œåˆ†éš”ï¼‰\n",
    "2. æ”¯æ´æŒ‰å¥å­åˆ†å¡Šï¼ˆä»¥å¥è™Ÿåˆ†éš”ï¼‰\n",
    "3. æ§åˆ¶åˆ†å¡Šå¤§å°ï¼ˆæœ€å°/æœ€å¤§å­—æ•¸ï¼‰\n",
    "4. ç”Ÿæˆå”¯ä¸€çš„åˆ†å¡Š ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class EnterpriseDocumentProcessor:\n    \"\"\"ä¼æ¥­ç´šæ–‡æª”è™•ç†å™¨ - èªç¾©åˆ†å¡Šæ–¹æ³•\"\"\"\n    \n    def _semantic_chunking(self, text: str, document_id: str) -> List[ProcessedChunk]:\n        \"\"\"\n        èªç¾©åˆ†å¡Šæ¼”ç®—æ³•\n        åŸºæ–¼å¥å­ç›¸ä¼¼åº¦çš„å‹•æ…‹åˆ†å¡Šç­–ç•¥\n        \n        Args:\n            text: æ–‡æª”å…§å®¹\n            document_id: æ–‡æª”ID\n            \n        Returns:\n            List[ProcessedChunk]: åˆ†å¡Šåˆ—è¡¨\n        \"\"\"\n        # TODO: å¯¦ä½œèªç¾©åˆ†å¡Š\n        # æç¤º:\n        # 1. å¦‚æœæœ‰èªç¾©æ¨¡å‹ï¼Œä½¿ç”¨é€²éšèªç¾©åˆ†å¡Š\n        # 2. å¦å‰‡é™ç´šåˆ°æ®µè½åˆ†å¡Š\n        # 3. åƒè€ƒä¸»è¦è§£ç­”æ–‡ä»¶ä¸­çš„å¯¦ä½œ\n        pass\n    \n    def _advanced_semantic_chunking(self, text: str, document_id: str) -> List[ProcessedChunk]:\n        \"\"\"é€²éšèªç¾©åˆ†å¡Šï¼ˆéœ€è¦ spaCy å’Œ SentenceTransformersï¼‰\"\"\"\n        # TODO: å¯¦ä½œé€²éšèªç¾©åˆ†å¡Š\n        # æç¤º:\n        # 1. ä½¿ç”¨ spaCy é€²è¡Œå¥å­åˆ†å‰²\n        # 2. è¨ˆç®—å¥å­åµŒå…¥å‘é‡\n        # 3. åŸºæ–¼ç›¸ä¼¼åº¦è­˜åˆ¥èªç¾©é‚Šç•Œ\n        # 4. ç”Ÿæˆ ProcessedChunk ç‰©ä»¶\n        pass\n    \n    def _paragraph_based_chunking(self, text: str, document_id: str) -> List[ProcessedChunk]:\n        \"\"\"åŸºæ–¼æ®µè½çš„åˆ†å¡Šï¼ˆé™ç´šæ–¹æ¡ˆï¼‰\"\"\"\n        # TODO: å¯¦ä½œæ®µè½åˆ†å¡Š\n        # æç¤º:\n        # 1. æŒ‰æ®µè½åˆ†å‰²ï¼ˆ\\n\\nï¼‰\n        # 2. æ§åˆ¶åˆ†å¡Šå¤§å°\n        # 3. ç”Ÿæˆ ProcessedChunk ç‰©ä»¶\n        pass\n\nprint(\"ğŸ“ è«‹åœ¨ä¸Šæ–¹å¯¦ä½œèªç¾©åˆ†å¡Šç›¸é—œæ–¹æ³•\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä»»å‹™ 2 æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å–æ¶ˆè¨»è§£ä¸¦æ¸¬è©¦æ‚¨çš„å¯¦ä½œ\n",
    "# chunker = BasicChunker(min_words=5, max_words=100)\n",
    "\n",
    "# # æ¸¬è©¦æ®µè½åˆ†å¡Š\n",
    "# paragraph_chunks = chunker.chunk_by_paragraph(test_content, \"test_doc_1\")\n",
    "# print(f\"æ®µè½åˆ†å¡Šæ•¸é‡: {len(paragraph_chunks)}\")\n",
    "# for chunk in paragraph_chunks[:2]:\n",
    "#     print(f\"åˆ†å¡Š {chunk.chunk_index}: {chunk.word_count} å­—\")\n",
    "#     print(f\"å…§å®¹: {chunk.content[:100]}...\\n\")\n",
    "\n",
    "# # æ¸¬è©¦å¥å­åˆ†å¡Š\n",
    "# sentence_chunks = chunker.chunk_by_sentences(test_content, \"test_doc_1\")\n",
    "# print(f\"å¥å­åˆ†å¡Šæ•¸é‡: {len(sentence_chunks)}\")\n",
    "\n",
    "print(\"è«‹å…ˆå®Œæˆ BasicChunker çš„å¯¦ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ ä»»å‹™ 3: å¯¦ä½œ QualityAssessor\n",
    "\n",
    "**è¦æ±‚**:\n",
    "1. è¨ˆç®—å¯è®€æ€§åˆ†æ•¸ï¼ˆåŸºæ–¼å¹³å‡å¥å­é•·åº¦ï¼‰\n",
    "2. è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸ï¼ˆåŸºæ–¼å…§å®¹é•·åº¦å’Œçµæ§‹ï¼‰\n",
    "3. è¨ˆç®—ç¶œåˆå“è³ªåˆ†æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class EnterpriseDocumentProcessor:\n    \"\"\"ä¼æ¥­ç´šæ–‡æª”è™•ç†å™¨ - å“è³ªè©•ä¼°æ–¹æ³•\"\"\"\n    \n    def _assess_quality(self, text: str, chunks: List[ProcessedChunk]) -> QualityMetrics:\n        \"\"\"\n        å¤šç¶­åº¦æ–‡æª”å“è³ªè©•ä¼°\n        åŸºæ–¼ ISO 25012 æ¨™æº–\n        \n        Args:\n            text: åŸå§‹æ–‡æª”å…§å®¹\n            chunks: åˆ†å¡Šåˆ—è¡¨\n            \n        Returns:\n            QualityMetrics: å“è³ªè©•ä¼°çµæœ\n        \"\"\"\n        # TODO: å¯¦ä½œå“è³ªè©•ä¼°\n        # æç¤º:\n        # 1. è¨ˆç®—æº–ç¢ºæ€§åˆ†æ•¸\n        # 2. è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸ \n        # 3. è¨ˆç®—å¯è®€æ€§åˆ†æ•¸\n        # 4. è¨ˆç®—çµæ§‹åˆ†æ•¸\n        # 5. è¨ˆç®—ç¶œåˆåˆ†æ•¸ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰\n        pass\n    \n    def _calculate_accuracy(self, text: str) -> float:\n        \"\"\"è¨ˆç®—æ–‡æœ¬æº–ç¢ºæ€§ï¼ˆèªè¨€å“è³ªï¼‰\"\"\"\n        # TODO: å¯¦ä½œæº–ç¢ºæ€§è¨ˆç®—\n        # æç¤º: åŸºæ–¼è©å½™æœ‰æ•ˆæ€§ã€æ–‡æœ¬ç‰¹å¾µç­‰\n        pass\n    \n    def _calculate_completeness(self, text: str, chunks: List[ProcessedChunk]) -> float:\n        \"\"\"è¨ˆç®—å…§å®¹å®Œæ•´æ€§\"\"\"\n        # TODO: å¯¦ä½œå®Œæ•´æ€§è¨ˆç®—\n        # æç¤º: åŸºæ–¼åˆ†å¡Šåˆ†ä½ˆå‡å‹»æ€§ã€å…§å®¹é•·åº¦ç­‰\n        pass\n    \n    def _calculate_readability(self, text: str) -> float:\n        \"\"\"è¨ˆç®—æ–‡æœ¬å¯è®€æ€§\"\"\"\n        # TODO: å¯¦ä½œå¯è®€æ€§è¨ˆç®—\n        # æç¤º: å¯ä½¿ç”¨ Flesch Reading Ease æˆ–ç°¡åŒ–ç®—æ³•\n        pass\n    \n    def _calculate_structure_score(self, text: str) -> float:\n        \"\"\"è¨ˆç®—æ–‡æª”çµæ§‹åˆ†æ•¸\"\"\"\n        # TODO: å¯¦ä½œçµæ§‹åˆ†æ•¸è¨ˆç®—\n        # æç¤º: æª¢æŸ¥æ®µè½ã€å¥å­ã€æ¨™é¡Œã€ç« ç¯€ç­‰çµæ§‹å…ƒç´ \n        pass\n\nprint(\"ğŸ“ è«‹åœ¨ä¸Šæ–¹å¯¦ä½œå“è³ªè©•ä¼°ç›¸é—œæ–¹æ³•\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä»»å‹™ 3 æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å–æ¶ˆè¨»è§£ä¸¦æ¸¬è©¦æ‚¨çš„å¯¦ä½œ\n",
    "# assessor = QualityAssessor()\n",
    "# quality = assessor.assess_quality(test_content, paragraph_chunks)\n",
    "\n",
    "# print(f\"å¯è®€æ€§åˆ†æ•¸: {quality.readability:.3f}\")\n",
    "# print(f\"å®Œæ•´æ€§åˆ†æ•¸: {quality.completeness:.3f}\")\n",
    "# print(f\"ç¶œåˆå“è³ª: {quality.overall:.3f}\")\n",
    "\n",
    "print(\"è«‹å…ˆå®Œæˆ QualityAssessor çš„å¯¦ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ ä»»å‹™ 4: æ•´åˆç‚º SimpleDocumentProcessor\n",
    "\n",
    "**è¦æ±‚**:\n",
    "1. æ•´åˆæ‰€æœ‰çµ„ä»¶\n",
    "2. å¯¦ä½œå®Œæ•´çš„è™•ç†æµç¨‹\n",
    "3. æä¾›çµ±ä¸€çš„ä»‹é¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class EnterpriseDocumentProcessor:\n    \"\"\"ä¼æ¥­ç´šæ–‡æª”è™•ç†å™¨ - å®Œæ•´æµç¨‹æ•´åˆ\"\"\"\n    \n    def process_document(self, file_path: str) -> Tuple[DocumentMetadata, List[ProcessedChunk], QualityMetrics]:\n        \"\"\"\n        å®Œæ•´çš„æ–‡æª”è™•ç†æµç¨‹\n        \n        Args:\n            file_path: æª”æ¡ˆè·¯å¾‘\n            \n        Returns:\n            Tuple[DocumentMetadata, List[ProcessedChunk], QualityMetrics]\n        \"\"\"\n        # TODO: å¯¦ä½œå®Œæ•´è™•ç†æµç¨‹\n        # æç¤º:\n        # 1. æå–å…ƒè³‡æ–™\n        # 2. æå–æ–‡æª”å…§å®¹\n        # 3. åŸ·è¡Œèªç¾©åˆ†å¡Š\n        # 4. é€²è¡Œå“è³ªè©•ä¼°\n        # 5. æ›´æ–°è™•ç†ç‹€æ…‹\n        pass\n    \n    def _extract_text_content(self, file_path: str) -> str:\n        \"\"\"æå–æ–‡æœ¬å…§å®¹ï¼ˆå…¼å®¹å¤šç¨®æ–¹æ³•ï¼‰\"\"\"\n        # TODO: å¯¦ä½œæ–‡æœ¬æå–\n        # æç¤º:\n        # 1. æ”¯æ´ .txt æ–‡ä»¶ç›´æ¥è®€å–\n        # 2. å¦‚æœæœ‰ Doclingï¼Œè™•ç† PDF/DOCX\n        # 3. é™ç´šè™•ç†æ–¹æ¡ˆ\n        pass\n\n# æ‰¹æ¬¡è™•ç†å™¨\nclass BatchDocumentProcessor:\n    \"\"\"æ‰¹æ¬¡æ–‡æª”è™•ç†å™¨\"\"\"\n    \n    def __init__(self, processor: EnterpriseDocumentProcessor):\n        self.processor = processor\n        self.processing_results = []\n    \n    def process_directory(self, directory_path: str, file_patterns: List[str] = ['*.pdf', '*.txt', '*.docx']) -> Dict:\n        \"\"\"\n        è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„æ–‡æª”\n        \"\"\"\n        # TODO: å¯¦ä½œæ‰¹æ¬¡è™•ç†\n        # æç¤º:\n        # 1. æ”¶é›†éœ€è¦è™•ç†çš„æ–‡ä»¶\n        # 2. é€ä¸€è™•ç†æ¯å€‹æ–‡ä»¶\n        # 3. æ”¶é›†çµ±è¨ˆçµæœ\n        pass\n    \n    def generate_report(self) -> pd.DataFrame:\n        \"\"\"ç”Ÿæˆè™•ç†å ±å‘Š\"\"\"\n        # TODO: å¯¦ä½œå ±å‘Šç”Ÿæˆ\n        pass\n\nprint(\"ğŸ“ è«‹åœ¨ä¸Šæ–¹å¯¦ä½œå®Œæ•´çš„æ–‡æª”è™•ç†æµç¨‹\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ ä»»å‹™ 5: æ¸¬è©¦èˆ‡é©—è­‰\n",
    "\n",
    "### å»ºç«‹æ¸¬è©¦è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹å¤šå€‹æ¸¬è©¦æª”æ¡ˆ\n",
    "test_files = {\n",
    "    'research_paper.txt': \"\"\"\n",
    "Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "Abstract\n",
    "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.\n",
    "\n",
    "1. Introduction\n",
    "Machine translation is a challenging task that has attracted researchers for decades. The goal is to translate a sentence from one language to another while preserving the meaning.\n",
    "\n",
    "2. Background\n",
    "Traditional approaches to machine translation rely on statistical models. These models are typically trained on large parallel corpora.\n",
    "\n",
    "3. Methodology\n",
    "We propose an attention mechanism that allows the model to focus on different parts of the input sentence when generating each word of the output.\n",
    "\n",
    "4. Experiments\n",
    "We evaluate our approach on several language pairs including English-French and English-German translation tasks.\n",
    "\n",
    "5. Conclusion\n",
    "Our results show significant improvements over baseline methods. The attention mechanism proves to be effective for neural machine translation.\n",
    "\"\"\",\n",
    "    \n",
    "    'business_report.txt': \"\"\"\n",
    "Quarterly Business Report - Q3 2024\n",
    "\n",
    "Executive Summary\n",
    "This report summarizes the company's performance in Q3 2024. Revenue increased by 15% compared to the previous quarter.\n",
    "\n",
    "Financial Performance\n",
    "Total revenue reached $2.5M in Q3. Operating expenses were controlled at $1.8M, resulting in a healthy profit margin.\n",
    "\n",
    "Market Analysis\n",
    "The market showed strong demand for our products. Customer satisfaction scores improved to 4.2/5.0.\n",
    "\n",
    "Recommendations\n",
    "We recommend increasing investment in R&D to maintain competitive advantage.\n",
    "\"\"\",\n",
    "    \n",
    "    'simple_document.txt': \"\"\"\n",
    "Simple Document Example\n",
    "\n",
    "This is a simple document for testing purposes. It contains multiple paragraphs with different content.\n",
    "\n",
    "The second paragraph discusses various topics. It should be processed correctly by our document processor.\n",
    "\n",
    "Finally, this is the last paragraph. It concludes the simple document example.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦ç›®éŒ„å’Œæª”æ¡ˆ\n",
    "test_dir = EXERCISE_DIR / 'test_documents'\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename, content in test_files.items():\n",
    "    file_path = test_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"âœ… å»ºç«‹äº† {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆæ–¼ {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œå®Œæ•´æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: å®Œæˆæ‰€æœ‰çµ„ä»¶å¯¦ä½œå¾Œï¼Œå–æ¶ˆè¨»è§£é€²è¡Œæ¸¬è©¦\n\n# # åˆå§‹åŒ–è™•ç†å™¨\n# processor = EnterpriseDocumentProcessor()\n\n# # è™•ç†å–®ä¸€æª”æ¡ˆæ¸¬è©¦\n# print(\"ğŸ§ª æ¸¬è©¦å–®ä¸€æª”æ¡ˆè™•ç†:\")\n# metadata, chunks, quality_metrics = processor.process_document(str(test_dir / 'research_paper.txt'))\n# print(f\"æ–‡æª”é¡å‹: {metadata.document_type}\")\n# print(f\"åˆ†å¡Šæ•¸é‡: {len(chunks)}\")\n# print(f\"å“è³ªåˆ†æ•¸: {quality_metrics.overall_score:.3f}\")\n\n# # æ‰¹æ¬¡è™•ç†æ¸¬è©¦\n# print(\"\\nğŸ§ª æ¸¬è©¦æ‰¹æ¬¡è™•ç†:\")\n# batch_processor = BatchDocumentProcessor(processor)\n# batch_results = batch_processor.process_directory(str(test_dir))\n# print(f\"è™•ç†äº† {len(batch_results['processed_files'])} å€‹æª”æ¡ˆ\")\n\n# # ç”Ÿæˆè™•ç†å ±å‘Š\n# report_df = batch_processor.generate_report()\n# print(\"\\nğŸ“Š è™•ç†å ±å‘Š:\")\n# print(report_df.to_string(index=False))\n\n# # å“è³ªåˆ†ä½ˆå¯è¦–åŒ–\n# if not report_df.empty:\n#     import plotly.express as px\n#     fig = px.bar(\n#         report_df, \n#         x='æ–‡ä»¶å', \n#         y='å“è³ªåˆ†æ•¸',\n#         title='å„æ–‡æª”å“è³ªåˆ†æ•¸æ¯”è¼ƒ',\n#         color='å“è³ªåˆ†æ•¸',\n#         color_continuous_scale='Viridis'\n#     )\n#     fig.show()\n\nprint(\"è«‹å…ˆå®Œæˆæ‰€æœ‰çµ„ä»¶çš„å¯¦ä½œï¼Œç„¶å¾Œå–æ¶ˆè¨»è§£é€²è¡Œæ¸¬è©¦\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç·´ç¿’ç¸½çµèˆ‡æª¢æ ¸\n",
    "\n",
    "### å®Œæˆåº¦æª¢æ ¸\n",
    "\n",
    "è«‹ç¢ºèªæ‚¨å·²å®Œæˆä»¥ä¸‹ä»»å‹™ï¼š\n",
    "\n",
    "- [ ] âœ… **ä»»å‹™ 1**: `SimpleMetadataExtractor` - å…ƒè³‡æ–™æå–\n",
    "- [ ] âœ… **ä»»å‹™ 2**: `BasicChunker` - æ–‡æª”åˆ†å¡Š\n",
    "- [ ] âœ… **ä»»å‹™ 3**: `QualityAssessor` - å“è³ªè©•ä¼°\n",
    "- [ ] âœ… **ä»»å‹™ 4**: `SimpleDocumentProcessor` - æµç¨‹æ•´åˆ\n",
    "- [ ] âœ… **ä»»å‹™ 5**: æ¸¬è©¦èˆ‡é©—è­‰\n",
    "\n",
    "### å­¸ç¿’æˆæœ\n",
    "\n",
    "å®Œæˆæœ¬ç·´ç¿’å¾Œï¼Œæ‚¨æ‡‰è©²æŒæ¡äº†ï¼š\n",
    "\n",
    "1. **æ–‡æª”è™•ç†åŸºæœ¬æµç¨‹** - å¾åŸå§‹æ–‡ä»¶åˆ°çµæ§‹åŒ–è³‡æ–™\n",
    "2. **å…ƒè³‡æ–™æå–æŠ€è¡“** - æª”æ¡ˆè³‡è¨Šå’Œå…§å®¹åˆ†æ\n",
    "3. **åˆ†å¡Šç­–ç•¥è¨­è¨ˆ** - æ®µè½å’Œå¥å­ç´šåˆ¥çš„åˆ†å‰²\n",
    "4. **å“è³ªè©•ä¼°æ–¹æ³•** - å¤šç¶­åº¦å“è³ªæŒ‡æ¨™è¨ˆç®—\n",
    "5. **ç³»çµ±æ•´åˆèƒ½åŠ›** - çµ„ä»¶åŒ–è¨­è¨ˆå’Œæµç¨‹ä¸²æ¥\n",
    "\n",
    "### å»¶ä¼¸æŒ‘æˆ°\n",
    "\n",
    "æƒ³è¦é€²ä¸€æ­¥æå‡ï¼Ÿå˜—è©¦é€™äº›æŒ‘æˆ°ï¼š\n",
    "\n",
    "1. **å¢å¼·åˆ†å¡Šç®—æ³•** - å¯¦ä½œé‡ç–Šåˆ†å¡Šã€èªç¾©åˆ†å¡Š\n",
    "2. **è±å¯Œå“è³ªæŒ‡æ¨™** - åŠ å…¥èªæ³•æª¢æŸ¥ã€è¡“èªä¸€è‡´æ€§\n",
    "3. **æ”¯æ´å¤šæ ¼å¼** - æ“´å±•æ”¯æ´ PDFã€DOCX æ ¼å¼\n",
    "4. **ä¸¦ç™¼è™•ç†** - å¯¦ä½œå¤šç·šç¨‹æ‰¹æ¬¡è™•ç†\n",
    "5. **çµæœå¯è¦–åŒ–** - æ·»åŠ åœ–è¡¨å±•ç¤ºè™•ç†çµæœ\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å®Œæˆç·´ç¿’å¾Œï¼Œè«‹é€²å…¥è§£ç­”æª”æ¡ˆæ¯”å°æ‚¨çš„å¯¦ä½œï¼Œç„¶å¾Œç¹¼çºŒå­¸ç¿’æ¨¡çµ„ 2ï¼šå…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}