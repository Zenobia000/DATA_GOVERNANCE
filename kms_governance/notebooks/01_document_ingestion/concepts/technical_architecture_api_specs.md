# Document Ingestion æŠ€è¡“æ¶æ§‹èˆ‡ API è¦æ ¼
## Technical Architecture & API Specifications - Document Ingestion Module

> **ç³»çµ±è¨­è¨ˆ**: ä¼æ¥­ç´šæ–‡æª”æ”å–ç³»çµ±çš„å®Œæ•´æŠ€è¡“æ¶æ§‹èˆ‡æ¥å£è¦æ ¼

---

## ğŸ—ï¸ ç³»çµ±æ•´é«”æ¶æ§‹

### æ¶æ§‹è¨­è¨ˆåŸå‰‡

1. **å¾®æœå‹™æ¶æ§‹** - æœå‹™è§£è€¦ï¼Œç¨ç«‹éƒ¨ç½²èˆ‡æ“´å±•
2. **äº‹ä»¶é©…å‹•** - éåŒæ­¥è™•ç†ï¼Œæå‡ç³»çµ±ååé‡
3. **é›²åŸç”Ÿ** - å®¹å™¨åŒ–éƒ¨ç½²ï¼Œæ”¯æ´æ°´å¹³æ“´å±•
4. **å®‰å…¨å„ªå…ˆ** - ç«¯åˆ°ç«¯åŠ å¯†ï¼Œé›¶ä¿¡ä»»æ¶æ§‹
5. **å¯è§€æ¸¬æ€§** - å®Œæ•´çš„ç›£æ§ã€æ—¥èªŒã€è¿½è¹¤

### ç³»çµ±æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph "å‰ç«¯å±¤ (Frontend Layer)"
        UI[React SPA]
        Mobile[Mobile App]
    end

    subgraph "API é–˜é“å±¤ (API Gateway Layer)"
        Gateway[Kong/Nginx]
        Auth[OAuth 2.0]
        RateLimit[Rate Limiting]
    end

    subgraph "æ‡‰ç”¨æœå‹™å±¤ (Application Services)"
        DocService[Document Service]
        ChunkService[Chunking Service]
        TagService[Tagging Service]
        QualityService[Quality Service]
        UserService[User Service]
    end

    subgraph "è™•ç†å¼•æ“å±¤ (Processing Engines)"
        DocEngine[Document Parser]
        MLEngine[ML Processing]
        QualityEngine[Quality Assessor]
    end

    subgraph "è³‡æ–™å­˜å„²å±¤ (Data Storage Layer)"
        PostgreSQL[(PostgreSQL)]
        ChromaDB[(ChromaDB)]
        Redis[(Redis Cache)]
        MinIO[(MinIO Storage)]
    end

    subgraph "åŸºç¤è¨­æ–½å±¤ (Infrastructure Layer)"
        K8s[Kubernetes]
        Monitor[Monitoring Stack]
        CI_CD[CI/CD Pipeline]
    end

    subgraph "å¤–éƒ¨æœå‹™ (External Services)"
        OCR[OCR Service]
        NLP[NLP APIs]
        Storage[Cloud Storage]
    end

    UI --> Gateway
    Mobile --> Gateway
    Gateway --> DocService
    Gateway --> ChunkService
    Gateway --> TagService
    Gateway --> QualityService
    Gateway --> UserService

    DocService --> DocEngine
    ChunkService --> MLEngine
    TagService --> MLEngine
    QualityService --> QualityEngine

    DocEngine --> PostgreSQL
    DocEngine --> MinIO
    MLEngine --> ChromaDB
    MLEngine --> Redis
    QualityEngine --> PostgreSQL

    DocEngine --> OCR
    MLEngine --> NLP

    K8s --> Monitor
    K8s --> CI_CD
```

---

## ğŸ”§ å¾®æœå‹™è¨­è¨ˆèˆ‡è²¬ä»»åŠƒåˆ†

### æœå‹™é‚Šç•Œèˆ‡è·è²¬

#### 1. Document Service (æ–‡æª”æœå‹™)
**è·è²¬**: æ–‡æª”ç”Ÿå‘½é€±æœŸç®¡ç†èˆ‡åŸºç¤è™•ç†

**æ ¸å¿ƒåŠŸèƒ½**:
- æ–‡æª”ä¸Šå‚³èˆ‡å­˜å„²
- æ–‡æª”è§£æèˆ‡å…§å®¹æå–
- æ–‡æª”å…ƒè³‡æ–™ç®¡ç†
- æ–‡æª”ç‰ˆæœ¬æ§åˆ¶
- æª”æ¡ˆæ ¼å¼é©—è­‰

**æŠ€è¡“æ£§**:
- **Framework**: FastAPI + Python 3.11
- **Storage**: MinIO (æª”æ¡ˆå­˜å„²) + PostgreSQL (å…ƒè³‡æ–™)
- **Processing**: Docling + PyPDF2 + python-docx
- **Queue**: Celery + Redis

#### 2. Chunking Service (åˆ†å¡Šæœå‹™)
**è·è²¬**: æ™ºèƒ½æ–‡æª”åˆ†å¡Šèˆ‡åˆ†å¡Šç®¡ç†

**æ ¸å¿ƒåŠŸèƒ½**:
- å¤šç­–ç•¥åˆ†å¡Šç®—æ³•
- åˆ†å¡Šå“è³ªè©•ä¼°
- åˆ†å¡Šé‚Šç•Œèª¿æ•´
- åˆ†å¡Šç‰ˆæœ¬æ§åˆ¶
- åˆ†å¡Šçµ±è¨ˆåˆ†æ

**æŠ€è¡“æ£§**:
- **Framework**: FastAPI + Python 3.11
- **ML**: sentence-transformers + spaCy
- **Storage**: PostgreSQL + ChromaDB
- **Cache**: Redis

#### 3. Tagging Service (æ¨™ç±¤æœå‹™)
**è·è²¬**: æ™ºèƒ½æ¨™ç±¤ç”Ÿæˆèˆ‡ç®¡ç†

**æ ¸å¿ƒåŠŸèƒ½**:
- è‡ªå‹•æ¨™ç±¤ç”Ÿæˆ
- æ¨™ç±¤å±¤ç´šç®¡ç†
- æ¨™ç±¤æ¨è–¦å¼•æ“
- æ¨™ç±¤å“è³ªè©•ä¼°
- å”ä½œæ¨™è¨»åŠŸèƒ½

**æŠ€è¡“æ£§**:
- **Framework**: FastAPI + Python 3.11
- **NLP**: spaCy + NLTK + scikit-learn
- **ML**: è‡ªè¨“ç·´åˆ†é¡æ¨¡å‹
- **Storage**: PostgreSQL + Redis

#### 4. Quality Service (å“è³ªæœå‹™)
**è·è²¬**: å¤šç¶­åº¦å“è³ªè©•ä¼°èˆ‡ç›£æ§

**æ ¸å¿ƒåŠŸèƒ½**:
- ISO 25012 å“è³ªè©•ä¼°
- ç•°å¸¸æª¢æ¸¬èˆ‡å‘Šè­¦
- å“è³ªè¶¨å‹¢åˆ†æ
- è‡ªå‹•ä¿®å¾©å»ºè­°
- å“è³ªå ±å‘Šç”Ÿæˆ

**æŠ€è¡“æ£§**:
- **Framework**: FastAPI + Python 3.11
- **ML**: scikit-learn + PyOD
- **Analysis**: pandas + numpy
- **Storage**: PostgreSQL + InfluxDB (æ™‚é–“åºåˆ—)

#### 5. User Service (ç”¨æˆ¶æœå‹™)
**è·è²¬**: ç”¨æˆ¶ç®¡ç†èˆ‡æ¬Šé™æ§åˆ¶

**æ ¸å¿ƒåŠŸèƒ½**:
- ç”¨æˆ¶èªè­‰èˆ‡æˆæ¬Š
- è§’è‰²æ¬Šé™ç®¡ç†
- æ“ä½œæ—¥èªŒè¨˜éŒ„
- ç”¨æˆ¶åå¥½è¨­å®š
- åœ˜éšŠç®¡ç†

**æŠ€è¡“æ£§**:
- **Framework**: FastAPI + Python 3.11
- **Auth**: OAuth 2.0 + JWT
- **Storage**: PostgreSQL
- **Cache**: Redis

---

## ğŸŒ RESTful API è©³ç´°è¦æ ¼

### API è¨­è¨ˆåŸå‰‡

1. **RESTful æ¨™æº–**: éµå¾ª HTTP èªç¾©èˆ‡ REST ç´„å®š
2. **ç‰ˆæœ¬æ§åˆ¶**: API ç‰ˆæœ¬å‰ç¶´ `/api/v1/`
3. **çµ±ä¸€éŸ¿æ‡‰æ ¼å¼**: æ¨™æº–åŒ–çš„æˆåŠŸèˆ‡éŒ¯èª¤éŸ¿æ‡‰
4. **åˆ†é æ”¯æ´**: å¤§æ•¸æ“šé›†çš„åˆ†é èˆ‡æ’åº
5. **éæ¿¾èˆ‡æœå°‹**: éˆæ´»çš„æŸ¥è©¢åƒæ•¸æ”¯æ´

### çµ±ä¸€éŸ¿æ‡‰æ ¼å¼

#### æˆåŠŸéŸ¿æ‡‰æ ¼å¼
```json
{
  "status": "success",
  "message": "æ“ä½œæˆåŠŸ",
  "data": {
    // å¯¦éš›è³‡æ–™å…§å®¹
  },
  "meta": {
    "timestamp": "2024-01-17T10:30:00Z",
    "request_id": "req_12345",
    "pagination": {  // å¦‚æœ‰åˆ†é 
      "page": 1,
      "limit": 20,
      "total": 100,
      "has_next": true
    }
  }
}
```

#### éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼
```json
{
  "status": "error",
  "error": {
    "code": "INVALID_FILE_FORMAT",
    "message": "ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼",
    "details": "åªæ”¯æ´ PDF, DOCX, TXT æ ¼å¼",
    "field": "file"  // å¦‚æœæ˜¯æ¬„ä½é©—è­‰éŒ¯èª¤
  },
  "meta": {
    "timestamp": "2024-01-17T10:30:00Z",
    "request_id": "req_12345"
  }
}
```

---

## ğŸ“¤ Document Service API è¦æ ¼

### æ–‡æª”ç®¡ç† API

#### 1. æ–‡æª”ä¸Šå‚³
```http
POST /api/v1/documents/upload
Content-Type: multipart/form-data
Authorization: Bearer {token}

# Request Body (Form Data)
file: [binary file data]
metadata: {
  "title": "å¯é¸çš„æ–‡æª”æ¨™é¡Œ",
  "description": "æ–‡æª”æè¿°",
  "tags": ["tag1", "tag2"],
  "category": "å ±å‘Š",
  "custom_metadata": {}
}

# Response (201 Created)
{
  "status": "success",
  "message": "æ–‡æª”ä¸Šå‚³æˆåŠŸ",
  "data": {
    "document_id": "doc_abc123",
    "filename": "report.pdf",
    "size": 1024000,
    "format": "pdf",
    "upload_url": "https://storage/doc_abc123.pdf",
    "processing_status": "pending",
    "estimated_processing_time": 30
  }
}

# Error Responses
400 - æª”æ¡ˆæ ¼å¼ä¸æ”¯æ´
413 - æª”æ¡ˆéå¤§
422 - å…ƒè³‡æ–™é©—è­‰å¤±æ•—
500 - æœå‹™å™¨å…§éƒ¨éŒ¯èª¤
```

#### 2. æ‰¹æ¬¡æ–‡æª”ä¸Šå‚³
```http
POST /api/v1/documents/batch-upload
Content-Type: multipart/form-data
Authorization: Bearer {token}

# Request Body
files[]: [multiple binary files]
batch_metadata: {
  "batch_name": "2024Q1å ±å‘Š",
  "default_category": "è²¡å‹™å ±å‘Š",
  "auto_process": true,
  "notification_email": "user@company.com"
}

# Response (202 Accepted)
{
  "status": "success",
  "message": "æ‰¹æ¬¡ä¸Šå‚³å·²é–‹å§‹",
  "data": {
    "batch_id": "batch_xyz789",
    "total_files": 15,
    "estimated_total_time": 450,
    "progress_url": "/api/v1/batches/xyz789/progress",
    "webhook_url": "/api/v1/batches/xyz789/webhook"
  }
}
```

#### 3. æ–‡æª”è™•ç†ç‹€æ…‹æŸ¥è©¢
```http
GET /api/v1/documents/{document_id}/status
Authorization: Bearer {token}

# Response (200 OK)
{
  "status": "success",
  "data": {
    "document_id": "doc_abc123",
    "processing_status": "processing",
    "progress_percentage": 45,
    "current_stage": "text_extraction",
    "stages": {
      "upload": { "status": "completed", "time": "2024-01-17T10:30:00Z" },
      "validation": { "status": "completed", "time": "2024-01-17T10:30:05Z" },
      "text_extraction": { "status": "processing", "progress": 45 },
      "chunking": { "status": "pending" },
      "tagging": { "status": "pending" },
      "quality_assessment": { "status": "pending" }
    },
    "estimated_completion": "2024-01-17T10:31:00Z"
  }
}
```

#### 4. æ–‡æª”åˆ—è¡¨æŸ¥è©¢
```http
GET /api/v1/documents
Authorization: Bearer {token}

# Query Parameters
?page=1&limit=20&sort=created_date&order=desc&status=completed&category=å ±å‘Š&search=æ©Ÿå™¨å­¸ç¿’

# Response (200 OK)
{
  "status": "success",
  "data": [
    {
      "document_id": "doc_abc123",
      "title": "æ©Ÿå™¨å­¸ç¿’æ‡‰ç”¨å ±å‘Š",
      "filename": "ml_report.pdf",
      "size": 1024000,
      "format": "pdf",
      "category": "æŠ€è¡“å ±å‘Š",
      "tags": ["æ©Ÿå™¨å­¸ç¿’", "AI", "æŠ€è¡“"],
      "created_date": "2024-01-17T10:30:00Z",
      "modified_date": "2024-01-17T10:35:00Z",
      "processing_status": "completed",
      "quality_score": 0.87,
      "chunk_count": 15,
      "author": "å¼µä¸‰",
      "department": "æŠ€è¡“éƒ¨"
    }
  ],
  "meta": {
    "pagination": {
      "page": 1,
      "limit": 20,
      "total": 156,
      "has_next": true,
      "has_prev": false
    },
    "filters_applied": {
      "status": "completed",
      "category": "å ±å‘Š"
    }
  }
}
```

#### 5. æ–‡æª”è©³ç´°è³‡è¨Š
```http
GET /api/v1/documents/{document_id}
Authorization: Bearer {token}

# Query Parameters
?include=chunks,tags,quality,lineage

# Response (200 OK)
{
  "status": "success",
  "data": {
    "document_id": "doc_abc123",
    "title": "æ©Ÿå™¨å­¸ç¿’æ‡‰ç”¨å ±å‘Š",
    "filename": "ml_report.pdf",
    "content_preview": "å ±å‘Šæ‘˜è¦ï¼šæœ¬å ±å‘Šåˆ†æäº†...",
    "metadata": {
      "authors": ["å¼µä¸‰", "æå››"],
      "organization": "ç§‘æŠ€å…¬å¸",
      "department": "æŠ€è¡“éƒ¨",
      "created_date": "2024-01-17T10:30:00Z",
      "document_type": "æŠ€è¡“å ±å‘Š",
      "language": "zh-TW",
      "page_count": 25,
      "word_count": 8500,
      "custom_fields": {}
    },
    "processing_info": {
      "processed_date": "2024-01-17T10:35:00Z",
      "processing_time": 45.2,
      "processor_version": "1.0",
      "processing_config": {
        "chunk_strategy": "semantic",
        "chunk_size": 512,
        "chunk_overlap": 50
      }
    },
    "chunks": [
      {
        "chunk_id": "chunk_001",
        "content": "æ©Ÿå™¨å­¸ç¿’æ˜¯ä¸€ç¨®äººå·¥æ™ºæ…§...",
        "chunk_index": 0,
        "word_count": 156,
        "semantic_type": "text",
        "quality_score": 0.89,
        "start_page": 1,
        "end_page": 1
      }
    ],
    "tags": [
      {
        "tag_id": "tag_ml",
        "name": "æ©Ÿå™¨å­¸ç¿’",
        "category": "æŠ€è¡“",
        "confidence": 0.95,
        "source": "auto_generated"
      }
    ],
    "quality": {
      "overall_score": 0.87,
      "dimensions": {
        "accuracy": 0.92,
        "completeness": 0.85,
        "consistency": 0.88,
        "currency": 0.75,
        "understandability": 0.90,
        "traceability": 0.82
      },
      "issues": [],
      "recommendations": ["å»ºè­°æ›´æ–°è¼ƒèˆŠçš„åƒè€ƒè³‡æ–™"]
    }
  }
}
```

#### 6. æ–‡æª”æ›´æ–°
```http
PUT /api/v1/documents/{document_id}
Content-Type: application/json
Authorization: Bearer {token}

# Request Body
{
  "title": "æ›´æ–°å¾Œçš„æ¨™é¡Œ",
  "metadata": {
    "authors": ["å¼µä¸‰", "æå››", "ç‹äº”"],
    "department": "æŠ€è¡“éƒ¨",
    "custom_fields": {
      "project": "AIå°ˆæ¡ˆ",
      "version": "2.0"
    }
  },
  "tags": ["æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’", "AI"],
  "processing_config": {
    "rechunk": true,
    "chunk_strategy": "paragraph",
    "chunk_size": 400
  }
}

# Response (200 OK)
{
  "status": "success",
  "message": "æ–‡æª”æ›´æ–°æˆåŠŸ",
  "data": {
    "document_id": "doc_abc123",
    "updated_fields": ["title", "metadata", "tags"],
    "reprocessing_triggered": true,
    "estimated_completion": "2024-01-17T11:00:00Z"
  }
}
```

#### 7. æ–‡æª”åˆªé™¤
```http
DELETE /api/v1/documents/{document_id}
Authorization: Bearer {token}

# Query Parameters
?permanent=false&reason=éæœŸæ–‡æª”

# Response (200 OK)
{
  "status": "success",
  "message": "æ–‡æª”å·²æ¨™è¨˜åˆªé™¤",
  "data": {
    "document_id": "doc_abc123",
    "deletion_type": "soft_delete",
    "deleted_at": "2024-01-17T10:45:00Z",
    "restore_before": "2024-02-17T10:45:00Z",
    "restore_url": "/api/v1/documents/doc_abc123/restore"
  }
}
```

---

## ğŸ§© Chunking Service API è¦æ ¼

### åˆ†å¡Šè™•ç† API

#### 1. åŸ·è¡Œæ–‡æª”åˆ†å¡Š
```http
POST /api/v1/documents/{document_id}/chunk
Content-Type: application/json
Authorization: Bearer {token}

# Request Body
{
  "strategy": "semantic",  // paragraph, sentence, semantic, custom
  "config": {
    "min_chunk_size": 100,
    "max_chunk_size": 800,
    "overlap_percentage": 10,
    "preserve_sentences": true,
    "similarity_threshold": 0.7  // for semantic chunking
  },
  "options": {
    "force_rechunk": false,
    "preserve_existing": false,
    "notify_on_completion": true
  }
}

# Response (202 Accepted)
{
  "status": "success",
  "message": "åˆ†å¡Šè™•ç†å·²é–‹å§‹",
  "data": {
    "task_id": "task_chunk_456",
    "document_id": "doc_abc123",
    "strategy": "semantic",
    "estimated_completion": "2024-01-17T10:32:00Z",
    "progress_url": "/api/v1/tasks/task_chunk_456/progress"
  }
}
```

#### 2. ç²å–åˆ†å¡Šçµæœ
```http
GET /api/v1/documents/{document_id}/chunks
Authorization: Bearer {token}

# Query Parameters
?page=1&limit=50&sort=chunk_index&include_content=true&min_quality=0.7

# Response (200 OK)
{
  "status": "success",
  "data": [
    {
      "chunk_id": "chunk_001",
      "chunk_index": 0,
      "content": "é€™æ˜¯ç¬¬ä¸€å€‹åˆ†å¡Šçš„å…§å®¹...",
      "word_count": 156,
      "sentence_count": 8,
      "semantic_type": "introduction",
      "quality_score": 0.89,
      "start_page": 1,
      "end_page": 1,
      "start_offset": 0,
      "end_offset": 156,
      "created_date": "2024-01-17T10:35:00Z",
      "embedding_status": "completed",
      "tags": ["ä»‹ç´¹", "æ¦‚è¿°"]
    }
  ],
  "meta": {
    "pagination": {
      "page": 1,
      "limit": 50,
      "total": 23,
      "has_next": false
    },
    "summary": {
      "total_chunks": 23,
      "avg_word_count": 267,
      "avg_quality_score": 0.84,
      "chunking_strategy": "semantic"
    }
  }
}
```

#### 3. æ›´æ–°åˆ†å¡Šé‚Šç•Œ
```http
PUT /api/v1/chunks/{chunk_id}/boundary
Content-Type: application/json
Authorization: Bearer {token}

# Request Body
{
  "start_offset": 100,
  "end_offset": 500,
  "preserve_semantics": true,
  "auto_adjust": true,
  "reason": "èª¿æ•´èªç¾©é‚Šç•Œ"
}

# Response (200 OK)
{
  "status": "success",
  "message": "åˆ†å¡Šé‚Šç•Œå·²æ›´æ–°",
  "data": {
    "chunk_id": "chunk_001",
    "old_boundary": { "start": 0, "end": 400 },
    "new_boundary": { "start": 100, "end": 500 },
    "adjusted_boundary": { "start": 98, "end": 502 },
    "adjustment_reason": "èª¿æ•´åˆ°å¥å­é‚Šç•Œ",
    "affected_chunks": ["chunk_000", "chunk_002"],
    "updated_at": "2024-01-17T10:40:00Z"
  }
}
```

#### 4. åˆ†å¡Šåˆä½µ
```http
POST /api/v1/chunks/merge
Content-Type: application/json
Authorization: Bearer {token}

# Request Body
{
  "chunk_ids": ["chunk_001", "chunk_002", "chunk_003"],
  "merge_strategy": "sequential",  // sequential, semantic
  "new_chunk_title": "åˆä½µå¾Œçš„åˆ†å¡Š",
  "preserve_metadata": true
}

# Response (200 OK)
{
  "status": "success",
  "message": "åˆ†å¡Šåˆä½µæˆåŠŸ",
  "data": {
    "new_chunk_id": "chunk_merged_001",
    "merged_chunk_ids": ["chunk_001", "chunk_002", "chunk_003"],
    "content": "åˆä½µå¾Œçš„å®Œæ•´å…§å®¹...",
    "word_count": 567,
    "quality_score": 0.91,
    "merge_metadata": {
      "strategy": "sequential",
      "merged_at": "2024-01-17T10:42:00Z",
      "merged_by": "user_123"
    }
  }
}
```

#### 5. åˆ†å¡Šåˆ†å‰²
```http
POST /api/v1/chunks/{chunk_id}/split
Content-Type: application/json
Authorization: Bearer {token}

# Request Body
{
  "split_positions": [200, 400],  // åˆ†å‰²ä½ç½® (å­—ç¬¦åç§»)
  "auto_adjust_positions": true,
  "split_strategy": "sentence_boundary",
  "naming_pattern": "{original_id}_part_{index}"
}

# Response (200 OK)
{
  "status": "success",
  "message": "åˆ†å¡Šåˆ†å‰²æˆåŠŸ",
  "data": {
    "original_chunk_id": "chunk_001",
    "new_chunks": [
      {
        "chunk_id": "chunk_001_part_1",
        "content": "ç¬¬ä¸€éƒ¨åˆ†å…§å®¹...",
        "word_count": 123
      },
      {
        "chunk_id": "chunk_001_part_2",
        "content": "ç¬¬äºŒéƒ¨åˆ†å…§å®¹...",
        "word_count": 156
      },
      {
        "chunk_id": "chunk_001_part_3",
        "content": "ç¬¬ä¸‰éƒ¨åˆ†å…§å®¹...",
        "word_count": 134
      }
    ],
    "split_metadata": {
      "strategy": "sentence_boundary",
      "split_at": "2024-01-17T10:44:00Z"
    }
  }
}
```

---

## ğŸ·ï¸ Tagging Service API è¦æ ¼

### æ¨™ç±¤ç®¡ç† API

#### 1. è‡ªå‹•æ¨™ç±¤ç”Ÿæˆ
```http
POST /api/v1/documents/{document_id}/generate-tags
Content-Type: application/json
Authorization: Bearer {token}

# Request Body
{
  "content_source": "full_document",  // full_document, chunks, summary
  "tag_types": ["keywords", "entities", "topics", "categories"],
  "max_tags_per_type": 10,
  "min_confidence": 0.7,
  "custom_rules": [],
  "language": "zh-TW"
}

# Response (200 OK)
{
  "status": "success",
  "message": "æ¨™ç±¤ç”Ÿæˆå®Œæˆ",
  "data": {
    "document_id": "doc_abc123",
    "generated_tags": {
      "keywords": [
        {
          "tag": "æ©Ÿå™¨å­¸ç¿’",
          "confidence": 0.95,
          "frequency": 15,
          "positions": [23, 156, 289]
        },
        {
          "tag": "æ·±åº¦å­¸ç¿’",
          "confidence": 0.87,
          "frequency": 8,
          "positions": [78, 234]
        }
      ],
      "entities": {
        "PERSON": [
          {"tag": "å¼µä¸‰", "confidence": 0.92}
        ],
        "ORG": [
          {"tag": "ç§‘æŠ€å…¬å¸", "confidence": 0.89}
        ]
      },
      "topics": [
        {
          "tag": "äººå·¥æ™ºæ…§",
          "confidence": 0.91,
          "related_keywords": ["æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’", "ç¥ç¶“ç¶²è·¯"]
        }
      ],
      "categories": [
        {
          "tag": "æŠ€è¡“å ±å‘Š",
          "confidence": 0.84,
          "reasoning": "åŒ…å«æŠ€è¡“åˆ†æå’Œå¯¦é©—çµæœ"
        }
      ]
    },
    "generation_metadata": {
      "processing_time": 2.3,
      "model_version": "tag_gen_v1.2",
      "total_candidates": 45,
      "selected_tags": 18
    }
  }
}
```

#### 2. æ¨™ç±¤ç®¡ç† CRUD
```http
# ç²å–æ‰€æœ‰æ¨™ç±¤
GET /api/v1/tags
?category=æŠ€è¡“&search=æ©Ÿå™¨&sort=usage_count&limit=50

# å»ºç«‹æ–°æ¨™ç±¤
POST /api/v1/tags
{
  "name": "é‡å­è¨ˆç®—",
  "category": "æŠ€è¡“",
  "description": "é‡å­è¨ˆç®—ç›¸é—œå…§å®¹",
  "color": "#FF6B6B",
  "aliases": ["quantum computing", "é‡å­è¨ˆç®—æ©Ÿ"],
  "parent_tag_id": "tag_advanced_tech"
}

# æ›´æ–°æ¨™ç±¤
PUT /api/v1/tags/{tag_id}
{
  "name": "æ©Ÿå™¨å­¸ç¿’èˆ‡AI",
  "merge_with": ["tag_ml", "tag_ai"],
  "update_existing_documents": true
}

# åˆªé™¤æ¨™ç±¤
DELETE /api/v1/tags/{tag_id}
?replace_with=tag_alternative&update_documents=true
```

#### 3. æ–‡æª”æ¨™ç±¤æ“ä½œ
```http
# ç‚ºæ–‡æª”æ·»åŠ æ¨™ç±¤
POST /api/v1/documents/{document_id}/tags
{
  "tag_ids": ["tag_001", "tag_002"],
  "tags": [
    {
      "name": "æ–°æ¨™ç±¤",
      "category": "è‡ªè¨‚",
      "confidence": 1.0,
      "source": "manual"
    }
  ],
  "replace_existing": false
}

# ç§»é™¤æ–‡æª”æ¨™ç±¤
DELETE /api/v1/documents/{document_id}/tags
{
  "tag_ids": ["tag_003", "tag_004"],
  "reason": "æ¨™ç±¤ä¸ç›¸é—œ"
}

# æ‰¹æ¬¡æ¨™ç±¤æ“ä½œ
POST /api/v1/documents/batch-tag
{
  "document_ids": ["doc_001", "doc_002", "doc_003"],
  "operation": "add",  // add, remove, replace
  "tag_ids": ["tag_urgent", "tag_review_needed"],
  "apply_to_chunks": true
}
```

---

## ğŸ—„ï¸ è³‡æ–™åº« Schema è¨­è¨ˆ

### PostgreSQL ä¸»è¦è³‡æ–™è¡¨

#### 1. Documents è¡¨ (æ–‡æª”ä¸»è¡¨)
```sql
CREATE TABLE documents (
    document_id VARCHAR(50) PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    filename VARCHAR(255) NOT NULL,
    original_filename VARCHAR(255) NOT NULL,

    -- æª”æ¡ˆè³‡è¨Š
    file_size BIGINT NOT NULL,
    file_format VARCHAR(10) NOT NULL,
    file_path VARCHAR(1000) NOT NULL,
    file_hash VARCHAR(64) NOT NULL,  -- SHA-256

    -- å…§å®¹çµ±è¨ˆ
    page_count INTEGER DEFAULT 1,
    word_count INTEGER DEFAULT 0,
    paragraph_count INTEGER DEFAULT 0,
    sentence_count INTEGER DEFAULT 0,

    -- åˆ†é¡è³‡è¨Š
    document_type VARCHAR(50),
    category VARCHAR(100),
    subcategory VARCHAR(100),
    language VARCHAR(10) DEFAULT 'zh-TW',

    -- ä½œè€…èˆ‡ä¾†æº
    authors JSONB DEFAULT '[]',
    organization VARCHAR(200),
    department VARCHAR(100),
    source_url VARCHAR(1000),

    -- è™•ç†ç‹€æ…‹
    processing_status VARCHAR(20) DEFAULT 'pending',
    processed_date TIMESTAMP,
    processing_time REAL,  -- seconds
    processor_version VARCHAR(20),
    processing_config JSONB,

    -- å“è³ªè³‡è¨Š
    quality_score REAL DEFAULT 0.0,
    quality_status VARCHAR(20),
    quality_issues JSONB DEFAULT '[]',

    -- å®‰å…¨èˆ‡æ¬Šé™
    security_level VARCHAR(20) DEFAULT 'internal',
    access_permissions JSONB DEFAULT '{}',

    -- æ™‚é–“æˆ³
    created_date TIMESTAMP NOT NULL DEFAULT NOW(),
    modified_date TIMESTAMP DEFAULT NOW(),
    accessed_date TIMESTAMP,

    -- ç‰ˆæœ¬æ§åˆ¶
    version VARCHAR(20) DEFAULT '1.0',
    parent_document_id VARCHAR(50) REFERENCES documents(document_id),

    -- ä½¿ç”¨çµ±è¨ˆ
    access_count INTEGER DEFAULT 0,
    download_count INTEGER DEFAULT 0,

    -- è‡ªè¨‚æ¬„ä½
    custom_metadata JSONB DEFAULT '{}',

    -- è»Ÿåˆªé™¤
    is_deleted BOOLEAN DEFAULT FALSE,
    deleted_date TIMESTAMP,
    deleted_by VARCHAR(50)
);

-- ç´¢å¼•è¨­è¨ˆ
CREATE INDEX idx_documents_status ON documents(processing_status);
CREATE INDEX idx_documents_type ON documents(document_type);
CREATE INDEX idx_documents_category ON documents(category);
CREATE INDEX idx_documents_created ON documents(created_date);
CREATE INDEX idx_documents_quality ON documents(quality_score);
CREATE INDEX idx_documents_search ON documents USING gin(to_tsvector('english', title || ' ' || COALESCE(custom_metadata->>'description', '')));
```

#### 2. Document_Chunks è¡¨ (åˆ†å¡Šè¡¨)
```sql
CREATE TABLE document_chunks (
    chunk_id VARCHAR(50) PRIMARY KEY,
    document_id VARCHAR(50) NOT NULL REFERENCES documents(document_id) ON DELETE CASCADE,

    -- å…§å®¹è³‡è¨Š
    content TEXT NOT NULL,
    content_hash VARCHAR(64) NOT NULL,  -- SHA-256

    -- ä½ç½®è³‡è¨Š
    chunk_index INTEGER NOT NULL,
    start_page INTEGER,
    end_page INTEGER,
    start_offset INTEGER,
    end_offset INTEGER,

    -- çµ±è¨ˆè³‡è¨Š
    word_count INTEGER NOT NULL,
    sentence_count INTEGER,
    character_count INTEGER,

    -- åˆ†å¡Šé¡å‹èˆ‡å“è³ª
    semantic_type VARCHAR(50) DEFAULT 'text',  -- text, table, figure, header, footer
    chunk_quality REAL DEFAULT 0.0,
    semantic_coherence REAL,  -- èªç¾©ä¸€è‡´æ€§åˆ†æ•¸

    -- å‘é‡åµŒå…¥è³‡è¨Š
    embedding_model VARCHAR(100),
    embedding_dim INTEGER,
    embedding_status VARCHAR(20) DEFAULT 'pending',

    -- åˆ†å¡Šç­–ç•¥è³‡è¨Š
    chunking_strategy VARCHAR(50),
    chunking_config JSONB,

    -- ç·¨è¼¯æ­·å²
    edit_count INTEGER DEFAULT 0,
    last_edited_date TIMESTAMP,
    last_edited_by VARCHAR(50),
    edit_summary TEXT,

    -- æ™‚é–“æˆ³
    created_date TIMESTAMP NOT NULL DEFAULT NOW(),

    -- ç‰ˆæœ¬æ§åˆ¶
    version INTEGER DEFAULT 1,
    original_chunk_id VARCHAR(50),  -- æŒ‡å‘åŸå§‹åˆ†å¡Š

    UNIQUE(document_id, chunk_index)
);

-- ç´¢å¼•è¨­è¨ˆ
CREATE INDEX idx_chunks_document ON document_chunks(document_id);
CREATE INDEX idx_chunks_quality ON document_chunks(chunk_quality);
CREATE INDEX idx_chunks_type ON document_chunks(semantic_type);
CREATE INDEX idx_chunks_embedding ON document_chunks(embedding_status);
CREATE INDEX idx_chunks_search ON document_chunks USING gin(to_tsvector('english', content));
```

#### 3. Tags è¡¨ (æ¨™ç±¤è¡¨)
```sql
CREATE TABLE tags (
    tag_id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(200) NOT NULL,
    normalized_name VARCHAR(200) NOT NULL,  -- æ­£è¦åŒ–å¾Œçš„åç¨±

    -- åˆ†é¡è³‡è¨Š
    category VARCHAR(100),
    subcategory VARCHAR(100),
    tag_type VARCHAR(50) DEFAULT 'keyword',  -- keyword, entity, topic, category

    -- å±¤ç´šçµæ§‹
    parent_tag_id VARCHAR(50) REFERENCES tags(tag_id),
    hierarchy_level INTEGER DEFAULT 0,
    hierarchy_path VARCHAR(1000),  -- å®Œæ•´è·¯å¾‘

    -- æ¨™ç±¤å±¬æ€§
    description TEXT,
    color VARCHAR(7),  -- Hex color code
    icon VARCHAR(50),
    aliases JSONB DEFAULT '[]',  -- åŒç¾©è©åˆ—è¡¨

    -- ä½¿ç”¨çµ±è¨ˆ
    usage_count INTEGER DEFAULT 0,
    document_count INTEGER DEFAULT 0,
    chunk_count INTEGER DEFAULT 0,

    -- å“è³ªè³‡è¨Š
    confidence_score REAL DEFAULT 1.0,
    quality_score REAL DEFAULT 1.0,

    -- ç”Ÿæˆè³‡è¨Š
    source VARCHAR(50) DEFAULT 'manual',  -- manual, auto_generated, imported
    generator_model VARCHAR(100),
    generation_config JSONB,

    -- ç®¡ç†è³‡è¨Š
    created_by VARCHAR(50),
    approved_by VARCHAR(50),
    approval_status VARCHAR(20) DEFAULT 'approved',

    -- æ™‚é–“æˆ³
    created_date TIMESTAMP NOT NULL DEFAULT NOW(),
    modified_date TIMESTAMP DEFAULT NOW(),
    last_used_date TIMESTAMP,

    -- è»Ÿåˆªé™¤
    is_active BOOLEAN DEFAULT TRUE,

    UNIQUE(normalized_name, category)
);

-- ç´¢å¼•è¨­è¨ˆ
CREATE INDEX idx_tags_category ON tags(category);
CREATE INDEX idx_tags_parent ON tags(parent_tag_id);
CREATE INDEX idx_tags_usage ON tags(usage_count DESC);
CREATE INDEX idx_tags_search ON tags USING gin(to_tsvector('english', name || ' ' || COALESCE(description, '')));
CREATE INDEX idx_tags_hierarchy ON tags(hierarchy_level, hierarchy_path);
```

#### 4. Document_Tags é—œè¯è¡¨ (æ–‡æª”æ¨™ç±¤é—œä¿‚)
```sql
CREATE TABLE document_tags (
    document_id VARCHAR(50) REFERENCES documents(document_id) ON DELETE CASCADE,
    tag_id VARCHAR(50) REFERENCES tags(tag_id) ON DELETE CASCADE,

    -- æ¨™ç±¤ä¾†æºèˆ‡ä¿¡å¿ƒåº¦
    confidence REAL DEFAULT 1.0,
    source VARCHAR(50) DEFAULT 'manual',  -- manual, auto_generated, suggested

    -- æ‡‰ç”¨å±¤ç´š
    applied_to VARCHAR(20) DEFAULT 'document',  -- document, chunk
    chunk_id VARCHAR(50) REFERENCES document_chunks(chunk_id),

    -- æ¨™è¨»è³‡è¨Š
    tagged_by VARCHAR(50),
    tagged_date TIMESTAMP NOT NULL DEFAULT NOW(),

    -- é©—è­‰è³‡è¨Š
    verified BOOLEAN DEFAULT FALSE,
    verified_by VARCHAR(50),
    verified_date TIMESTAMP,

    -- æ¨™ç±¤ä½ç½® (for chunk-level tags)
    start_position INTEGER,
    end_position INTEGER,
    context TEXT,  -- æ¨™ç±¤å‘¨åœçš„ä¸Šä¸‹æ–‡

    PRIMARY KEY (document_id, tag_id, COALESCE(chunk_id, 'document'))
);

-- ç´¢å¼•è¨­è¨ˆ
CREATE INDEX idx_document_tags_doc ON document_tags(document_id);
CREATE INDEX idx_document_tags_tag ON document_tags(tag_id);
CREATE INDEX idx_document_tags_source ON document_tags(source);
CREATE INDEX idx_document_tags_confidence ON document_tags(confidence DESC);
```

#### 5. Processing_Tasks è¡¨ (è™•ç†ä»»å‹™)
```sql
CREATE TABLE processing_tasks (
    task_id VARCHAR(50) PRIMARY KEY,
    document_id VARCHAR(50) REFERENCES documents(document_id),

    -- ä»»å‹™è³‡è¨Š
    task_type VARCHAR(50) NOT NULL,  -- parse, chunk, tag, quality_assess
    task_status VARCHAR(20) DEFAULT 'pending',
    priority INTEGER DEFAULT 0,

    -- é…ç½®èˆ‡åƒæ•¸
    task_config JSONB,
    input_data JSONB,

    -- åŸ·è¡Œè³‡è¨Š
    worker_id VARCHAR(50),
    started_date TIMESTAMP,
    completed_date TIMESTAMP,
    processing_time REAL,

    -- çµæœè³‡è¨Š
    output_data JSONB,
    success BOOLEAN,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    max_retries INTEGER DEFAULT 3,

    -- é€²åº¦è¿½è¹¤
    progress_percentage INTEGER DEFAULT 0,
    current_step VARCHAR(100),
    total_steps INTEGER,

    -- ä¾è³´é—œä¿‚
    depends_on JSONB DEFAULT '[]',  -- ä¾è³´çš„å…¶ä»–ä»»å‹™

    -- æ™‚é–“æˆ³
    created_date TIMESTAMP NOT NULL DEFAULT NOW(),
    modified_date TIMESTAMP DEFAULT NOW()
);

-- ç´¢å¼•è¨­è¨ˆ
CREATE INDEX idx_tasks_document ON processing_tasks(document_id);
CREATE INDEX idx_tasks_status ON processing_tasks(task_status);
CREATE INDEX idx_tasks_type ON processing_tasks(task_type);
CREATE INDEX idx_tasks_priority ON processing_tasks(priority DESC, created_date);
CREATE INDEX idx_tasks_worker ON processing_tasks(worker_id);
```

---

## âš¡ äº‹ä»¶é©…å‹•æ¶æ§‹è¨­è¨ˆ

### äº‹ä»¶ç¸½ç·šé…ç½®

#### äº‹ä»¶å®šç¾©è¦ç¯„
```yaml
Event Schema:
  event_id: string (UUID)
  event_type: string (enum)
  source_service: string
  timestamp: datetime (ISO 8601)
  version: string
  correlation_id: string
  causation_id: string
  data: object
  metadata:
    user_id: string
    session_id: string
    ip_address: string
    user_agent: string
```

#### æ ¸å¿ƒäº‹ä»¶é¡å‹

##### æ–‡æª”ç”Ÿå‘½é€±æœŸäº‹ä»¶
```yaml
# æ–‡æª”ä¸Šå‚³äº‹ä»¶
DocumentUploadStarted:
  event_type: "document.upload.started"
  data:
    document_id: string
    filename: string
    file_size: integer
    user_id: string
    upload_session_id: string

DocumentUploadCompleted:
  event_type: "document.upload.completed"
  data:
    document_id: string
    file_path: string
    metadata: object
    upload_duration: number

DocumentUploadFailed:
  event_type: "document.upload.failed"
  data:
    document_id: string
    error_code: string
    error_message: string
    retry_count: integer

# æ–‡æª”è™•ç†äº‹ä»¶
DocumentProcessingStarted:
  event_type: "document.processing.started"
  data:
    document_id: string
    processing_config: object
    estimated_duration: number

DocumentProcessingCompleted:
  event_type: "document.processing.completed"
  data:
    document_id: string
    processing_result: object
    processing_duration: number
    quality_score: number
```

##### åˆ†å¡Šè™•ç†äº‹ä»¶
```yaml
ChunkingStarted:
  event_type: "chunking.started"
  data:
    document_id: string
    chunking_strategy: string
    chunking_config: object

ChunkingCompleted:
  event_type: "chunking.completed"
  data:
    document_id: string
    chunk_ids: array
    chunking_stats: object
    quality_metrics: object

ChunkUpdated:
  event_type: "chunk.updated"
  data:
    chunk_id: string
    document_id: string
    change_type: string  # content, boundary, metadata
    old_value: object
    new_value: object
    updated_by: string
```

##### æ¨™ç±¤è™•ç†äº‹ä»¶
```yaml
TaggingStarted:
  event_type: "tagging.started"
  data:
    document_id: string
    tagging_config: object
    tag_types: array

TagsGenerated:
  event_type: "tags.generated"
  data:
    document_id: string
    generated_tags: array
    confidence_scores: object
    generation_model: string

TagAdded:
  event_type: "tag.added"
  data:
    document_id: string
    tag_id: string
    chunk_id: string  # optional
    source: string
    confidence: number
    added_by: string
```

### äº‹ä»¶è™•ç†å™¨è¨­è¨ˆ

#### äº‹ä»¶è™•ç†å™¨åˆ—è¡¨

##### Document Service äº‹ä»¶è™•ç†å™¨
```python
@event_handler("document.upload.completed")
async def trigger_document_processing(event: Event):
    """æ–‡æª”ä¸Šå‚³å®Œæˆå¾Œè§¸ç™¼è™•ç†æµç¨‹"""
    document_id = event.data["document_id"]

    # 1. é©—è­‰æª”æ¡ˆå®Œæ•´æ€§
    await validate_document_integrity(document_id)

    # 2. ç™¼èµ·å…§å®¹è§£æä»»å‹™
    await publish_event("document.processing.started", {
        "document_id": document_id,
        "processing_config": get_default_config()
    })

@event_handler("document.processing.completed")
async def trigger_downstream_processing(event: Event):
    """æ–‡æª”è™•ç†å®Œæˆå¾Œè§¸ç™¼ä¸‹æ¸¸è™•ç†"""
    document_id = event.data["document_id"]

    # 1. è§¸ç™¼åˆ†å¡Šè™•ç†
    await publish_event("chunking.started", {
        "document_id": document_id
    })

    # 2. è§¸ç™¼å“è³ªè©•ä¼°
    await publish_event("quality.assessment.started", {
        "document_id": document_id
    })
```

##### Chunking Service äº‹ä»¶è™•ç†å™¨
```python
@event_handler("chunking.started")
async def process_document_chunking(event: Event):
    """è™•ç†æ–‡æª”åˆ†å¡Š"""
    document_id = event.data["document_id"]
    config = event.data.get("chunking_config", {})

    # 1. ç²å–æ–‡æª”å…§å®¹
    content = await get_document_content(document_id)

    # 2. åŸ·è¡Œåˆ†å¡Šè™•ç†
    chunks = await perform_chunking(content, config)

    # 3. å­˜å„²åˆ†å¡Šçµæœ
    await save_chunks(document_id, chunks)

    # 4. ç™¼å¸ƒå®Œæˆäº‹ä»¶
    await publish_event("chunking.completed", {
        "document_id": document_id,
        "chunk_ids": [c.chunk_id for c in chunks]
    })

@event_handler("chunking.completed")
async def trigger_embedding_generation(event: Event):
    """åˆ†å¡Šå®Œæˆå¾Œè§¸ç™¼å‘é‡åµŒå…¥ç”Ÿæˆ"""
    await publish_event("embedding.generation.started", {
        "document_id": event.data["document_id"],
        "chunk_ids": event.data["chunk_ids"]
    })
```

##### Tagging Service äº‹ä»¶è™•ç†å™¨
```python
@event_handler("chunking.completed")
async def generate_automatic_tags(event: Event):
    """åˆ†å¡Šå®Œæˆå¾Œè‡ªå‹•ç”Ÿæˆæ¨™ç±¤"""
    document_id = event.data["document_id"]

    # 1. ç²å–æ–‡æª”å…§å®¹èˆ‡åˆ†å¡Š
    doc_data = await get_document_with_chunks(document_id)

    # 2. ç”Ÿæˆæ¨™ç±¤
    tags = await generate_tags(doc_data)

    # 3. å­˜å„²æ¨™ç±¤
    await save_document_tags(document_id, tags)

    # 4. ç™¼å¸ƒäº‹ä»¶
    await publish_event("tags.generated", {
        "document_id": document_id,
        "generated_tags": tags
    })
```

---

## ğŸ” å®‰å…¨æ¶æ§‹è¨­è¨ˆ

### èªè­‰èˆ‡æˆæ¬Š

#### OAuth 2.0 æµç¨‹
```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant AuthService
    participant ResourceService

    User->>Frontend: ç™»å…¥è«‹æ±‚
    Frontend->>AuthService: èªè­‰è«‹æ±‚
    AuthService->>AuthService: é©—è­‰ç”¨æˆ¶èº«ä»½
    AuthService->>Frontend: è¿”å› Access Token + Refresh Token
    Frontend->>ResourceService: API è«‹æ±‚ + Access Token
    ResourceService->>AuthService: é©—è­‰ Token
    AuthService->>ResourceService: Token æœ‰æ•ˆæ€§ç¢ºèª
    ResourceService->>Frontend: API éŸ¿æ‡‰
```

#### æ¬Šé™æ§åˆ¶æ¨¡å‹ (RBAC)
```yaml
Roles:
  - admin:
      permissions:
        - documents.create
        - documents.read
        - documents.update
        - documents.delete
        - chunks.edit
        - tags.manage
        - quality.configure

  - editor:
      permissions:
        - documents.create
        - documents.read
        - documents.update
        - chunks.edit
        - tags.add
        - tags.remove

  - viewer:
      permissions:
        - documents.read
        - chunks.read
        - tags.read

  - analyst:
      permissions:
        - documents.read
        - documents.export
        - chunks.read
        - chunks.export
        - quality.view
```

### è³‡æ–™å®‰å…¨

#### æ•æ„Ÿè³‡è¨Šæª¢æ¸¬
```python
class SensitiveInfoDetector:
    """æ•æ„Ÿè³‡è¨Šè‡ªå‹•æª¢æ¸¬å™¨"""

    PATTERNS = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'(\+886|0)\d{2,3}-?\d{3,4}-?\d{4}',
        'id_number': r'\b[A-Z][12]\d{8}\b',  # å°ç£èº«åˆ†è­‰
        'credit_card': r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b'
    }

    async def scan_content(self, content: str) -> List[Dict]:
        """æƒæå…§å®¹ä¸­çš„æ•æ„Ÿè³‡è¨Š"""
        findings = []

        for info_type, pattern in self.PATTERNS.items():
            matches = re.finditer(pattern, content)
            for match in matches:
                findings.append({
                    'type': info_type,
                    'value': match.group(),
                    'position': match.span(),
                    'confidence': 0.9,
                    'action_required': True
                })

        return findings
```

#### è³‡æ–™åŠ å¯†ç­–ç•¥
```yaml
Encryption Layers:
  # å‚³è¼¸åŠ å¯†
  transport:
    protocol: TLS 1.3
    cipher_suites:
      - TLS_AES_256_GCM_SHA384
      - TLS_CHACHA20_POLY1305_SHA256

  # æ‡‰ç”¨å±¤åŠ å¯†
  application:
    algorithm: AES-256-GCM
    key_rotation: 90 days
    key_management: HashiCorp Vault

  # è³‡æ–™åº«åŠ å¯†
  database:
    transparent_encryption: enabled
    column_encryption:
      - documents.custom_metadata
      - document_chunks.content (if sensitive)
      - processing_tasks.input_data

  # æª”æ¡ˆå­˜å„²åŠ å¯†
  storage:
    server_side_encryption: AES-256
    client_side_encryption: true
    key_per_document: true
```

---

## ğŸ“Š æ€§èƒ½è¨­è¨ˆèˆ‡å„ªåŒ–

### æ€§èƒ½éœ€æ±‚æŒ‡æ¨™

#### éŸ¿æ‡‰æ™‚é–“éœ€æ±‚
| æ“ä½œ | P50 | P95 | P99 | æœ€å¤§å€¼ |
|------|-----|-----|-----|-------|
| æ–‡æª”ä¸Šå‚³ | 100ms | 300ms | 500ms | 1s |
| æ–‡æª”åˆ—è¡¨æŸ¥è©¢ | 50ms | 150ms | 300ms | 500ms |
| åˆ†å¡Šè™•ç† | 2s | 8s | 15s | 30s |
| æ¨™ç±¤ç”Ÿæˆ | 500ms | 2s | 5s | 10s |
| æœå°‹æŸ¥è©¢ | 100ms | 500ms | 1s | 2s |

#### ååé‡éœ€æ±‚
| æ“ä½œ | ç›®æ¨™ TPS | å³°å€¼ TPS | ä¸¦ç™¼æ•¸ |
|------|-----------|-----------|---------|
| API è«‹æ±‚ | 500 | 1000 | 100 |
| æ–‡æª”ä¸Šå‚³ | 50 | 100 | 20 |
| æ–‡æª”è™•ç† | 10 | 25 | 5 |

### æ€§èƒ½å„ªåŒ–ç­–ç•¥

#### 1. å¿«å–ç­–ç•¥
```python
# Redis å¿«å–å±¤ç´š
CACHE_STRATEGIES = {
    # æ–‡æª”å…ƒè³‡æ–™å¿«å–
    'document_metadata': {
        'ttl': 3600,  # 1å°æ™‚
        'key_pattern': 'doc:meta:{document_id}',
        'invalidation': ['document.updated', 'document.deleted']
    },

    # åˆ†å¡Šçµæœå¿«å–
    'chunk_results': {
        'ttl': 7200,  # 2å°æ™‚
        'key_pattern': 'doc:chunks:{document_id}',
        'invalidation': ['chunking.completed', 'chunk.updated']
    },

    # æ¨™ç±¤å»ºè­°å¿«å–
    'tag_suggestions': {
        'ttl': 1800,  # 30åˆ†é˜
        'key_pattern': 'tags:suggest:{content_hash}',
        'max_size': '100MB'
    },

    # æœå°‹çµæœå¿«å–
    'search_results': {
        'ttl': 300,   # 5åˆ†é˜
        'key_pattern': 'search:{query_hash}',
        'max_entries': 10000
    }
}
```

#### 2. è³‡æ–™åº«å„ªåŒ–
```sql
-- åˆ†å€ç­–ç•¥ (æŒ‰æ™‚é–“åˆ†å€)
CREATE TABLE documents (
    -- ... existing columns ...
) PARTITION BY RANGE (created_date);

CREATE TABLE documents_2024_q1 PARTITION OF documents
FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');

CREATE TABLE documents_2024_q2 PARTITION OF documents
FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX CONCURRENTLY idx_documents_composite
ON documents(processing_status, quality_score, created_date DESC)
WHERE is_deleted = FALSE;

-- çµ±è¨ˆè³‡è¨Šæ›´æ–°
ANALYZE documents;
ANALYZE document_chunks;
ANALYZE tags;
```

#### 3. éåŒæ­¥è™•ç†æ¶æ§‹
```python
# Celery ä»»å‹™é…ç½®
CELERY_CONFIG = {
    'broker_url': 'redis://redis:6379/0',
    'result_backend': 'redis://redis:6379/0',

    'task_routes': {
        'document.parse': {'queue': 'document_processing'},
        'document.chunk': {'queue': 'ml_processing'},
        'document.tag': {'queue': 'ml_processing'},
        'document.quality': {'queue': 'quality_assessment'},
        'notification.*': {'queue': 'notifications'}
    },

    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_max_tasks_per_child': 100
}

# å„ªå…ˆéšŠåˆ—è¨­è¨ˆ
PRIORITY_QUEUES = {
    'critical': {
        'priority': 10,
        'routing_key': 'critical.#',
        'max_workers': 5
    },
    'high': {
        'priority': 7,
        'routing_key': 'high.#',
        'max_workers': 10
    },
    'normal': {
        'priority': 5,
        'routing_key': 'normal.#',
        'max_workers': 15
    },
    'low': {
        'priority': 2,
        'routing_key': 'low.#',
        'max_workers': 5
    }
}
```

---

## ğŸ”Œ å¤–éƒ¨ç³»çµ±æ•´åˆè¦æ ¼

### æª”æ¡ˆå­˜å„²ç³»çµ± (MinIO/S3)

#### å­˜å„²ç­–ç•¥è¨­è¨ˆ
```yaml
Bucket Structure:
  # åŸå§‹æ–‡æª”å­˜å„²
  raw-documents:
    path_pattern: "/{year}/{month}/{document_id}/{filename}"
    retention: 7 years
    versioning: enabled
    encryption: AES-256
    access_policy: private

  # è™•ç†çµæœå­˜å„²
  processed-documents:
    path_pattern: "/{document_id}/processed/{version}/{filename}"
    retention: 3 years
    compression: gzip

  # ç¸®ç•¥åœ–èˆ‡é è¦½
  thumbnails:
    path_pattern: "/{document_id}/thumbnails/{size}/{format}"
    retention: 1 year
    cdn_enabled: true

  # å‚™ä»½å­˜å„²
  backups:
    path_pattern: "/backups/{date}/{document_id}.tar.gz"
    retention: 10 years
    cold_storage: true
```

#### æª”æ¡ˆæ“ä½œ API
```python
class DocumentStorageService:
    """æ–‡æª”å­˜å„²æœå‹™"""

    async def upload_document(self,
                            file_data: bytes,
                            document_id: str,
                            metadata: Dict) -> str:
        """ä¸Šå‚³æ–‡æª”åˆ°å­˜å„²"""

    async def get_document_url(self,
                             document_id: str,
                             expires_in: int = 3600) -> str:
        """ç²å–æ–‡æª”è¨ªå• URL (é ç°½å)"""

    async def delete_document(self,
                            document_id: str,
                            permanent: bool = False) -> bool:
        """åˆªé™¤æ–‡æª” (è»Ÿåˆªé™¤æˆ–æ°¸ä¹…åˆªé™¤)"""

    async def backup_document(self,
                            document_id: str,
                            backup_type: str) -> str:
        """å‚™ä»½æ–‡æª”"""
```

### å‘é‡è³‡æ–™åº«æ•´åˆ (ChromaDB)

#### å‘é‡å­˜å„²ç­–ç•¥
```python
# ChromaDB é›†åˆé…ç½®
VECTOR_COLLECTIONS = {
    'document_chunks': {
        'embedding_model': 'all-MiniLM-L6-v2',
        'dimension': 384,
        'distance_metric': 'cosine',
        'metadata_schema': {
            'document_id': 'string',
            'chunk_index': 'integer',
            'semantic_type': 'string',
            'quality_score': 'float',
            'created_date': 'datetime'
        },
        'index_config': {
            'hnsw_space': 'cosine',
            'hnsw_construction_ef': 200,
            'hnsw_m': 16
        }
    },

    'document_embeddings': {
        'embedding_model': 'text-embedding-ada-002',
        'dimension': 1536,
        'distance_metric': 'cosine',
        'use_case': 'document_similarity'
    }
}

class VectorIndexService:
    """å‘é‡ç´¢å¼•æœå‹™"""

    async def add_chunk_embeddings(self,
                                 chunks: List[DocumentChunk]) -> List[str]:
        """æ·»åŠ åˆ†å¡Šå‘é‡åµŒå…¥"""

    async def semantic_search(self,
                            query: str,
                            filters: Dict,
                            top_k: int = 10) -> List[SearchResult]:
        """èªç¾©æœå°‹"""

    async def find_similar_chunks(self,
                                chunk_id: str,
                                top_k: int = 5) -> List[SimilarChunk]:
        """æ‰¾åˆ°ç›¸ä¼¼åˆ†å¡Š"""
```

---

## ğŸ“ˆ ç›£æ§èˆ‡å¯è§€æ¸¬æ€§

### æŒ‡æ¨™æ”¶é›†ç­–ç•¥

#### æ‡‰ç”¨æŒ‡æ¨™ (Application Metrics)
```python
# Prometheus æŒ‡æ¨™å®šç¾©
from prometheus_client import Counter, Histogram, Gauge

# æ–‡æª”è™•ç†æŒ‡æ¨™
documents_processed = Counter(
    'documents_processed_total',
    'å·²è™•ç†æ–‡æª”ç¸½æ•¸',
    ['status', 'document_type', 'size_category']
)

document_processing_duration = Histogram(
    'document_processing_duration_seconds',
    'æ–‡æª”è™•ç†è€—æ™‚',
    ['processing_stage', 'document_type'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

# å“è³ªæŒ‡æ¨™
quality_score_distribution = Histogram(
    'quality_score_distribution',
    'å“è³ªåˆ†æ•¸åˆ†ä½ˆ',
    ['document_type'],
    buckets=[0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 1.0]
)

# ç³»çµ±å¥åº·æŒ‡æ¨™
active_connections = Gauge(
    'active_connections',
    'æ´»èºé€£ç·šæ•¸'
)

processing_queue_size = Gauge(
    'processing_queue_size',
    'è™•ç†éšŠåˆ—å¤§å°',
    ['queue_name']
)
```

#### æ¥­å‹™æŒ‡æ¨™ (Business Metrics)
```python
# ç”¨æˆ¶è¡Œç‚ºæŒ‡æ¨™
user_actions = Counter(
    'user_actions_total',
    'ç”¨æˆ¶æ“ä½œç¸½æ•¸',
    ['action_type', 'user_role']
)

document_lifecycle = Histogram(
    'document_lifecycle_duration',
    'æ–‡æª”ç”Ÿå‘½é€±æœŸè€—æ™‚',
    ['lifecycle_stage']
)

# åŠŸèƒ½ä½¿ç”¨æŒ‡æ¨™
feature_usage = Counter(
    'feature_usage_total',
    'åŠŸèƒ½ä½¿ç”¨æ¬¡æ•¸',
    ['feature_name', 'user_segment']
)
```

### æ—¥èªŒè¨­è¨ˆ

#### çµæ§‹åŒ–æ—¥èªŒæ ¼å¼
```json
{
  "timestamp": "2024-01-17T10:30:00.123Z",
  "level": "INFO",
  "service": "document-service",
  "logger": "document.processor",
  "message": "Document processing completed",
  "context": {
    "document_id": "doc_abc123",
    "user_id": "user_456",
    "processing_time": 5.67,
    "chunk_count": 15,
    "quality_score": 0.87
  },
  "trace_id": "trace_xyz789",
  "span_id": "span_123",
  "request_id": "req_456",
  "version": "1.0",
  "environment": "production"
}
```

#### æ—¥èªŒç­‰ç´šèˆ‡å…§å®¹
```python
# æ—¥èªŒè¨˜éŒ„ç­–ç•¥
LOGGING_CONFIG = {
    'ERROR': [
        'system_errors',
        'processing_failures',
        'security_incidents',
        'data_corruption'
    ],

    'WARN': [
        'quality_threshold_breach',
        'unusual_processing_time',
        'deprecated_api_usage',
        'resource_limit_approaching'
    ],

    'INFO': [
        'document_lifecycle_events',
        'user_actions',
        'system_state_changes',
        'performance_milestones'
    ],

    'DEBUG': [
        'detailed_processing_steps',
        'algorithm_decisions',
        'cache_hits_misses',
        'internal_service_calls'
    ]
}
```

### åˆ†æ•£å¼è¿½è¹¤

#### OpenTelemetry æ•´åˆ
```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider

# è¿½è¹¤é…ç½®
tracer = trace.get_tracer("document-ingestion-service")

@tracer.start_as_current_span("process_document")
async def process_document(document_id: str) -> ProcessingResult:
    """æ–‡æª”è™•ç†ä¸»æµç¨‹ - åˆ†æ•£å¼è¿½è¹¤"""

    with tracer.start_as_current_span("extract_text") as span:
        span.set_attributes({
            "document.id": document_id,
            "document.type": "pdf",
            "processing.stage": "text_extraction"
        })

        text_content = await extract_text_content(document_id)
        span.set_attribute("text.length", len(text_content))

    with tracer.start_as_current_span("chunk_text") as span:
        chunks = await chunk_text_content(text_content)
        span.set_attribute("chunks.count", len(chunks))

    return ProcessingResult(chunks=chunks)
```

---

## ğŸš¢ éƒ¨ç½²èˆ‡é‹ç¶­æ¶æ§‹

### Kubernetes éƒ¨ç½²é…ç½®

#### æœå‹™éƒ¨ç½²æ¸…å–®
```yaml
# Document Service éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: document-service
  labels:
    app: document-service
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: document-service
  template:
    metadata:
      labels:
        app: document-service
    spec:
      containers:
      - name: document-service
        image: kms/document-service:v1.0
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: postgres_url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: document-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: document-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### Ingress é…ç½®
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kms-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/client-max-body-size: "50m"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.kms.company.com
    secretName: kms-tls
  rules:
  - host: api.kms.company.com
    http:
      paths:
      - path: /api/v1/documents
        pathType: Prefix
        backend:
          service:
            name: document-service
            port:
              number: 8000
      - path: /api/v1/chunks
        pathType: Prefix
        backend:
          service:
            name: chunking-service
            port:
              number: 8001
```

### CI/CD Pipeline è¨­è¨ˆ

#### GitOps å·¥ä½œæµç¨‹
```yaml
# .github/workflows/deploy.yml
name: Deploy KMS Services

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run tests
      run: |
        pytest tests/ --cov=src --cov-report=xml

    - name: Code quality checks
      run: |
        black --check src/
        isort --check-only src/
        flake8 src/
        mypy src/

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Run security scan
      run: |
        bandit -r src/
        safety check

  build:
    needs: [test, security]
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Build Docker images
      run: |
        docker build -t kms/document-service:${{ github.sha }} .

    - name: Push to registry
      run: |
        echo ${{ secrets.REGISTRY_PASSWORD }} | docker login -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin
        docker push kms/document-service:${{ github.sha }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Deploy to Kubernetes
      run: |
        kubectl set image deployment/document-service document-service=kms/document-service:${{ github.sha }}
        kubectl rollout status deployment/document-service
```

---

## ğŸ›¡ï¸ å®‰å…¨æ¶æ§‹æ·±åº¦è¨­è¨ˆ

### API å®‰å…¨ç­–ç•¥

#### 1. èªè­‰èˆ‡æˆæ¬Šä¸­é–“ä»¶
```python
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """JWT Token é©—è­‰ä¸­é–“ä»¶"""
    token = credentials.credentials

    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        user_id = payload.get("sub")

        if not user_id:
            raise HTTPException(status_code=401, detail="Invalid token")

        # æª¢æŸ¥ token æ˜¯å¦åœ¨é»‘åå–®
        if await is_token_blacklisted(token):
            raise HTTPException(status_code=401, detail="Token revoked")

        return {
            "user_id": user_id,
            "roles": payload.get("roles", []),
            "permissions": payload.get("permissions", [])
        }

    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

async def require_permission(permission: str):
    """æ¬Šé™æª¢æŸ¥è£é£¾å™¨"""
    def permission_checker(user = Depends(verify_token)):
        if permission not in user["permissions"]:
            raise HTTPException(
                status_code=403,
                detail=f"Missing required permission: {permission}"
            )
        return user
    return permission_checker
```

#### 2. API é€Ÿç‡é™åˆ¶
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

# ä¸åŒç«¯é»çš„é€Ÿç‡é™åˆ¶
RATE_LIMITS = {
    "/api/v1/documents/upload": "10/minute",
    "/api/v1/documents/batch-upload": "2/hour",
    "/api/v1/search": "100/minute",
    "/api/v1/documents": "200/minute"
}

@app.post("/api/v1/documents/upload")
@limiter.limit("10/minute")
async def upload_document(request: Request, ...):
    """æ–‡æª”ä¸Šå‚³ - é™åˆ¶ 10æ¬¡/åˆ†é˜"""
    pass
```

#### 3. è¼¸å…¥é©—è­‰èˆ‡æ¸…ç†
```python
from pydantic import BaseModel, validator, Field
import bleach

class DocumentUploadRequest(BaseModel):
    """æ–‡æª”ä¸Šå‚³è«‹æ±‚é©—è­‰"""

    title: str = Field(..., min_length=1, max_length=500)
    description: Optional[str] = Field(None, max_length=2000)
    category: str = Field(..., regex="^[a-zA-Z0-9_\\u4e00-\\u9fff\\s-]+$")
    tags: List[str] = Field(default_factory=list, max_items=20)

    @validator('title')
    def sanitize_title(cls, v):
        return bleach.clean(v, tags=[], strip=True)

    @validator('description')
    def sanitize_description(cls, v):
        if v:
            return bleach.clean(v, tags=[], strip=True)
        return v

    @validator('tags')
    def validate_tags(cls, v):
        sanitized_tags = []
        for tag in v:
            clean_tag = bleach.clean(tag, tags=[], strip=True)
            if 1 <= len(clean_tag) <= 50:
                sanitized_tags.append(clean_tag)
        return sanitized_tags[:20]  # é™åˆ¶æ¨™ç±¤æ•¸é‡
```

### è³‡æ–™ä¿è­·ç­–ç•¥

#### 1. æ•æ„Ÿè³‡æ–™è™•ç†
```python
class SensitiveDataHandler:
    """æ•æ„Ÿè³‡æ–™è™•ç†å™¨"""

    PII_PATTERNS = {
        'taiwan_id': r'\b[A-Z][12]\d{8}\b',
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'(\+886|0)\d{2,3}-?\d{3,4}-?\d{4}',
        'credit_card': r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b'
    }

    async def scan_and_mask_sensitive_data(self, content: str) -> Tuple[str, List[Dict]]:
        """æƒæä¸¦é®ç½©æ•æ„Ÿè³‡æ–™"""
        masked_content = content
        findings = []

        for data_type, pattern in self.PII_PATTERNS.items():
            for match in re.finditer(pattern, content):
                # è¨˜éŒ„ç™¼ç¾çš„æ•æ„Ÿè³‡æ–™
                findings.append({
                    'type': data_type,
                    'position': match.span(),
                    'confidence': 0.9,
                    'action': 'masked'
                })

                # é®ç½©æ•æ„Ÿè³‡æ–™
                sensitive_value = match.group()
                masked_value = self._mask_value(sensitive_value, data_type)
                masked_content = masked_content.replace(sensitive_value, masked_value)

        return masked_content, findings

    def _mask_value(self, value: str, data_type: str) -> str:
        """é®ç½©æ•æ„Ÿå€¼"""
        if data_type == 'taiwan_id':
            return value[:2] + '*' * 6 + value[-2:]
        elif data_type == 'email':
            user, domain = value.split('@')
            return user[:2] + '*' * (len(user)-2) + '@' + domain
        elif data_type == 'phone':
            return value[:4] + '*' * (len(value)-6) + value[-2:]
        elif data_type == 'credit_card':
            return '*' * 12 + value[-4:]

        return '*' * len(value)
```

#### 2. å¯©è¨ˆæ—¥èªŒ
```python
class AuditLogger:
    """å¯©è¨ˆæ—¥èªŒè¨˜éŒ„å™¨"""

    async def log_user_action(self,
                            user_id: str,
                            action: str,
                            resource: str,
                            details: Dict):
        """è¨˜éŒ„ç”¨æˆ¶æ“ä½œ"""
        audit_record = {
            'timestamp': datetime.utcnow(),
            'user_id': user_id,
            'action': action,
            'resource': resource,
            'resource_id': details.get('resource_id'),
            'ip_address': details.get('ip_address'),
            'user_agent': details.get('user_agent'),
            'request_id': details.get('request_id'),
            'result': details.get('result', 'success'),
            'error_message': details.get('error_message'),
            'additional_context': details.get('context', {})
        }

        # å¯«å…¥å¯©è¨ˆè³‡æ–™åº«
        await self.audit_db.insert_audit_record(audit_record)

        # æ•æ„Ÿæ“ä½œå³æ™‚å‘Šè­¦
        if action in ['delete', 'export', 'admin_access']:
            await self.send_security_alert(audit_record)
```

---

## ğŸ“Š æ€§èƒ½åŸºæº–èˆ‡å®¹é‡è¦åŠƒ

### ç³»çµ±å®¹é‡è¨­è¨ˆ

#### è³‡æ–™å®¹é‡é ä¼°
```yaml
Data Growth Projections:
  # ç¬¬ä¸€å¹´
  year_1:
    documents: 100,000
    avg_document_size: 2MB
    total_storage: 200GB
    chunks: 2,000,000
    vector_storage: 50GB

  # ç¬¬ä¸‰å¹´
  year_3:
    documents: 1,000,000
    avg_document_size: 2MB
    total_storage: 2TB
    chunks: 20,000,000
    vector_storage: 500GB

  # ç¬¬äº”å¹´
  year_5:
    documents: 5,000,000
    avg_document_size: 2MB
    total_storage: 10TB
    chunks: 100,000,000
    vector_storage: 2.5TB
```

#### ç¡¬é«”éœ€æ±‚è¦åŠƒ
```yaml
Infrastructure Requirements:

  # é–‹ç™¼ç’°å¢ƒ
  development:
    cpu: 4 cores
    memory: 16GB
    storage: 100GB SSD
    network: 100Mbps

  # æ¸¬è©¦ç’°å¢ƒ
  testing:
    cpu: 8 cores
    memory: 32GB
    storage: 500GB SSD
    network: 1Gbps

  # ç”Ÿç”¢ç’°å¢ƒ
  production:
    web_tier:
      replicas: 3
      cpu: 2 cores each
      memory: 4GB each

    app_tier:
      replicas: 5
      cpu: 4 cores each
      memory: 8GB each

    ml_tier:
      replicas: 3
      cpu: 8 cores each (with GPU support)
      memory: 16GB each
      gpu: NVIDIA T4 (optional)

    data_tier:
      postgresql:
        cpu: 8 cores
        memory: 32GB
        storage: 2TB SSD

      redis:
        cpu: 4 cores
        memory: 16GB

      minio:
        cpu: 4 cores
        memory: 8GB
        storage: 10TB (å¯æ“´å±•)
```

### æ€§èƒ½èª¿å„ªç­–ç•¥

#### 1. è³‡æ–™åº«å„ªåŒ–
```sql
-- åˆ†å€è¡¨ç­–ç•¥
CREATE TABLE documents_2024 PARTITION OF documents
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

-- éƒ¨åˆ†ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_documents_active
ON documents(created_date DESC, quality_score)
WHERE is_deleted = FALSE AND processing_status = 'completed';

-- ç‰©åŒ–è¦–åœ–
CREATE MATERIALIZED VIEW document_statistics AS
SELECT
    document_type,
    category,
    COUNT(*) as document_count,
    AVG(quality_score) as avg_quality,
    AVG(word_count) as avg_word_count,
    DATE_TRUNC('month', created_date) as month
FROM documents
WHERE is_deleted = FALSE
GROUP BY document_type, category, DATE_TRUNC('month', created_date);

-- å®šæœŸåˆ·æ–°
CREATE OR REPLACE FUNCTION refresh_statistics()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY document_statistics;
END;
$$ LANGUAGE plpgsql;
```

#### 2. æ‡‰ç”¨å±¤å„ªåŒ–
```python
# é€£æ¥æ± é…ç½®
DATABASE_CONFIG = {
    'pool_size': 20,
    'max_overflow': 30,
    'pool_timeout': 30,
    'pool_recycle': 3600,
    'pool_pre_ping': True
}

# æ‰¹æ¬¡è™•ç†å„ªåŒ–
async def process_documents_batch(document_ids: List[str], batch_size: int = 10):
    """æ‰¹æ¬¡è™•ç†æ–‡æª” - æ§åˆ¶ä¸¦ç™¼æ•¸"""

    semaphore = asyncio.Semaphore(batch_size)

    async def process_single_document(doc_id: str):
        async with semaphore:
            return await process_document(doc_id)

    tasks = [process_single_document(doc_id) for doc_id in document_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # è™•ç†çµæœèˆ‡ç•°å¸¸
    successes = []
    failures = []

    for i, result in enumerate(results):
        if isinstance(result, Exception):
            failures.append({'doc_id': document_ids[i], 'error': str(result)})
        else:
            successes.append(result)

    return {'successes': successes, 'failures': failures}
```

---

## ğŸ“± å‰ç«¯æ¶æ§‹è¨­è¨ˆ

### React æ‡‰ç”¨æ¶æ§‹

#### ç‹€æ…‹ç®¡ç†è¨­è¨ˆ
```typescript
// Redux Store çµæ§‹
interface AppState {
  auth: {
    user: User | null;
    token: string | null;
    permissions: string[];
  };

  documents: {
    list: Document[];
    current: Document | null;
    loading: boolean;
    filters: FilterState;
    pagination: PaginationState;
  };

  chunks: {
    byDocument: Record<string, Chunk[]>;
    editing: {
      chunkId: string | null;
      originalContent: string;
      modifications: ChunkModification[];
    };
    preview: ChunkPreviewState;
  };

  tags: {
    all: Tag[];
    suggestions: TagSuggestion[];
    hierarchy: TagHierarchy;
    loading: boolean;
  };

  quality: {
    assessments: QualityAssessment[];
    trends: QualityTrend[];
    anomalies: QualityAnomaly[];
  };

  ui: {
    sidebar: boolean;
    theme: 'light' | 'dark';
    notifications: Notification[];
    modals: ModalState;
  };
}
```

#### çµ„ä»¶æ¶æ§‹è¨­è¨ˆ
```typescript
// çµ„ä»¶å±¤æ¬¡çµæ§‹
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ common/              # é€šç”¨çµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ Button/
â”‚   â”‚   â”œâ”€â”€ Modal/
â”‚   â”‚   â”œâ”€â”€ Table/
â”‚   â”‚   â””â”€â”€ Form/
â”‚   â”œâ”€â”€ document/            # æ–‡æª”ç›¸é—œçµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ DocumentUpload/
â”‚   â”‚   â”œâ”€â”€ DocumentList/
â”‚   â”‚   â”œâ”€â”€ DocumentPreview/
â”‚   â”‚   â””â”€â”€ DocumentEditor/
â”‚   â”œâ”€â”€ chunk/               # åˆ†å¡Šç›¸é—œçµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ ChunkViewer/
â”‚   â”‚   â”œâ”€â”€ ChunkEditor/
â”‚   â”‚   â”œâ”€â”€ ChunkBoundary/
â”‚   â”‚   â””â”€â”€ ChunkStatistics/
â”‚   â”œâ”€â”€ tag/                 # æ¨™ç±¤ç›¸é—œçµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ TagManager/
â”‚   â”‚   â”œâ”€â”€ TagInput/
â”‚   â”‚   â”œâ”€â”€ TagHierarchy/
â”‚   â”‚   â””â”€â”€ TagSuggestions/
â”‚   â””â”€â”€ quality/             # å“è³ªç›¸é—œçµ„ä»¶
â”‚       â”œâ”€â”€ QualityDashboard/
â”‚       â”œâ”€â”€ QualityMetrics/
â”‚       â””â”€â”€ QualityTrends/
â”œâ”€â”€ hooks/                   # è‡ªè¨‚ Hooks
â”‚   â”œâ”€â”€ useDocuments.ts
â”‚   â”œâ”€â”€ useChunks.ts
â”‚   â”œâ”€â”€ useTags.ts
â”‚   â””â”€â”€ useQuality.ts
â”œâ”€â”€ services/                # API æœå‹™å±¤
â”‚   â”œâ”€â”€ documentService.ts
â”‚   â”œâ”€â”€ chunkService.ts
â”‚   â”œâ”€â”€ tagService.ts
â”‚   â””â”€â”€ qualityService.ts
â””â”€â”€ utils/                   # å·¥å…·å‡½æ•¸
    â”œâ”€â”€ apiClient.ts
    â”œâ”€â”€ validators.ts
    â”œâ”€â”€ formatters.ts
    â””â”€â”€ constants.ts
```

---

## ğŸ§ª API æ¸¬è©¦ç­–ç•¥

### è‡ªå‹•åŒ–æ¸¬è©¦è¨­è¨ˆ

#### 1. å–®å…ƒæ¸¬è©¦
```python
# test_document_service.py
import pytest
from fastapi.testclient import TestClient
from unittest.mock import AsyncMock

@pytest.fixture
def client():
    return TestClient(app)

@pytest.fixture
def mock_storage_service():
    return AsyncMock()

class TestDocumentUpload:
    """æ–‡æª”ä¸Šå‚³åŠŸèƒ½æ¸¬è©¦"""

    def test_upload_valid_pdf(self, client):
        """æ¸¬è©¦æœ‰æ•ˆ PDF ä¸Šå‚³"""
        with open("test_files/sample.pdf", "rb") as f:
            response = client.post(
                "/api/v1/documents/upload",
                files={"file": ("sample.pdf", f, "application/pdf")},
                data={"metadata": json.dumps({"title": "æ¸¬è©¦æ–‡æª”"})}
            )

        assert response.status_code == 201
        data = response.json()["data"]
        assert data["filename"] == "sample.pdf"
        assert data["format"] == "pdf"
        assert "document_id" in data

    def test_upload_invalid_format(self, client):
        """æ¸¬è©¦ç„¡æ•ˆæ ¼å¼æª”æ¡ˆä¸Šå‚³"""
        with open("test_files/image.jpg", "rb") as f:
            response = client.post(
                "/api/v1/documents/upload",
                files={"file": ("image.jpg", f, "image/jpeg")}
            )

        assert response.status_code == 400
        error = response.json()["error"]
        assert error["code"] == "INVALID_FILE_FORMAT"

    def test_upload_file_too_large(self, client):
        """æ¸¬è©¦æª”æ¡ˆéå¤§è™•ç†"""
        # æ¨¡æ“¬å¤§æª”æ¡ˆä¸Šå‚³
        large_content = b"x" * (51 * 1024 * 1024)  # 51MB

        response = client.post(
            "/api/v1/documents/upload",
            files={"file": ("large.pdf", large_content, "application/pdf")}
        )

        assert response.status_code == 413
        assert "æª”æ¡ˆéå¤§" in response.json()["error"]["message"]
```

#### 2. æ•´åˆæ¸¬è©¦
```python
class TestDocumentProcessingFlow:
    """æ–‡æª”è™•ç†æµç¨‹æ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_complete_processing_flow(self):
        """æ¸¬è©¦å®Œæ•´è™•ç†æµç¨‹"""

        # 1. ä¸Šå‚³æ–‡æª”
        upload_result = await upload_test_document()
        document_id = upload_result["document_id"]

        # 2. ç­‰å¾…è™•ç†å®Œæˆ
        await wait_for_processing_completion(document_id)

        # 3. é©—è­‰åˆ†å¡Šçµæœ
        chunks = await get_document_chunks(document_id)
        assert len(chunks) > 0
        assert all(chunk["word_count"] > 0 for chunk in chunks)

        # 4. é©—è­‰æ¨™ç±¤ç”Ÿæˆ
        tags = await get_document_tags(document_id)
        assert len(tags) > 0

        # 5. é©—è­‰å“è³ªè©•ä¼°
        quality = await get_quality_assessment(document_id)
        assert quality["overall_score"] > 0
```

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0
**å»ºç«‹æ—¥æœŸ**: 2024-01-17
**è² è²¬äºº**: ç³»çµ±æ¶æ§‹å¸«
**å¯©æ ¸ç‹€æ…‹**: æŠ€è¡“å¯©æ ¸ä¸­

> ğŸš€ **æº–å‚™å°±ç·’**: åŸºæ–¼æ­¤æ¶æ§‹å¯é–‹å§‹è©³ç´°å¯¦ä½œèˆ‡éƒ¨ç½²è¦åŠƒ!