{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è§£ç­” 1: åŸºç¤æ–‡æª”è™•ç†å¯¦ä½œ\n",
    "## Solution 1: Basic Document Processing Implementation\n",
    "\n",
    "> **èªªæ˜**: ç·´ç¿’ 1 çš„å®Œæ•´å¯¦ä½œè§£ç­”èˆ‡è©³ç´°è¨»è§£\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ è§£ç­”æ¦‚è¦½\n",
    "\n",
    "æœ¬è§£ç­”æä¾›äº†å®Œæ•´çš„å¯¦ä½œä»£ç¢¼ï¼ŒåŒ…å«ï¼š\n",
    "1. **è©³ç´°çš„å¯¦ä½œé‚è¼¯** - æ¯å€‹æ–¹æ³•çš„å®Œæ•´å¯¦ç¾\n",
    "2. **ç¨‹å¼ç¢¼è¨»è§£** - è§£é‡‹é—œéµè¨­è¨ˆæ±ºç­–\n",
    "3. **æœ€ä½³å¯¦å‹™** - éŒ¯èª¤è™•ç†å’Œé‚Šç•Œæ¢ä»¶\n",
    "4. **æ¸¬è©¦èˆ‡é©—è­‰** - å®Œæ•´çš„æ¸¬è©¦æµç¨‹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# è¨­å®šå°ˆæ¡ˆè·¯å¾‘\n",
    "PROJECT_ROOT = Path('/home/os-sunnie.gd.weng/python_workstation/side-project/RAG/data_governance/kms_governance')\n",
    "SOLUTION_DIR = PROJECT_ROOT / 'notebooks' / '01_document_ingestion' / 'solutions'\n",
    "EXERCISE_DIR = PROJECT_ROOT / 'notebooks' / '01_document_ingestion' / 'exercises'\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ è³‡æ–™çµæ§‹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleMetadata:\n",
    "    \"\"\"ç°¡åŒ–çš„æ–‡æª”å…ƒè³‡æ–™\"\"\"\n",
    "    doc_id: str\n",
    "    filename: str\n",
    "    file_size: int\n",
    "    created_time: datetime\n",
    "    doc_type: str\n",
    "    word_count: int\n",
    "    language: str = 'en'\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"è½‰æ›ç‚ºå­—å…¸æ ¼å¼\"\"\"\n",
    "        return {\n",
    "            'doc_id': self.doc_id,\n",
    "            'filename': self.filename,\n",
    "            'file_size': self.file_size,\n",
    "            'created_time': self.created_time.isoformat(),\n",
    "            'doc_type': self.doc_type,\n",
    "            'word_count': self.word_count,\n",
    "            'language': self.language\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class SimpleChunk:\n",
    "    \"\"\"ç°¡åŒ–çš„åˆ†å¡Šè³‡æ–™\"\"\"\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    content: str\n",
    "    chunk_index: int\n",
    "    word_count: int\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"è½‰æ›ç‚ºå­—å…¸æ ¼å¼\"\"\"\n",
    "        return {\n",
    "            'chunk_id': self.chunk_id,\n",
    "            'doc_id': self.doc_id,\n",
    "            'content': self.content,\n",
    "            'chunk_index': self.chunk_index,\n",
    "            'word_count': self.word_count\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class SimpleQuality:\n",
    "    \"\"\"ç°¡åŒ–çš„å“è³ªæŒ‡æ¨™\"\"\"\n",
    "    readability: float\n",
    "    completeness: float\n",
    "    overall: float\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"è½‰æ›ç‚ºå­—å…¸æ ¼å¼\"\"\"\n",
    "        return {\n",
    "            'readability': self.readability,\n",
    "            'completeness': self.completeness,\n",
    "            'overall': self.overall\n",
    "        }\n",
    "\n",
    "print(\"âœ… è³‡æ–™çµæ§‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è§£ç­” 1: SimpleMetadataExtractor å¯¦ä½œ\n",
    "\n",
    "### è¨­è¨ˆè¦é»\n",
    "1. **å”¯ä¸€ ID ç”Ÿæˆ**: ä½¿ç”¨æª”æ¡ˆè·¯å¾‘ + å»ºç«‹æ™‚é–“çš„ MD5 é›œæ¹Š\n",
    "2. **æ–‡æª”é¡å‹åˆ†é¡**: åŸºæ–¼å…§å®¹é—œéµå­—çš„å•Ÿç™¼å¼åˆ†é¡\n",
    "3. **éŒ¯èª¤è™•ç†**: å„ªé›…è™•ç†æª”æ¡ˆè®€å–ç•°å¸¸\n",
    "4. **å¯æ“´å±•æ€§**: æ˜“æ–¼æ·»åŠ æ–°çš„åˆ†é¡è¦å‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMetadataExtractor:\n",
    "    \"\"\"ç°¡å–®çš„å…ƒè³‡æ–™æå–å™¨ - å®Œæ•´å¯¦ä½œ\"\"\"\n",
    "    \n",
    "    def extract_from_file(self, file_path: str, content: str) -> SimpleMetadata:\n",
    "        \"\"\"\n",
    "        å¾æ–‡ä»¶å’Œå…§å®¹æå–å…ƒè³‡æ–™\n",
    "        \n",
    "        Args:\n",
    "            file_path: æª”æ¡ˆè·¯å¾‘\n",
    "            content: æª”æ¡ˆå…§å®¹\n",
    "            \n",
    "        Returns:\n",
    "            SimpleMetadata: æå–çš„å…ƒè³‡æ–™\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        try:\n",
    "            # ç²å–æª”æ¡ˆçµ±è¨ˆè³‡è¨Š\n",
    "            file_stats = file_path.stat()\n",
    "            \n",
    "            # ç”Ÿæˆå”¯ä¸€æ–‡æª” ID (æª”æ¡ˆè·¯å¾‘ + å»ºç«‹æ™‚é–“çš„ MD5)\n",
    "            id_string = f\"{file_path.name}_{file_stats.st_ctime}\"\n",
    "            doc_id = hashlib.md5(id_string.encode()).hexdigest()[:12]\n",
    "            \n",
    "            # è¨ˆç®—å­—æ•¸\n",
    "            word_count = len(content.split()) if content.strip() else 0\n",
    "            \n",
    "            # åˆ†é¡æ–‡æª”é¡å‹\n",
    "            doc_type = self._classify_document_type(str(file_path), content)\n",
    "            \n",
    "            return SimpleMetadata(\n",
    "                doc_id=doc_id,\n",
    "                filename=file_path.name,\n",
    "                file_size=file_stats.st_size,\n",
    "                created_time=datetime.fromtimestamp(file_stats.st_ctime),\n",
    "                doc_type=doc_type,\n",
    "                word_count=word_count,\n",
    "                language='en'  # ç°¡åŒ–å¯¦ä½œï¼Œå›ºå®šç‚ºè‹±æ–‡\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  å…ƒè³‡æ–™æå–å¤±æ•—: {e}\")\n",
    "            # å›å‚³æœ€å°åŒ–çš„å…ƒè³‡æ–™\n",
    "            return SimpleMetadata(\n",
    "                doc_id=\"unknown\",\n",
    "                filename=file_path.name,\n",
    "                file_size=0,\n",
    "                created_time=datetime.now(),\n",
    "                doc_type=\"unknown\",\n",
    "                word_count=0\n",
    "            )\n",
    "    \n",
    "    def _classify_document_type(self, file_path: str, content: str) -> str:\n",
    "        \"\"\"\n",
    "        æ ¹æ“šè·¯å¾‘å’Œå…§å®¹åˆ†é¡æ–‡æª”é¡å‹\n",
    "        \n",
    "        åˆ†é¡é‚è¼¯:\n",
    "        1. å„ªå…ˆæª¢æŸ¥æª”æ¡ˆè·¯å¾‘ä¸­çš„é—œéµå­—\n",
    "        2. ç„¶å¾Œæª¢æŸ¥å…§å®¹ä¸­çš„çµæ§‹é—œéµå­—\n",
    "        3. æœ€å¾Œæ ¹æ“šå…§å®¹ç‰¹å¾µé€²è¡Œå•Ÿç™¼å¼åˆ¤æ–·\n",
    "        \"\"\"\n",
    "        file_path_lower = file_path.lower()\n",
    "        content_lower = content.lower() if content else \"\"\n",
    "        \n",
    "        # 1. åŸºæ–¼æª”æ¡ˆè·¯å¾‘çš„åˆ†é¡\n",
    "        path_indicators = {\n",
    "            'paper': ['paper', 'arxiv', 'research', 'journal', 'conference'],\n",
    "            'report': ['report', 'quarterly', 'annual', 'summary', 'analysis'],\n",
    "            'policy': ['policy', 'guideline', 'standard', 'procedure']\n",
    "        }\n",
    "        \n",
    "        for doc_type, keywords in path_indicators.items():\n",
    "            if any(keyword in file_path_lower for keyword in keywords):\n",
    "                return doc_type\n",
    "        \n",
    "        # 2. åŸºæ–¼å…§å®¹çµæ§‹çš„åˆ†é¡\n",
    "        structure_indicators = {\n",
    "            'paper': ['abstract', 'introduction', 'methodology', 'conclusion', \n",
    "                     'references', 'related work', 'experiments'],\n",
    "            'report': ['executive summary', 'financial performance', 'quarterly',\n",
    "                      'recommendations', 'market analysis', 'revenue'],\n",
    "            'policy': ['policy', 'guidelines', 'procedures', 'compliance',\n",
    "                      'regulations', 'standards']\n",
    "        }\n",
    "        \n",
    "        # è¨ˆç®—æ¯ç¨®é¡å‹çš„åŒ¹é…åˆ†æ•¸\n",
    "        type_scores = {}\n",
    "        for doc_type, keywords in structure_indicators.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in content_lower)\n",
    "            type_scores[doc_type] = score\n",
    "        \n",
    "        # å›å‚³æœ€é«˜åˆ†æ•¸çš„é¡å‹\n",
    "        if type_scores and max(type_scores.values()) > 0:\n",
    "            return max(type_scores, key=type_scores.get)\n",
    "        \n",
    "        # 3. é è¨­åˆ†é¡\n",
    "        return 'document'\n",
    "\n",
    "print(\"âœ… SimpleMetadataExtractor å¯¦ä½œå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è§£ç­” 2: BasicChunker å¯¦ä½œ\n",
    "\n",
    "### è¨­è¨ˆè¦é»\n",
    "1. **å½ˆæ€§åˆ†å¡Šç­–ç•¥**: æ”¯æ´æ®µè½å’Œå¥å­å…©ç¨®åˆ†å‰²æ–¹å¼\n",
    "2. **å¤§å°æ§åˆ¶**: åš´æ ¼æ§åˆ¶åˆ†å¡Šçš„æœ€å°å’Œæœ€å¤§å­—æ•¸\n",
    "3. **å…§å®¹éæ¿¾**: è‡ªå‹•éæ¿¾ç©ºç™½å’ŒéçŸ­çš„åˆ†å¡Š\n",
    "4. **ID ç®¡ç†**: ç¢ºä¿åˆ†å¡Š ID çš„å”¯ä¸€æ€§å’Œå¯è¿½æº¯æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicChunker:\n",
    "    \"\"\"åŸºç¤æ–‡æª”åˆ†å¡Šå™¨ - å®Œæ•´å¯¦ä½œ\"\"\"\n",
    "    \n",
    "    def __init__(self, min_words: int = 10, max_words: int = 300):\n",
    "        self.min_words = min_words\n",
    "        self.max_words = max_words\n",
    "    \n",
    "    def chunk_by_paragraph(self, content: str, doc_id: str) -> List[SimpleChunk]:\n",
    "        \"\"\"\n",
    "        æŒ‰æ®µè½åˆ†å¡Š\n",
    "        \n",
    "        æ¼”ç®—æ³•æ­¥é©Ÿ:\n",
    "        1. ä»¥é›™æ›è¡Œç¬¦åˆ†å‰²æ®µè½\n",
    "        2. æ¸…ç†å’Œéæ¿¾æ®µè½\n",
    "        3. é©—è­‰åˆ†å¡Šå¤§å°\n",
    "        4. ç”Ÿæˆåˆ†å¡Šç‰©ä»¶\n",
    "        \"\"\"\n",
    "        if not content.strip():\n",
    "            return []\n",
    "        \n",
    "        # 1. æŒ‰æ®µè½åˆ†å‰²\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # 2. æ¸…ç†æ®µè½ä¸¦éæ¿¾ç©ºç™½\n",
    "        cleaned_paragraphs = []\n",
    "        for para in paragraphs:\n",
    "            cleaned = para.strip()\n",
    "            if cleaned:  # éæ¿¾ç©ºæ®µè½\n",
    "                cleaned_paragraphs.append(cleaned)\n",
    "        \n",
    "        # 3. ç”Ÿæˆç¬¦åˆå¤§å°è¦æ±‚çš„åˆ†å¡Š\n",
    "        chunks = []\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for para in cleaned_paragraphs:\n",
    "            # æª¢æŸ¥æ®µè½æ˜¯å¦ç¬¦åˆå¤§å°è¦æ±‚\n",
    "            if self._is_valid_chunk(para):\n",
    "                chunk = SimpleChunk(\n",
    "                    chunk_id=f\"{doc_id}_chunk_{chunk_index}\",\n",
    "                    doc_id=doc_id,\n",
    "                    content=para,\n",
    "                    chunk_index=chunk_index,\n",
    "                    word_count=len(para.split())\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                chunk_index += 1\n",
    "            \n",
    "            elif len(para.split()) > self.max_words:\n",
    "                # å¦‚æœæ®µè½å¤ªé•·ï¼Œé€²ä¸€æ­¥åˆ†å‰²\n",
    "                sub_chunks = self._split_long_paragraph(para, doc_id, chunk_index)\n",
    "                chunks.extend(sub_chunks)\n",
    "                chunk_index += len(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_sentences(self, content: str, doc_id: str, sentences_per_chunk: int = 3) -> List[SimpleChunk]:\n",
    "        \"\"\"\n",
    "        æŒ‰å¥å­åˆ†å¡Š\n",
    "        \n",
    "        æ¼”ç®—æ³•æ­¥é©Ÿ:\n",
    "        1. ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²å¥å­\n",
    "        2. æ¸…ç†å¥å­ä¸¦éæ¿¾ç©ºç™½\n",
    "        3. å°‡å¤šå€‹å¥å­çµ„åˆæˆåˆ†å¡Š\n",
    "        4. é©—è­‰åˆ†å¡Šå¤§å°ä¸¦èª¿æ•´\n",
    "        \"\"\"\n",
    "        if not content.strip():\n",
    "            return []\n",
    "        \n",
    "        # 1. åˆ†å‰²å¥å­ï¼ˆåŸºæ–¼æ¨™é»ç¬¦è™Ÿï¼‰\n",
    "        sentence_pattern = r'[.!?]+\\s+'\n",
    "        sentences = re.split(sentence_pattern, content)\n",
    "        \n",
    "        # 2. æ¸…ç†å¥å­ä¸¦éæ¿¾ç©ºç™½\n",
    "        cleaned_sentences = []\n",
    "        for sent in sentences:\n",
    "            cleaned = sent.strip()\n",
    "            if cleaned and len(cleaned.split()) >= 3:  # è‡³å°‘3å€‹å­—\n",
    "                cleaned_sentences.append(cleaned)\n",
    "        \n",
    "        # 3. çµ„åˆå¥å­æˆåˆ†å¡Š\n",
    "        chunks = []\n",
    "        chunk_index = 0\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(cleaned_sentences):\n",
    "            # æ”¶é›†æŒ‡å®šæ•¸é‡çš„å¥å­\n",
    "            chunk_sentences = []\n",
    "            words_count = 0\n",
    "            \n",
    "            # å‹•æ…‹èª¿æ•´å¥å­æ•¸é‡ä»¥ç¬¦åˆå­—æ•¸é™åˆ¶\n",
    "            for j in range(sentences_per_chunk):\n",
    "                if i + j >= len(cleaned_sentences):\n",
    "                    break\n",
    "                \n",
    "                sentence = cleaned_sentences[i + j]\n",
    "                sentence_words = len(sentence.split())\n",
    "                \n",
    "                # æª¢æŸ¥æ˜¯å¦è¶…éæœ€å¤§å­—æ•¸\n",
    "                if words_count + sentence_words > self.max_words and chunk_sentences:\n",
    "                    break\n",
    "                \n",
    "                chunk_sentences.append(sentence)\n",
    "                words_count += sentence_words\n",
    "                \n",
    "                # å¦‚æœå·²é”æœ€å°å­—æ•¸è¦æ±‚ï¼Œå¯ä»¥çµæŸé€™å€‹åˆ†å¡Š\n",
    "                if words_count >= self.min_words:\n",
    "                    break\n",
    "            \n",
    "            if chunk_sentences:\n",
    "                chunk_content = ' '.join(chunk_sentences)\n",
    "                \n",
    "                if self._is_valid_chunk(chunk_content):\n",
    "                    chunk = SimpleChunk(\n",
    "                        chunk_id=f\"{doc_id}_chunk_{chunk_index}\",\n",
    "                        doc_id=doc_id,\n",
    "                        content=chunk_content,\n",
    "                        chunk_index=chunk_index,\n",
    "                        word_count=words_count\n",
    "                    )\n",
    "                    chunks.append(chunk)\n",
    "                    chunk_index += 1\n",
    "                \n",
    "                # ç§»å‹•åˆ°ä¸‹ä¸€çµ„å¥å­\n",
    "                i += len(chunk_sentences)\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _is_valid_chunk(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        æª¢æŸ¥åˆ†å¡Šæ˜¯å¦ç¬¦åˆå¤§å°è¦æ±‚\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return False\n",
    "        \n",
    "        word_count = len(text.split())\n",
    "        return self.min_words <= word_count <= self.max_words\n",
    "    \n",
    "    def _split_long_paragraph(self, paragraph: str, doc_id: str, start_index: int) -> List[SimpleChunk]:\n",
    "        \"\"\"\n",
    "        åˆ†å‰²éé•·çš„æ®µè½\n",
    "        \n",
    "        ç­–ç•¥: å˜—è©¦åœ¨å¥å­é‚Šç•Œåˆ†å‰²ï¼Œé¿å…ç ´å£èªç¾©å®Œæ•´æ€§\n",
    "        \"\"\"\n",
    "        # å…ˆå˜—è©¦æŒ‰å¥å­åˆ†å‰²\n",
    "        temp_chunker = BasicChunker(self.min_words, self.max_words)\n",
    "        sentence_chunks = temp_chunker.chunk_by_sentences(paragraph, doc_id, 2)\n",
    "        \n",
    "        # é‡æ–°ç·¨è™Ÿ\n",
    "        for i, chunk in enumerate(sentence_chunks):\n",
    "            chunk.chunk_id = f\"{doc_id}_chunk_{start_index + i}\"\n",
    "            chunk.chunk_index = start_index + i\n",
    "        \n",
    "        return sentence_chunks\n",
    "\n",
    "print(\"âœ… BasicChunker å¯¦ä½œå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è§£ç­” 3: QualityAssessor å¯¦ä½œ\n",
    "\n",
    "### è¨­è¨ˆè¦é»\n",
    "1. **å¯è®€æ€§è©•ä¼°**: åŸºæ–¼å¹³å‡å¥å­é•·åº¦çš„å•Ÿç™¼å¼ç®—æ³•\n",
    "2. **å®Œæ•´æ€§è©•ä¼°**: è€ƒæ…®çµæ§‹å®Œæ•´æ€§å’Œåˆ†å¡Šåˆ†ä½ˆå‡å‹»æ€§\n",
    "3. **ç¶œåˆè©•åˆ†**: åŠ æ¬Šå¹³å‡å¤šå€‹ç¶­åº¦çš„åˆ†æ•¸\n",
    "4. **ç©©å®šæ€§**: ç¢ºä¿é‚Šç•Œæ¢ä»¶çš„è™•ç†ç©©å®šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityAssessor:\n",
    "    \"\"\"ç°¡å–®çš„å“è³ªè©•ä¼°å™¨ - å®Œæ•´å¯¦ä½œ\"\"\"\n",
    "    \n",
    "    def assess_quality(self, content: str, chunks: List[SimpleChunk]) -> SimpleQuality:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ–‡æª”å“è³ª\n",
    "        \n",
    "        è©•ä¼°ç¶­åº¦:\n",
    "        1. å¯è®€æ€§ (40%) - åŸºæ–¼å¥å­é•·åº¦åˆ†ä½ˆ\n",
    "        2. å®Œæ•´æ€§ (60%) - åŸºæ–¼çµæ§‹å’Œåˆ†å¡Šå“è³ª\n",
    "        \"\"\"\n",
    "        if not content.strip():\n",
    "            return SimpleQuality(0.0, 0.0, 0.0)\n",
    "        \n",
    "        # è¨ˆç®—å„ç¶­åº¦åˆ†æ•¸\n",
    "        readability = self._calculate_readability(content)\n",
    "        completeness = self._calculate_completeness(content, chunks)\n",
    "        \n",
    "        # åŠ æ¬Šå¹³å‡è¨ˆç®—ç¶œåˆåˆ†æ•¸\n",
    "        weights = {'readability': 0.4, 'completeness': 0.6}\n",
    "        overall = (\n",
    "            weights['readability'] * readability +\n",
    "            weights['completeness'] * completeness\n",
    "        )\n",
    "        \n",
    "        return SimpleQuality(\n",
    "            readability=readability,\n",
    "            completeness=completeness,\n",
    "            overall=overall\n",
    "        )\n",
    "    \n",
    "    def _calculate_readability(self, content: str) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å¯è®€æ€§åˆ†æ•¸\n",
    "        \n",
    "        ç®—æ³•åŸºç¤:\n",
    "        - ç†æƒ³å¥å­é•·åº¦: 15-20 å­—\n",
    "        - åˆ†æ•¸ = 1.0 - |å¹³å‡é•·åº¦ - ç†æƒ³é•·åº¦| / èª¿ç¯€å› å­\n",
    "        - è€ƒæ…®å¥å­é•·åº¦è®Šç•°æ€§\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # åˆ†å‰²å¥å­\n",
    "            sentences = re.split(r'[.!?]+', content)\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "            \n",
    "            if not sentences:\n",
    "                return 0.0\n",
    "            \n",
    "            # è¨ˆç®—å¥å­é•·åº¦çµ±è¨ˆ\n",
    "            sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "            avg_length = np.mean(sentence_lengths)\n",
    "            std_length = np.std(sentence_lengths)\n",
    "            \n",
    "            # ç†æƒ³é•·åº¦ç¯„åœ (15-20 å­—)\n",
    "            ideal_length = 17.5\n",
    "            length_deviation = abs(avg_length - ideal_length)\n",
    "            \n",
    "            # åŸºç¤å¯è®€æ€§åˆ†æ•¸\n",
    "            base_score = max(0.0, 1.0 - length_deviation / 25.0)\n",
    "            \n",
    "            # è®Šç•°æ€§æ‡²ç½° (éå¤§çš„è®Šç•°æ€§é™ä½å¯è®€æ€§)\n",
    "            if avg_length > 0:\n",
    "                cv = std_length / avg_length  # è®Šç•°ä¿‚æ•¸\n",
    "                variability_penalty = min(cv / 2.0, 0.3)  # æœ€å¤§æ‡²ç½°30%\n",
    "            else:\n",
    "                variability_penalty = 0.0\n",
    "            \n",
    "            final_score = max(0.0, base_score - variability_penalty)\n",
    "            return min(final_score, 1.0)\n",
    "            \n",
    "        except Exception:\n",
    "            return 0.5  # é è¨­ä¸­ç­‰å¯è®€æ€§\n",
    "    \n",
    "    def _calculate_completeness(self, content: str, chunks: List[SimpleChunk]) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸\n",
    "        \n",
    "        è©•ä¼°å› ç´ :\n",
    "        1. çµæ§‹å®Œæ•´æ€§ (40%) - æ˜¯å¦åŒ…å«æ¨™æº–æ–‡æª”çµæ§‹\n",
    "        2. åˆ†å¡Šå“è³ª (35%) - åˆ†å¡Šå¤§å°åˆ†ä½ˆçš„å‡å‹»æ€§\n",
    "        3. å…§å®¹é•·åº¦é©ä¸­æ€§ (25%) - å…§å®¹é•·åº¦æ˜¯å¦åˆç†\n",
    "        \"\"\"\n",
    "        # 1. çµæ§‹å®Œæ•´æ€§è©•ä¼°\n",
    "        structure_score = self._assess_structure_quality(content)\n",
    "        \n",
    "        # 2. åˆ†å¡Šå“è³ªè©•ä¼°\n",
    "        chunk_quality_score = self._assess_chunk_quality(chunks)\n",
    "        \n",
    "        # 3. å…§å®¹é•·åº¦è©•ä¼°\n",
    "        length_score = self._assess_content_length(content)\n",
    "        \n",
    "        # åŠ æ¬Šå¹³å‡\n",
    "        completeness = (\n",
    "            0.4 * structure_score +\n",
    "            0.35 * chunk_quality_score +\n",
    "            0.25 * length_score\n",
    "        )\n",
    "        \n",
    "        return min(completeness, 1.0)\n",
    "    \n",
    "    def _assess_structure_quality(self, content: str) -> float:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ–‡æª”çµæ§‹å“è³ª\n",
    "        \"\"\"\n",
    "        structure_indicators = {\n",
    "            # å­¸è¡“è«–æ–‡çµæ§‹\n",
    "            'academic': ['abstract', 'introduction', 'method', 'conclusion', \n",
    "                        'references', 'background', 'related work'],\n",
    "            # å ±å‘Šçµæ§‹\n",
    "            'business': ['summary', 'overview', 'analysis', 'recommendations', \n",
    "                        'performance', 'results'],\n",
    "            # åŸºæœ¬çµæ§‹\n",
    "            'basic': ['title', 'introduction', 'conclusion', 'summary']\n",
    "        }\n",
    "        \n",
    "        content_lower = content.lower()\n",
    "        max_score = 0.0\n",
    "        \n",
    "        for struct_type, keywords in structure_indicators.items():\n",
    "            matches = sum(1 for keyword in keywords if keyword in content_lower)\n",
    "            score = min(matches / len(keywords), 1.0)\n",
    "            max_score = max(max_score, score)\n",
    "        \n",
    "        # é¡å¤–åŠ åˆ†ï¼šåŒ…å«ç·¨è™Ÿçµæ§‹ (1. 2. 3.)\n",
    "        numbered_sections = len(re.findall(r'\\n\\s*\\d+\\.\\s+', content))\n",
    "        if numbered_sections >= 2:\n",
    "            max_score += 0.2\n",
    "        \n",
    "        return min(max_score, 1.0)\n",
    "    \n",
    "    def _assess_chunk_quality(self, chunks: List[SimpleChunk]) -> float:\n",
    "        \"\"\"\n",
    "        è©•ä¼°åˆ†å¡Šå“è³ª\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return 0.0\n",
    "        \n",
    "        # åˆ†å¡Šæ•¸é‡åˆç†æ€§\n",
    "        chunk_count = len(chunks)\n",
    "        if chunk_count == 1:\n",
    "            count_score = 0.6  # å–®ä¸€åˆ†å¡Šæ‰£åˆ†\n",
    "        elif 2 <= chunk_count <= 10:\n",
    "            count_score = 1.0  # ç†æƒ³ç¯„åœ\n",
    "        elif 11 <= chunk_count <= 20:\n",
    "            count_score = 0.8  # ç¨å¤šä½†å¯æ¥å—\n",
    "        else:\n",
    "            count_score = 0.5  # éå¤šåˆ†å¡Š\n",
    "        \n",
    "        # åˆ†å¡Šå¤§å°åˆ†ä½ˆå‡å‹»æ€§\n",
    "        chunk_sizes = [chunk.word_count for chunk in chunks]\n",
    "        if len(chunk_sizes) > 1:\n",
    "            mean_size = np.mean(chunk_sizes)\n",
    "            std_size = np.std(chunk_sizes)\n",
    "            \n",
    "            if mean_size > 0:\n",
    "                cv = std_size / mean_size  # è®Šç•°ä¿‚æ•¸\n",
    "                uniformity_score = max(0.0, 1.0 - cv)  # è®Šç•°ä¿‚æ•¸è¶Šå°è¶Šå¥½\n",
    "            else:\n",
    "                uniformity_score = 0.0\n",
    "        else:\n",
    "            uniformity_score = 1.0\n",
    "        \n",
    "        return 0.6 * count_score + 0.4 * uniformity_score\n",
    "    \n",
    "    def _assess_content_length(self, content: str) -> float:\n",
    "        \"\"\"\n",
    "        è©•ä¼°å…§å®¹é•·åº¦åˆç†æ€§\n",
    "        \n",
    "        ç†æƒ³é•·åº¦ç¯„åœ:\n",
    "        - éçŸ­ (<50å­—): 0.3\n",
    "        - çŸ­ (50-200å­—): 0.6  \n",
    "        - é©ä¸­ (200-2000å­—): 1.0\n",
    "        - é•· (2000-5000å­—): 0.8\n",
    "        - éé•· (>5000å­—): 0.6\n",
    "        \"\"\"\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        if word_count < 50:\n",
    "            return 0.3\n",
    "        elif word_count < 200:\n",
    "            return 0.6\n",
    "        elif word_count < 2000:\n",
    "            return 1.0\n",
    "        elif word_count < 5000:\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.6\n",
    "\n",
    "print(\"âœ… QualityAssessor å¯¦ä½œå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è§£ç­” 4: SimpleDocumentProcessor æ•´åˆå¯¦ä½œ\n",
    "\n",
    "### è¨­è¨ˆè¦é»\n",
    "1. **çµ±ä¸€ä»‹é¢**: æä¾›ç°¡å–®æ˜“ç”¨çš„ API\n",
    "2. **éŒ¯èª¤è™•ç†**: å„ªé›…è™•ç†å„ç¨®ç•°å¸¸æƒ…æ³\n",
    "3. **æ‰¹æ¬¡è™•ç†**: æ”¯æ´ç›®éŒ„ç´šåˆ¥çš„æ‰¹æ¬¡æ“ä½œ\n",
    "4. **çµæœæ¨™æº–åŒ–**: çµ±ä¸€çš„è¿”å›æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDocumentProcessor:\n",
    "    \"\"\"ç°¡å–®çš„æ–‡æª”è™•ç†å™¨ - å®Œæ•´å¯¦ä½œ\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_strategy: str = 'paragraph', min_words: int = 10, max_words: int = 300):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è™•ç†å™¨\n",
    "        \n",
    "        Args:\n",
    "            chunk_strategy: åˆ†å¡Šç­–ç•¥ ('paragraph' æˆ– 'sentence')\n",
    "            min_words: æœ€å°å­—æ•¸\n",
    "            max_words: æœ€å¤§å­—æ•¸\n",
    "        \"\"\"\n",
    "        self.chunk_strategy = chunk_strategy\n",
    "        self.metadata_extractor = SimpleMetadataExtractor()\n",
    "        self.chunker = BasicChunker(min_words, max_words)\n",
    "        self.quality_assessor = QualityAssessor()\n",
    "        \n",
    "        print(f\"âœ… æ–‡æª”è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ (ç­–ç•¥: {chunk_strategy})\")\n",
    "    \n",
    "    def process_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        è™•ç†å–®ä¸€æ–‡ä»¶\n",
    "        \n",
    "        è™•ç†æµç¨‹:\n",
    "        1. è®€å–æª”æ¡ˆå…§å®¹\n",
    "        2. æå–å…ƒè³‡æ–™\n",
    "        3. åŸ·è¡Œåˆ†å¡Š\n",
    "        4. è©•ä¼°å“è³ª\n",
    "        5. å›å‚³çµæœå­—å…¸\n",
    "        \n",
    "        Returns:\n",
    "            Dict: åŒ…å« metadata, chunks, quality çš„è™•ç†çµæœ\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ“„ è™•ç†æª”æ¡ˆ: {Path(file_path).name}\")\n",
    "            \n",
    "            # 1. è®€å–æª”æ¡ˆå…§å®¹\n",
    "            content = self._read_file_content(file_path)\n",
    "            \n",
    "            # 2. æå–å…ƒè³‡æ–™\n",
    "            metadata = self.metadata_extractor.extract_from_file(file_path, content)\n",
    "            \n",
    "            # 3. åŸ·è¡Œåˆ†å¡Š\n",
    "            chunks = self._perform_chunking(content, metadata.doc_id)\n",
    "            \n",
    "            # 4. è©•ä¼°å“è³ª\n",
    "            quality = self.quality_assessor.assess_quality(content, chunks)\n",
    "            \n",
    "            # 5. çµ„ç¹”å›å‚³çµæœ\n",
    "            result = {\n",
    "                'metadata': metadata,\n",
    "                'chunks': chunks,\n",
    "                'quality': quality,\n",
    "                'processing_status': 'success',\n",
    "                'error_message': None\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… è™•ç†å®Œæˆ - å“è³ª: {quality.overall:.3f}, åˆ†å¡Š: {len(chunks)}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è™•ç†å¤±æ•—: {e}\")\n",
    "            return {\n",
    "                'metadata': None,\n",
    "                'chunks': [],\n",
    "                'quality': SimpleQuality(0.0, 0.0, 0.0),\n",
    "                'processing_status': 'failed',\n",
    "                'error_message': str(e)\n",
    "            }\n",
    "    \n",
    "    def process_directory(self, dir_path: str, file_extensions: List[str] = ['.txt', '.md']) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰æ–‡æœ¬æª”æ¡ˆ\n",
    "        \n",
    "        Args:\n",
    "            dir_path: ç›®éŒ„è·¯å¾‘\n",
    "            file_extensions: è¦è™•ç†çš„æª”æ¡ˆå‰¯æª”ååˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: æ‰€æœ‰æ–‡ä»¶çš„è™•ç†çµæœ\n",
    "        \"\"\"\n",
    "        directory = Path(dir_path)\n",
    "        \n",
    "        if not directory.exists() or not directory.is_dir():\n",
    "            print(f\"âŒ ç›®éŒ„ä¸å­˜åœ¨æˆ–ç„¡æ•ˆ: {dir_path}\")\n",
    "            return []\n",
    "        \n",
    "        # æ”¶é›†è¦è™•ç†çš„æª”æ¡ˆ\n",
    "        files_to_process = []\n",
    "        for ext in file_extensions:\n",
    "            pattern = f\"*{ext}\"\n",
    "            files_to_process.extend(directory.glob(pattern))\n",
    "        \n",
    "        print(f\"ğŸ” æ‰¾åˆ° {len(files_to_process)} å€‹æª”æ¡ˆå¾…è™•ç†\")\n",
    "        \n",
    "        # æ‰¹æ¬¡è™•ç†æª”æ¡ˆ\n",
    "        results = []\n",
    "        success_count = 0\n",
    "        \n",
    "        for file_path in files_to_process:\n",
    "            result = self.process_file(str(file_path))\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['processing_status'] == 'success':\n",
    "                success_count += 1\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ‰¹æ¬¡è™•ç†å®Œæˆ: {success_count}/{len(files_to_process)} æˆåŠŸ\")\n",
    "        return results\n",
    "    \n",
    "    def _read_file_content(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        è®€å–æª”æ¡ˆå…§å®¹ï¼Œæ”¯æ´å¤šç¨®ç·¨ç¢¼\n",
    "        \"\"\"\n",
    "        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    return f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        raise ValueError(f\"ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}ï¼Œå˜—è©¦äº†å¤šç¨®ç·¨ç¢¼å‡å¤±æ•—\")\n",
    "    \n",
    "    def _perform_chunking(self, content: str, doc_id: str) -> List[SimpleChunk]:\n",
    "        \"\"\"\n",
    "        æ ¹æ“šè¨­å®šçš„ç­–ç•¥åŸ·è¡Œåˆ†å¡Š\n",
    "        \"\"\"\n",
    "        if self.chunk_strategy == 'paragraph':\n",
    "            return self.chunker.chunk_by_paragraph(content, doc_id)\n",
    "        elif self.chunk_strategy == 'sentence':\n",
    "            return self.chunker.chunk_by_sentences(content, doc_id)\n",
    "        else:\n",
    "            print(f\"âš ï¸  æœªçŸ¥çš„åˆ†å¡Šç­–ç•¥: {self.chunk_strategy}ï¼Œä½¿ç”¨é è¨­æ®µè½åˆ†å¡Š\")\n",
    "            return self.chunker.chunk_by_paragraph(content, doc_id)\n",
    "    \n",
    "    def generate_summary_report(self, results: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆè™•ç†çµæœæ‘˜è¦å ±å‘Š\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        report_data = []\n",
    "        for result in results:\n",
    "            if result['processing_status'] == 'success':\n",
    "                metadata = result['metadata']\n",
    "                chunks = result['chunks']\n",
    "                quality = result['quality']\n",
    "                \n",
    "                report_data.append({\n",
    "                    'æª”å': metadata.filename,\n",
    "                    'é¡å‹': metadata.doc_type,\n",
    "                    'å­—æ•¸': metadata.word_count,\n",
    "                    'æª”æ¡ˆå¤§å°(KB)': round(metadata.file_size / 1024, 2),\n",
    "                    'åˆ†å¡Šæ•¸': len(chunks),\n",
    "                    'å¯è®€æ€§': round(quality.readability, 3),\n",
    "                    'å®Œæ•´æ€§': round(quality.completeness, 3),\n",
    "                    'å“è³ª': round(quality.overall, 3),\n",
    "                    'ç‹€æ…‹': 'æˆåŠŸ'\n",
    "                })\n",
    "            else:\n",
    "                # è™•ç†å¤±æ•—çš„æª”æ¡ˆ\n",
    "                report_data.append({\n",
    "                    'æª”å': 'æœªçŸ¥',\n",
    "                    'é¡å‹': 'æœªçŸ¥',\n",
    "                    'å­—æ•¸': 0,\n",
    "                    'æª”æ¡ˆå¤§å°(KB)': 0,\n",
    "                    'åˆ†å¡Šæ•¸': 0,\n",
    "                    'å¯è®€æ€§': 0.0,\n",
    "                    'å®Œæ•´æ€§': 0.0,\n",
    "                    'å“è³ª': 0.0,\n",
    "                    'ç‹€æ…‹': 'å¤±æ•—'\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(report_data)\n",
    "\n",
    "print(\"âœ… SimpleDocumentProcessor æ•´åˆå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª å®Œæ•´æ¸¬è©¦èˆ‡é©—è­‰\n",
    "\n",
    "### å»ºç«‹æ¸¬è©¦è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è©³ç´°çš„æ¸¬è©¦æª”æ¡ˆ\n",
    "test_files = {\n",
    "    'academic_paper.txt': \"\"\"\n",
    "Transformer Networks for Natural Language Processing\n",
    "\n",
    "Abstract\n",
    "This paper introduces the Transformer architecture, a novel neural network design based entirely on attention mechanisms. We demonstrate that Transformers achieve state-of-the-art results on machine translation tasks while being more parallelizable than recurrent architectures.\n",
    "\n",
    "1. Introduction\n",
    "Natural language processing has been dominated by recurrent neural networks and convolutional neural networks. These architectures have several limitations including sequential processing requirements and difficulty in capturing long-range dependencies.\n",
    "\n",
    "The Transformer addresses these issues through a self-attention mechanism that relates different positions of a single sequence. This allows for significantly more parallelization and better performance on long sequences.\n",
    "\n",
    "2. Related Work\n",
    "Attention mechanisms have been used successfully in various NLP tasks. However, most previous work used attention in conjunction with recurrent or convolutional networks. Our work is the first to rely entirely on attention.\n",
    "\n",
    "3. Methodology\n",
    "The Transformer consists of an encoder and decoder, each composed of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
    "\n",
    "3.1 Multi-Head Attention\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces. We compute attention as scaled dot-product attention.\n",
    "\n",
    "3.2 Position Encoding\n",
    "Since our model contains no recurrence or convolution, we add positional encodings to the input embeddings to provide information about the relative or absolute position of tokens.\n",
    "\n",
    "4. Experiments\n",
    "We evaluate our model on English-German and English-French translation tasks from the WMT 2014 dataset. Our model achieves 28.4 BLEU on English-German translation, establishing a new state-of-the-art.\n",
    "\n",
    "5. Conclusion\n",
    "We presented the Transformer, a novel architecture based solely on attention mechanisms. Our results show that attention-based models can outperform recurrent and convolutional networks while being more parallelizable.\n",
    "\n",
    "References\n",
    "[1] Vaswani et al. Attention Is All You Need. NIPS 2017.\n",
    "\"\"\",\n",
    "    \n",
    "    'business_report.txt': \"\"\"\n",
    "Q3 2024 Financial Performance Report\n",
    "\n",
    "Executive Summary\n",
    "Our company delivered strong financial results in Q3 2024, with revenue growth of 18% year-over-year. Net income increased to $2.1M, representing a 25% improvement from Q3 2023.\n",
    "\n",
    "Revenue Analysis\n",
    "Total revenue reached $15.8M in Q3 2024, compared to $13.4M in the previous year. This growth was driven primarily by increased demand for our cloud services and successful product launches.\n",
    "\n",
    "Product revenue accounted for 65% of total revenue, while services contributed 35%. The shift toward higher-margin services continued this quarter.\n",
    "\n",
    "Operating Expenses\n",
    "Operating expenses were $11.2M, up from $10.1M in Q3 2023. The increase was primarily due to expanded R&D investments and increased headcount to support growth.\n",
    "\n",
    "Research and development expenses increased to $3.5M, reflecting our commitment to innovation. Sales and marketing expenses were $4.2M, supporting our market expansion efforts.\n",
    "\n",
    "Market Performance\n",
    "Customer satisfaction scores remained high at 4.3/5.0. Customer retention rate improved to 92%, up from 89% in the previous quarter. These metrics demonstrate the quality of our products and services.\n",
    "\n",
    "Our market share in the cloud services segment increased to 12%, positioning us as a strong competitor in this growing market.\n",
    "\n",
    "Outlook and Recommendations\n",
    "Looking ahead to Q4 2024, we expect continued growth driven by seasonal demand and new product releases. We recommend increasing investment in customer support to maintain satisfaction levels.\n",
    "\n",
    "Key initiatives for the next quarter include expanding our international presence and enhancing our product portfolio through strategic partnerships.\n",
    "\"\"\",\n",
    "    \n",
    "    'short_note.txt': \"\"\"\n",
    "Meeting Notes - Project Kickoff\n",
    "\n",
    "Date: November 11, 2024\n",
    "Attendees: John, Sarah, Mike, Lisa\n",
    "\n",
    "Key Decisions:\n",
    "- Project timeline: 6 months\n",
    "- Budget approved: $50K\n",
    "- Weekly meetings on Wednesdays\n",
    "\n",
    "Next Steps:\n",
    "- Sarah will create project plan by Friday\n",
    "- Mike will set up development environment\n",
    "- Lisa will coordinate with stakeholders\n",
    "\"\"\",\n",
    "    \n",
    "    'technical_doc.txt': \"\"\"\n",
    "API Documentation - User Management Service\n",
    "\n",
    "Overview\n",
    "The User Management Service provides REST API endpoints for user authentication, authorization, and profile management.\n",
    "\n",
    "Authentication Endpoints\n",
    "\n",
    "POST /auth/login\n",
    "Authenticates a user with email and password. Returns JWT token on success.\n",
    "\n",
    "Request Body:\n",
    "{\n",
    "  \"email\": \"user@example.com\",\n",
    "  \"password\": \"secure_password\"\n",
    "}\n",
    "\n",
    "Response:\n",
    "{\n",
    "  \"token\": \"jwt_token_here\",\n",
    "  \"expires_at\": \"2024-12-11T10:00:00Z\"\n",
    "}\n",
    "\n",
    "POST /auth/logout\n",
    "Invalidates the current user session. Requires valid JWT token in Authorization header.\n",
    "\n",
    "User Profile Endpoints\n",
    "\n",
    "GET /users/profile\n",
    "Returns the current user's profile information. Requires authentication.\n",
    "\n",
    "PUT /users/profile\n",
    "Updates user profile information. Requires authentication and valid data.\n",
    "\n",
    "Error Handling\n",
    "All endpoints return standard HTTP status codes. Error responses include detailed error messages in JSON format.\n",
    "\n",
    "Rate Limiting\n",
    "API calls are limited to 1000 requests per hour per user. Exceeding this limit returns HTTP 429 status.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦ç›®éŒ„å’Œæª”æ¡ˆ\n",
    "test_dir = SOLUTION_DIR / 'test_documents'\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename, content in test_files.items():\n",
    "    file_path = test_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"âœ… å»ºç«‹äº† {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆæ–¼ {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œå®Œæ•´æ¸¬è©¦æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª é–‹å§‹å®Œæ•´æ¸¬è©¦æµç¨‹\\n\")\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒåˆ†å¡Šç­–ç•¥\n",
    "strategies = ['paragraph', 'sentence']\n",
    "all_results = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nğŸ“‹ æ¸¬è©¦ç­–ç•¥: {strategy.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åˆå§‹åŒ–è™•ç†å™¨\n",
    "    processor = SimpleDocumentProcessor(\n",
    "        chunk_strategy=strategy,\n",
    "        min_words=15,\n",
    "        max_words=200\n",
    "    )\n",
    "    \n",
    "    # æ‰¹æ¬¡è™•ç†\n",
    "    results = processor.process_directory(str(test_dir))\n",
    "    all_results[strategy] = results\n",
    "    \n",
    "    # ç”Ÿæˆå ±å‘Š\n",
    "    report_df = processor.generate_summary_report(results)\n",
    "    print(f\"\\nğŸ“Š {strategy.upper()} ç­–ç•¥è™•ç†å ±å‘Š:\")\n",
    "    print(report_df.to_string(index=False))\n",
    "    \n",
    "    # çµ±è¨ˆåˆ†æ\n",
    "    successful_results = [r for r in results if r['processing_status'] == 'success']\n",
    "    if successful_results:\n",
    "        avg_quality = np.mean([r['quality'].overall for r in successful_results])\n",
    "        avg_chunks = np.mean([len(r['chunks']) for r in successful_results])\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ {strategy.upper()} ç­–ç•¥çµ±è¨ˆ:\")\n",
    "        print(f\"å¹³å‡å“è³ªåˆ†æ•¸: {avg_quality:.3f}\")\n",
    "        print(f\"å¹³å‡åˆ†å¡Šæ•¸é‡: {avg_chunks:.1f}\")\n",
    "        print(f\"è™•ç†æˆåŠŸç‡: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ å®Œæ•´æ¸¬è©¦æµç¨‹çµæŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è©³ç´°çµæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” è©³ç´°çµæœåˆ†æ\\n\")\n",
    "\n",
    "# åˆ†æå–®ä¸€æª”æ¡ˆçš„è™•ç†çµæœ\n",
    "sample_file = test_dir / 'academic_paper.txt'\n",
    "processor_detailed = SimpleDocumentProcessor(chunk_strategy='paragraph')\n",
    "detailed_result = processor_detailed.process_file(str(sample_file))\n",
    "\n",
    "if detailed_result['processing_status'] == 'success':\n",
    "    metadata = detailed_result['metadata']\n",
    "    chunks = detailed_result['chunks']\n",
    "    quality = detailed_result['quality']\n",
    "    \n",
    "    print(f\"ğŸ“‹ æª”æ¡ˆ: {metadata.filename}\")\n",
    "    print(f\"æ–‡æª” ID: {metadata.doc_id}\")\n",
    "    print(f\"æ–‡æª”é¡å‹: {metadata.doc_type}\")\n",
    "    print(f\"ç¸½å­—æ•¸: {metadata.word_count}\")\n",
    "    print(f\"æª”æ¡ˆå¤§å°: {metadata.file_size} bytes\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å“è³ªè©•ä¼°è©³ç´°:\")\n",
    "    print(f\"å¯è®€æ€§åˆ†æ•¸: {quality.readability:.3f}\")\n",
    "    print(f\"å®Œæ•´æ€§åˆ†æ•¸: {quality.completeness:.3f}\")\n",
    "    print(f\"ç¶œåˆå“è³ª: {quality.overall:.3f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ åˆ†å¡Šè©³ç´°è³‡è¨Š:\")\n",
    "    print(f\"ç¸½åˆ†å¡Šæ•¸: {len(chunks)}\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:3]):  # é¡¯ç¤ºå‰3å€‹åˆ†å¡Š\n",
    "        print(f\"\\nåˆ†å¡Š {i+1}:\")\n",
    "        print(f\"  ID: {chunk.chunk_id}\")\n",
    "        print(f\"  å­—æ•¸: {chunk.word_count}\")\n",
    "        print(f\"  å…§å®¹é è¦½: {chunk.content[:150]}...\")\n",
    "    \n",
    "    if len(chunks) > 3:\n",
    "        print(f\"\\n... (é‚„æœ‰ {len(chunks) - 3} å€‹åˆ†å¡Š)\")\n",
    "    \n",
    "    # åˆ†å¡Šå¤§å°çµ±è¨ˆ\n",
    "    chunk_sizes = [chunk.word_count for chunk in chunks]\n",
    "    print(f\"\\nğŸ“ˆ åˆ†å¡Šçµ±è¨ˆ:\")\n",
    "    print(f\"æœ€å°åˆ†å¡Š: {min(chunk_sizes)} å­—\")\n",
    "    print(f\"æœ€å¤§åˆ†å¡Š: {max(chunk_sizes)} å­—\")\n",
    "    print(f\"å¹³å‡åˆ†å¡Š: {np.mean(chunk_sizes):.1f} å­—\")\n",
    "    print(f\"æ¨™æº–å·®: {np.std(chunk_sizes):.1f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ è™•ç†å¤±æ•—: {detailed_result['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è§£ç­”ç¸½çµèˆ‡å­¸ç¿’é‡é»\n",
    "\n",
    "### ğŸ† å¯¦ä½œäº®é»\n",
    "\n",
    "1. **æ¨¡çµ„åŒ–è¨­è¨ˆ**\n",
    "   - æ¯å€‹çµ„ä»¶è·è²¬å–®ä¸€æ˜ç¢º\n",
    "   - æ˜“æ–¼æ¸¬è©¦å’Œç¶­è­·\n",
    "   - æ”¯æ´éˆæ´»é…ç½®\n",
    "\n",
    "2. **éŒ¯èª¤è™•ç†æ©Ÿåˆ¶**\n",
    "   - å„ªé›…è™•ç†æª”æ¡ˆè®€å–ç•°å¸¸\n",
    "   - å¤šç¨®ç·¨ç¢¼æ ¼å¼æ”¯æ´\n",
    "   - é‚Šç•Œæ¢ä»¶ä¿è­·\n",
    "\n",
    "3. **æ¼”ç®—æ³•è¨­è¨ˆ**\n",
    "   - å•Ÿç™¼å¼æ–‡æª”åˆ†é¡\n",
    "   - å‹•æ…‹åˆ†å¡Šå¤§å°èª¿æ•´\n",
    "   - å¤šç¶­åº¦å“è³ªè©•ä¼°\n",
    "\n",
    "### ğŸ”‘ é—œéµå­¸ç¿’é»\n",
    "\n",
    "#### 1. å…ƒè³‡æ–™æå–ç­–ç•¥\n",
    "```python\n",
    "# çµ„åˆå¤šç¨®è³‡è¨Šä¾†æº\n",
    "- æª”æ¡ˆç³»çµ±è³‡è¨Š (å¤§å°ã€æ™‚é–“)\n",
    "- æª”æ¡ˆè·¯å¾‘ç‰¹å¾µ\n",
    "- å…§å®¹çµæ§‹åˆ†æ\n",
    "- å•Ÿç™¼å¼åˆ†é¡è¦å‰‡\n",
    "```\n",
    "\n",
    "#### 2. åˆ†å¡Šæ¼”ç®—æ³•æ ¸å¿ƒ\n",
    "```python\n",
    "# æ®µè½åˆ†å¡Š: ä¿æŒèªç¾©å®Œæ•´æ€§\n",
    "# å¥å­åˆ†å¡Š: ç²¾ç´°æ§åˆ¶åˆ†å¡Šå¤§å°\n",
    "# å‹•æ…‹èª¿æ•´: æ ¹æ“šå…§å®¹ç‰¹å¾µèª¿æ•´ç­–ç•¥\n",
    "```\n",
    "\n",
    "#### 3. å“è³ªè©•ä¼°æ¡†æ¶\n",
    "```python\n",
    "# å¤šç¶­åº¦è©•ä¼°\n",
    "- å¯è®€æ€§: å¥å­é•·åº¦åˆ†ä½ˆ\n",
    "- å®Œæ•´æ€§: çµæ§‹ + åˆ†å¡Šå“è³ª + é•·åº¦\n",
    "- ç¶œåˆè©•åˆ†: åŠ æ¬Šå¹³å‡\n",
    "```\n",
    "\n",
    "### ğŸš€ é€²éšæ”¹é€²æ–¹å‘\n",
    "\n",
    "1. **èªç¾©åˆ†å¡Š**: æ•´åˆ sentence-transformers\n",
    "2. **å¤šèªè¨€æ”¯æ´**: æ·»åŠ èªè¨€æª¢æ¸¬å’Œåˆ†è©\n",
    "3. **ä¸¦ç™¼è™•ç†**: å¯¦ä½œ async/await ä¸¦ç™¼\n",
    "4. **é…ç½®ç®¡ç†**: JSON/YAML é…ç½®æª”æ¡ˆ\n",
    "5. **çµæœå¿«å–**: é¿å…é‡è¤‡è™•ç†\n",
    "\n",
    "### ğŸ’¡ æœ€ä½³å¯¦å‹™ç¸½çµ\n",
    "\n",
    "1. **å§‹çµ‚è€ƒæ…®é‚Šç•Œæ¢ä»¶** - ç©ºæª”æ¡ˆã€è¶…å¤§æª”æ¡ˆã€ç‰¹æ®Šå­—ç¬¦\n",
    "2. **æä¾›æ¸…æ™°çš„éŒ¯èª¤ä¿¡æ¯** - å¹«åŠ©ä½¿ç”¨è€…å¿«é€Ÿå®šä½å•é¡Œ\n",
    "3. **ä¿æŒä»‹é¢ç°¡æ½”** - éš±è—è¤‡é›œæ€§ï¼Œæä¾›ç°¡å–® API\n",
    "4. **æ–‡æª”å’Œæ¸¬è©¦åŒç­‰é‡è¦** - ç¢ºä¿ä»£ç¢¼å¯ç¶­è­·æ€§\n",
    "5. **æ•ˆèƒ½èˆ‡å“è³ªä¸¦é‡** - åœ¨é€Ÿåº¦å’Œæº–ç¢ºæ€§ä¹‹é–“æ‰¾å¹³è¡¡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°\n",
    "\n",
    "å®Œæˆæœ¬è§£ç­”å­¸ç¿’å¾Œï¼Œå»ºè­°ï¼š\n",
    "\n",
    "1. **å˜—è©¦æ”¹é€²ç®—æ³•** - å¯¦ä½œèªç¾©åˆ†å¡Šã€å„ªåŒ–å“è³ªè©•ä¼°\n",
    "2. **è™•ç†çœŸå¯¦è³‡æ–™** - æ¸¬è©¦ä¸åŒæ ¼å¼å’Œèªè¨€çš„æ–‡æª”\n",
    "3. **æ•´åˆé€²éšå·¥å…·** - çµåˆ Doclingã€spaCy ç­‰å°ˆæ¥­å·¥å…·\n",
    "4. **å­¸ç¿’æ¨¡çµ„ 2** - é€²å…¥å…ƒè³‡æ–™ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹\n",
    "\n",
    "æ­å–œå®ŒæˆåŸºç¤æ–‡æª”è™•ç†çš„å­¸ç¿’ï¼ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}