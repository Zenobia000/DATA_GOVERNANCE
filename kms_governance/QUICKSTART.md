# KMS è³‡æ–™æ²»ç†ç³»çµ± - å¿«é€Ÿé–‹å§‹æŒ‡å—
## 5 åˆ†é˜ä¸Šæ‰‹æ•™å­¸

> ä»¥ AI è«–æ–‡é›†åˆ (papers/) ç‚ºç¯„ä¾‹å ´åŸŸ

---

## ğŸš€ å¿«é€Ÿé–‹å§‹ä¸‰æ­¥é©Ÿ

### Step 1: ç’°å¢ƒè¨­å®š (2 åˆ†é˜)

```bash
# 1. é€²å…¥å°ˆæ¡ˆç›®éŒ„
cd d:\python_workspace\project_nlp\data_governance\kms_governance

# 2. å®‰è£ä¾è³´å¥—ä»¶
pip install -r requirements.txt

# 3. ä¸‹è¼‰ spaCy èªè¨€æ¨¡å‹
python -m spacy download en_core_web_sm

# 4. é©—è­‰å®‰è£
python -c "import docling; print('âœ… Docling OK')"
python -c "import langchain; print('âœ… LangChain OK')"
```

### Step 2: åŸ·è¡Œ Demo Notebook (2 åˆ†é˜)

```bash
# å•Ÿå‹• Jupyter Lab
jupyter lab notebooks/04_end_to_end_demo.ipynb
```

ç„¶å¾Œåœ¨ Notebook ä¸­åŸ·è¡Œæ‰€æœ‰ cellsï¼Œä½ å°‡çœ‹åˆ°ï¼š
- âœ… æƒæ 31 ç¯‡ AI è«–æ–‡
- âœ… ä½¿ç”¨ Docling æå–å…§å®¹
- âœ… å»ºç«‹å…ƒè³‡æ–™ç›®éŒ„
- âœ… å“è³ªè©•ä¼°å ±å‘Š
- âœ… è¦–è¦ºåŒ–åˆ†æ

### Step 3: ä½¿ç”¨ Python API (1 åˆ†é˜)

```python
from pathlib import Path
from utils.document_processor import DocumentProcessor

# åˆå§‹åŒ–è™•ç†å™¨
processor = DocumentProcessor(config={
    'quality_threshold': 0.75,
    'chunk_size': 1000
})

# è™•ç†å–®ç¯‡è«–æ–‡
result = processor.process_document(
    "../../papers/01_model_paradigm/2017_Transformer.pdf"
)

if result.success:
    print(f"âœ… æˆåŠŸ: {result.metadata.title}")
    print(f"   å“è³ªåˆ†æ•¸: {result.metadata.quality_score}")
    print(f"   å­—æ•¸: {result.metadata.word_count:,}")

    # åˆ†å¡Šè™•ç†
    chunks = processor.chunk_content(result.content)
    print(f"   åˆ†å¡Šæ•¸: {len(chunks)}")
```

---

## ğŸ“‚ ç›®éŒ„çµæ§‹

```
kms_governance/
â”œâ”€â”€ ğŸ““ notebooks/               # Jupyter æ•™å­¸ç­†è¨˜æœ¬
â”‚   â”œâ”€â”€ 00_architecture_overview.ipynb
â”‚   â”œâ”€â”€ 01_document_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_metadata_management.ipynb
â”‚   â”œâ”€â”€ 03_quality_control.ipynb
â”‚   â””â”€â”€ 04_end_to_end_demo.ipynb â­
â”‚
â”œâ”€â”€ ğŸ”§ utils/                   # æ ¸å¿ƒå·¥å…·æ¨¡çµ„
â”‚   â”œâ”€â”€ document_processor.py   â­
â”‚   â”œâ”€â”€ metadata_extractor.py
â”‚   â”œâ”€â”€ quality_assessor.py
â”‚   â””â”€â”€ lineage_tracker.py
â”‚
â”œâ”€â”€ âš™ï¸ configs/                 # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ governance_policy.yaml  â­
â”‚   â”œâ”€â”€ quality_thresholds.json
â”‚   â””â”€â”€ processing_rules.yaml
â”‚
â”œâ”€â”€ ğŸ“ 01_raw/                  # åŸå§‹æ–‡æª”ï¼ˆæŒ‡å‘ papers/ï¼‰
â”œâ”€â”€ ğŸ“ 02_processed/            # è™•ç†å¾Œæ–‡æª”
â”œâ”€â”€ ğŸ“ 03_indexed/              # ç´¢å¼•å±¤
â”œâ”€â”€ ğŸ“ 04_metadata/             # å…ƒè³‡æ–™å±¤
â”œâ”€â”€ ğŸ“ 05_quality/              # å“è³ªå ±å‘Š
â””â”€â”€ ğŸ“ 06_lineage/              # è¡€ç·£è¿½è¹¤

â­ = é‡é»æª”æ¡ˆ
```

---

## ğŸ’¡ å¸¸è¦‹ä½¿ç”¨å ´æ™¯

### å ´æ™¯ 1: æ‰¹é‡è™•ç†è«–æ–‡è³‡æ–™å¤¾

```python
from pathlib import Path
from utils.document_processor import (
    DocumentProcessor,
    scan_directory,
    results_to_dataframe
)

# æƒæç›®éŒ„
papers_dir = Path("../../papers/01_model_paradigm")
files = scan_directory(papers_dir, extensions=['.pdf'])

print(f"æ‰¾åˆ° {len(files)} ç¯‡è«–æ–‡")

# æ‰¹é‡è™•ç†
processor = DocumentProcessor()
results = processor.batch_process([str(f) for f in files])

# è½‰ç‚º DataFrame
df = results_to_dataframe(results)

# å„²å­˜çµæœ
df.to_csv("processed_papers.csv", index=False)

print(f"âœ… è™•ç†å®Œæˆ: {processor.get_stats()}")
```

### å ´æ™¯ 2: å“è³ªéæ¿¾èˆ‡å ±å‘Š

```python
import pandas as pd

# è¼‰å…¥è™•ç†çµæœ
df = pd.read_csv("processed_papers.csv")

# å“è³ªéæ¿¾
high_quality = df[df['quality_score'] >= 0.8]

print(f"é«˜å“è³ªè«–æ–‡: {len(high_quality)}/{len(df)}")
print(f"å¹³å‡å“è³ªåˆ†æ•¸: {df['quality_score'].mean():.3f}")

# å“è³ªåˆ†å¸ƒ
print(df['quality_score'].describe())

# è¼¸å‡ºå ±å‘Š
report = {
    "total": len(df),
    "high_quality": len(high_quality),
    "avg_score": df['quality_score'].mean(),
    "median_score": df['quality_score'].median()
}

import json
with open("quality_report.json", "w") as f:
    json.dump(report, f, indent=2)
```

### å ´æ™¯ 3: å»ºç«‹æ–‡æª”ç›®éŒ„è³‡æ–™åº«

```python
import sqlite3
import pandas as pd

# è¼‰å…¥å…ƒè³‡æ–™
df = pd.read_csv("processed_papers.csv")

# é€£æ¥ SQLite
conn = sqlite3.connect("04_metadata/document_catalog.db")

# å¯«å…¥è³‡æ–™åº«
df.to_sql('documents', conn, if_exists='replace', index=False)

# æŸ¥è©¢ç¯„ä¾‹
query = """
SELECT filename, category, year, quality_score
FROM documents
WHERE quality_score >= 0.8
ORDER BY year DESC
"""

results = pd.read_sql(query, conn)
print(results)

conn.close()
```

---

## ğŸ¯ é—œéµé…ç½®

### èª¿æ•´å“è³ªé–¾å€¼

ç·¨è¼¯ `configs/governance_policy.yaml`:

```yaml
quality_control:
  thresholds:
    minimum_score: 0.70   # æœ€ä½æ¥å—åˆ†æ•¸
    target_score: 0.85    # ç›®æ¨™åˆ†æ•¸
```

### èª¿æ•´åˆ†å¡Šåƒæ•¸

```yaml
document_processing:
  chunking:
    chunk_size: 1000      # èª¿æ•´åˆ†å¡Šå¤§å°
    chunk_overlap: 200    # èª¿æ•´é‡ç–Šå¤§å°
```

---

## ğŸ“Š æª¢è¦–çµæœ

### å…ƒè³‡æ–™ç›®éŒ„

```bash
# SQLite è³‡æ–™åº«
sqlite3 04_metadata/document_catalog.db

# æŸ¥çœ‹æ‰€æœ‰æ–‡æª”
SELECT * FROM documents LIMIT 5;

# ä¾åˆ†é¡çµ±è¨ˆ
SELECT category, COUNT(*), AVG(quality_score)
FROM documents
GROUP BY category;
```

### å“è³ªå ±å‘Š

```bash
# JSON å ±å‘Š
cat 05_quality/governance_report.json | python -m json.tool
```

---

## ğŸ”§ é€²éšåŠŸèƒ½

### 1. è‡ªè¨‚æ–‡æª”è™•ç†å™¨

```python
class CustomProcessor(DocumentProcessor):
    def _assess_quality(self, metadata, content):
        # è‡ªè¨‚å“è³ªè©•ä¼°é‚è¼¯
        score = super()._assess_quality(metadata, content)

        # åŠ å…¥è‡ªè¨‚è¦å‰‡
        if "important_keyword" in content:
            score += 0.1

        return min(1.0, score)
```

### 2. æ•´åˆå‘é‡è³‡æ–™åº«ï¼ˆæº–å‚™ RAGï¼‰

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# è™•ç†æ–‡æª”
processor = DocumentProcessor()
result = processor.process_document("paper.pdf")

# åˆ†å¡Š
chunks = processor.chunk_content(result.content, result.metadata)

# å‘é‡åŒ–
embeddings = OpenAIEmbeddings()
texts = [c['text'] for c in chunks]
metadatas = [{'source': c['source_file']} for c in chunks]

# å»ºç«‹å‘é‡è³‡æ–™åº«
vectorstore = Chroma.from_texts(
    texts=texts,
    embedding=embeddings,
    metadatas=metadatas,
    persist_directory="03_indexed/vector_index"
)
```

### 3. è¡€ç·£è¿½è¹¤

```python
# è¨˜éŒ„è™•ç†æ­·å²
import json
from datetime import datetime

lineage_record = {
    "document_id": result.metadata.document_id,
    "source_file": result.metadata.filename,
    "processed_at": datetime.now().isoformat(),
    "transformations": [
        {"step": "content_extraction", "tool": "docling"},
        {"step": "chunking", "tool": "langchain"},
        {"step": "quality_assessment", "score": result.metadata.quality_score}
    ]
}

# å„²å­˜è¡€ç·£è¨˜éŒ„
with open(f"06_lineage/{result.metadata.document_id}.json", "w") as f:
    json.dump(lineage_record, f, indent=2)
```

---

## ğŸ› ç–‘é›£æ’è§£

### å•é¡Œ 1: Docling å®‰è£å¤±æ•—

```bash
# ç¢ºèª Python ç‰ˆæœ¬ (éœ€è¦ 3.11+)
python --version

# å‡ç´š pip
python -m pip install --upgrade pip

# é‡æ–°å®‰è£
pip install --no-cache-dir docling
```

### å•é¡Œ 2: è¨˜æ†¶é«”ä¸è¶³

```python
# æ‰¹æ¬¡è™•ç†æ™‚è¨­å®šè¼ƒå°çš„æ‰¹æ¬¡å¤§å°
processor = DocumentProcessor()

# åˆ†æ‰¹è™•ç†
batch_size = 5
for i in range(0, len(files), batch_size):
    batch = files[i:i+batch_size]
    results = processor.batch_process(batch)
    # è™•ç†çµæœ...
```

### å•é¡Œ 3: PDF æå–å¤±æ•—

```python
# ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
try:
    result = processor.process_document(file_path)
except Exception as e:
    print(f"Docling å¤±æ•—ï¼Œå˜—è©¦ PyPDF: {e}")

    # ä½¿ç”¨ PyPDF ä½œç‚ºå‚™ç”¨
    from pypdf import PdfReader
    reader = PdfReader(file_path)
    content = "".join(page.extract_text() for page in reader.pages)
```

---

## ğŸ“š ä¸‹ä¸€æ­¥å­¸ç¿’

1. **å®Œæ•´æ•™å­¸**: é–±è®€ `notebooks/00_architecture_overview.ipynb`
2. **ç†è«–åŸºç¤**: åƒè€ƒ `../ch1_document_governance/lectures/`
3. **API æ–‡æª”**: æŸ¥çœ‹ `docs/api_reference.md`
4. **æœ€ä½³å¯¦è¸**: é–±è®€ `docs/governance_guide.md`

---

## ğŸ¤ æŠ€è¡“æ”¯æ´

- **Issues**: GitHub Issues
- **æ–‡æª”**: [KMS Governance Docs](./README.md)
- **ç¯„ä¾‹**: `notebooks/` ç›®éŒ„

---

**ç‰ˆæœ¬**: v1.0
**æ›´æ–°**: 2025-01-17
**ä½œè€…**: Data Governance Team
